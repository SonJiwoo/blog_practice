---
collapsible: false
date: "2021-01-21T10:08:56+09:00"
description: Normal Model
draft: false
title: Normal Model
weight: 5
---

## Chapter 05. <br> Normal Model
본 포스팅은 **First Course in Bayesian Statistical Methods**와 **Bayesian Data Analysis**를 참고하였다.

### [Warm up!](/posts/statistics/statistics/probability_distribution/)
- Gamma Distribution
- Inverse Gamma Distribution
- Scaled Inverse Chi-squared Distribution

### 1. Single Parameter Conjugacy
평균이나 분산 중 하나만을 모르는 경우

#### 1-1. 평균을 모르는 경우
Prior: `$\mu \text{ ~ } N(\mu_0, \tau_0^{2})$`
Likelihood: `$y|\mu \text{ ~ } N(\mu, \sigma^2)$`
Posterior: `$\mu|y \text{ ~ } N(\mu_n, \tau_n^{2})$`

where `$\frac{1}{\tau_n^{2}} = \frac{1}{\tau_0^{2}} + \frac{n}{\sigma^2}$` and `$\mu_n = \frac{\frac{1}{\tau_0^{2}}}{\frac{1}{\tau_0^{2}} + \frac{n}{\sigma^2}}\mu_0 + \frac{\frac{n}{\sigma^2}}{\frac{1}{\tau_0^{2}} + \frac{n}{\sigma^2}}\bar{y} $`

Posterior Predictive: `$\tilde{y}|y \text{ ~ } N(\mu_n, \sigma^2+\tau_n^{2})$`

#### 1-2. 분산을 모르는 경우
Prior: `$\sigma^2 \text{ ~ } \chi^{-2}(\nu_0, \sigma_0^2)$`
Likelihood: `$y|\sigma^2 \text{ ~ } N(\mu, \sigma^2)$` 
Posterior:  `$\sigma^2|y \text{ ~ } \chi^{-2}(\nu_n, \sigma_n^2)$`

where `$\nu_n = \nu_0 + n$` and `$\sigma_n^2 = \frac{\nu_0\sigma_0^2 + ns(y)}{\nu_0 + n}$`  
c.f. `$s(y) = \frac{1}{n}\sum_{i=1}^{n}(y_i-\mu)^2$`, 이는 MLE이다(biased estimator). 참고로, 베이지안은 frequentist들의 기준인 unbiasedness를 중요하게 생각하지 않는다.

### 2. Two Parameter

###### marginal distribution 얻는 두 가지 방법
1. `Integreation`: joint posterior distribution을 구한 후, 관심 없는 모수(nuisance parameter)에 대해 적분
2. `Simulation`: joint posterior distribution에서 sample을 구한 후, 관심 있는 모수의 분포만 고려(나머지는 무시)

###### 그렇다면 joint posterior distribution은 어떻게 구할까?
1. marginal and conditional simulation을 통해서 구할 수 있다.
`$\theta_2 \text{ ~ } \theta_2|y$` and `$\theta_1 | \theta_2, y$`
`$\rightarrow (\theta_1, \theta_2) \text{ ~ } (\theta_1, \theta_2|y)$`

#### 2-1. noninformative prior
Prior: `$p(\mu, \sigma^2) = p(\mu)p(\sigma^2) \propto (\sigma^2)^{-1} $` (독립 가정, improper prior)
Likelihood: `$p(y|\mu, \sigma^2) \propto \sigma^{-n}exp(\frac{-1}{2}\sigma^2\sum_{i=1}^{n}(y_i - \mu)^2) $`
Posterior: `$\mu, \sigma^2 |y \text{ ~ } N(\bar{y}, \frac{\sigma^2}{n}) \times \chi^{-2}(n-1, s^2)$`
Posterior Predictive: `$\tilde{y}|y \text{ ~ } t_{n-1}(\bar{y}, (1+\frac{1}{n}s^2))$`
이는 posterior과 비교해서, data의 uncertainty(`$s^2$`)이 추가된 형태라고 해석할 수 있다.

##### Posterior Distribution 구하기 (Noninformative)
해당 Posterior Distribution을 구하는 과정은 다소 복잡하기 때문에 자세하게 서술해보도록 하겠다.
우선 시작하기에 앞서, 한마디로 이 과정을 요악한다면 `Conditional Posterior X Marginal`일 것이다. 

STEP1. `$p(\mu|\sigma^2,y)$` `$p(\sigma^2|y)$`의 형태를 파악한다.
1) `$\mu|\sigma^2,y \text{ ~ } N(\bar{y}, \frac{\sigma^2}{n})$`
이부분은 위의 [평균을 모르지만, 분산을 아는 경우](/posts/statistics/bayesian/fcb05/#1-1-평균을-모르는-경우)에서 prior precision `$\frac{1}{\tau^2}=0$`으로 주면 위와 같이 나온다. prior precision을 0으로 주는 이유는, non-informative prior를 가정하고 있기 때문이다.

2) `$\sigma^2|y \text{ ~ } \chi^{-2}(n-1, s^2)$`
이는 아래의 수식을 계산해서 얻을 수 있다.

\begin{align}
p(\mu, \sigma^2|y) &\propto p(\mu, \sigma^2) \times p(y|\mu, \sigma^2) \\
&\propto \sigma^{-n-2}exp\bigg(\frac{-1}{2\sigma^2}\big[(n-1)s^2 + n(\bar{y}-\mu)^2\big]\bigg) \\
\rightarrow p(\sigma^2|y) &= \int p(\mu,\sigma^2|y)d\mu 
\end{align}

STEP2. 베이즈룰을 이용하여 posterior distribution을 계산해준다.

위의 과정을 거친다면, 그 결과는 아래와 같이 정리할 수 있다.

\begin{align}
\mu|\sigma^2,y &\text{ ~ } N(\bar{y}, \frac{\sigma^2}{n}) \\
\sigma^2|y &\text{ ~ } \chi^{-2}(n-1, s^2) \\
\mu, \sigma^2 |y &\text{ ~ } N(\bar{y}, \frac{\sigma^2}{n}) \times \chi^{-2}(n-1, s^2)
\end{align}

<br> 

##### Posterior Mean의 Marginal Distribution 구하기
번외로, `$\mu$`의 marginal posterior distribution `$p(\mu|y)$`은 `$\int p(\mu,\sigma^2)d\sigma^2$`를 통해서 구할 수 있다. 그리고 형태는 아래와 같다.
$$p(\mu|y) \text{ ~ } t_{n-1}(\bar{y}, \frac{s^2}{n})$$

##### Posterior Prediction 구하는 과정
\begin{align}
p(\tilde{y}|y) &= \int\int p(\tilde{y}|\mu,\sigma^2) p(\mu, \sigma^2|y)\ d\mu \ d\sigma^2 \\
&= \int\int p(\tilde{y}|\mu,\sigma^2) \ p(\mu|\sigma^2,y)\ d\mu \cdot p(\sigma^2|y) \ d\sigma^2 \\ 
&= \int p(\tilde{y}|\sigma^2) \ p(\sigma^2|y) \ d\sigma^2
\end{align}

Posterior Predictive: `$\tilde{y}|y \text{ ~ } t_{n-1}(\bar{y}, (1+\frac{1}{n}s^2))$`
이 결과를 바로 위의 Posterior Mean의 marginal 분포와 비교해보는 것이 중요하다.
왜냐하면 prediction을 할 때에 `$s^2$`, 즉 uncertainty가 추가된다고 해석할 수 있기 때문이다.

Two parameter Normal model이 중요한 이유는 다음 [3. Frequentist와 Bayesian의 차이](/posts/statistics/bayesian/fcb05/#3-frequentist와-bayesian의-차이)을 보면 명확하다. Frequentist와 Bayesian의 기본적인 전제와 입장 차이를 이해한다면, 정보가 없는 prior가 결국 어떠한 결론으로 이어가는지 이해할 수 있다.

#### 2-2. conjugate prior
Prior: `$p(\mu, \sigma^2) = p(\mu|\sigma^2) \times p(\sigma_0^2) \text{ ~ N-Inv-} \chi^2(\mu_0, \frac{\sigma^2}{k_0}; v_0, \sigma_0^2)$`
\begin{align}
\mu|\sigma^2 &\text{ ~ } N(\mu_0, \frac{\sigma^2}{k_0}) \\
\sigma^2 &\text{ ~ } \chi^{-2}(v_0, \sigma^2_0) \\
\rightarrow \mu, \sigma^2 &\propto \sigma^{-1}(\sigma^2)^{-(\frac{v_0}{2}+1)}exp\bigg(\frac{-1}{2\sigma^2}\big[v_0\sigma_0^2 + k_0(\mu_0 - \mu)^2\big]\bigg)
\end{align}
Likelihood: `$p(y|\mu, \sigma^2) \propto \sigma^{-n}exp\bigg(\frac{-1}{2\sigma^2}\sum_{i=1}{n}(y_i-\mu)^2\bigg)$`
Posterior: `$p(\mu, \sigma^2|y) \text{ ~ N-Inv-}\chi^2(\mu_n, \frac{\sigma_n^2}{k_n}; v_n, \sigma_n^2) $`

\begin{align}
\mu_n &= \frac{k_0}{k_0+n}\mu_0 + \frac{n}{k_0+n}\bar{y} \\
k_n &= k_0 +n \\
v_n &= v_o + n \\
v_n\sigma_n^2 &= v_0\sigma_0^2 + (n-1)s^2 + \frac{k_0n}{k_0+n}(\bar{y}-\mu_0)^2 \\
\rightarrow \text{posterior ss}    &= \text{prior ss} + \text{sample ss} + \text{additional uncertainty}(\bar{y}-\mu_0)
\end{align}

#### 3. Frequentist와 Bayesian의 차이
**Frequentist**: `parameter를 알 때, 통계량의 분포`에 대해 이야기한다.
let `$y \text{ ~ } N(\mu, \sigma^2)$`
1. `$\bar{y} \text{ ~ } N(\mu, \frac{\sigma^2}{n}) $`
2. `$\frac{(n-1)s^2}{\sigma^2} \text{ ~ } \chi^2(n-1)$`
3. `$\frac{\bar{y}-\mu}{s/\sqrt{n}}|\mu,\sigma^2 \text{ ~ } t_{n-1}$`

**Bayesian**: `data를 알 때, parameter의 분포`에 대해 이야기한다.
1. `$\mu \text{ ~ } N(\bar{y}, \frac{\sigma^2}{n})$`
2. `$\sigma^2 \text{ ~ } \chi^{-2}(n-1, s^2)$`
3. `$\frac{\mu-\bar{y}}{s/\sqrt{n}}|y \text{ ~ } t_{n-1} $`

만약 Bayesian이 noninformative prior를 가정한다면, 즉 prior가 거의 없다고 생각한다면 frequetist랑 결과가 비슷하게 나오는 것은 당연하다.

#### 4. Multinomial Model
Likelihood: `$y|\theta \text{ ~ Multinomial}(\theta) \propto \prod_{j=1}^{k}\theta_j^{y_j}$`
Prior: `$\theta \text{ ~ } Dir(\alpha) \propto \prod_{j=1}^{k}\theta_j^{\alpha_j-1}$`
Posterior: `$\theta|y \text{ ~ } Dir(\alpha +y) \propto \prod_{j=1}^{k}\theta_j^{\alpha_j-y_j-1}$`

참고로 Multinomial distribution은 이항분포의 확장이며, Dirichlet distribution은 베타분포의 확장이라고 생각하면 쉽다. 왜냐하면 Beta-Binomial 모델에 대해서는 [Chapter3](/posts/statistics/bayesian/fcb03/)에서 이미 충분히 다루었기 때문이다.

---

<br> 
<p style='text-align: center; color:gray'> 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </p>

<br>
<br>