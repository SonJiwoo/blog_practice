<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on JW Blog</title>
    <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/</link>
    <description>Recent content in Bayesian on JW Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <copyright>&amp;copy;{year}, All Rights Reserved</copyright>
    <lastBuildDate>Fri, 08 Jan 2021 10:08:56 +0900</lastBuildDate>
    
        <atom:link href="https://jiwooblog.netlify.app/posts/statistics/bayesian/index.xml" rel="self" type="application/rss+xml" />
    
    
    
      
      <item>
        <title>What is Bayesian</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/</guid>
        <description>Chapter 01. Introduction and Examples 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
이번 장을 통해서는 Likelihood and Prior를 살펴보고 Full probability model의 의미를 보는 데에 주목해보쟈.
베이지안 추론의 목적 우리는 데이터 획득을 통해, 모집단 특성에 대한 불확실성을 줄여나가고자 한다. 이때, 불확실성 정도의 변화 수준을 계량화하는 것이 베이지안 추론통계의 목적이라고 할 수 있다.
핵심 개념  prior distribution $p(\theta)$  사전확률 모수에 대해 기존에 갖고 있던 믿음의 정도   sampling model $p(y|\theta)$  일종의 가능도 함수(likelihood) 사전확률이 참이라는 가정 하에, 특정 데이터가 관찰된 확률   posterior distribution $p(\theta|y)$  데이터가 관찰되었을 때, 이를 바탕으로 수정된 모수에 대한 믿음의 정도    Bayes&#39; Rule $$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{\int_{\Theta}p(y|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}$$</description>
      </item>
      
      <item>
        <title>Exchangeability</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/</guid>
        <description>Chapter 02. Belief, Probability and Exchangeability본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.이번 장의 목표는 independence와 exchangeability를 이해하는 것이다. 이를 바탕으로 de Finetti’s theorem이 Bayesian에 갖는 의의를 이해한다면, 베이즈 통계를 공부할 준비가 된 것이다.
Belief functions and Probabilities$Be()$는 belief function이라고 하자. 예를 들어, $Be(F) &amp;gt; Be(G)$는 G보다 F를 더 믿는다고 해석하면 된다. F, G, H를 아래와 같은 각각의 상황이라고 가정해보자.
F : 좌파 후보자를 투표하는 경우 G : 소득이 하위 10%에 속하는 경우 H : 대도시에 거주하는 경우</description>
      </item>
      
      <item>
        <title>Conjugacy</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/</guid>
        <description>Chapter 03. One-parameter Models 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
Binomial Model Prior: $\theta \text{ ~ } Beta(a,b)$
Likelihood: $Y|\theta \text{ ~ } Binomial(n, \theta) $
Posterior: $\theta|y \text{ ~ } Beta(a+y, b+n-y) $ a: prior 성공횟수, b: prior 실패횟수, $\omega$=a+b: concentration $E[\theta|y] = \frac{a+y}{a+b+n} = \frac{n}{a+b+n}\times\frac{y}{n} + \frac{a+b}{a+b+n}\times\frac{a}{a+b}$ where $\frac{y}{n}$ = sample mean, $\frac{a}{a+b}$ = prior expectation Posterior Predictive
$n^* = 1$일 때 : $\tilde{Y}|y \text{ ~ } Ber(\frac{a+y}{a+b+n})$</description>
      </item>
      
      <item>
        <title>Monte Carlo Method</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/</guid>
        <description>Chapter 04. Monte Carlo Approximation 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
Monte Carlo Method Monte Carlo Method는 이름은 거창해보이지만 사실 그 방법은 매우 간단하다.
우선, 사후분포($p(\theta|y_1,...,y_n)$)로부터 S개의 random sample을 뽑는다.
$$\theta^{1}, &amp;hellip;, \theta^{S} ~ i.i.d p(\theta|y_1, &amp;hellip;, y_n) $$
그러면 S가 커질수록 {$\theta^{1}, ..., \theta^{S}$}는 근사적으로 사후분포($p(\theta|y_1,...,y_n)$)를 따른다.
이를 통해 $E[\theta|y_1, ..., y_n]$, $Var[\theta|y_1, ..., y_n]$부터 중앙값, $\alpha$ percentile 등의 통계량값들을 근사적으로 계산할 수 있다.
이때 approximate Monte Carlo Standard error은 $\sqrt{\hat{\sigma}^2/S}$이다.</description>
      </item>
      
      <item>
        <title>Normal Model</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/</guid>
        <description>Chapter 05. Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods와 Bayesian Data Analysis를 참고하였다.
Warm up! Gamma Distribution Inverse Gamma Distribution Scaled Inverse Chi-squared Distribution Single Parameter Conjugacy unknown ${\sigma}^2$ unknown ${\mu}$ Two Parameter Conjugacy unknown ${\sigma}^2, {\mu}$ semi-conjugacy 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>Gibbs Sampling</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb06/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb06/</guid>
        <description>Chapter 06. Posterior Approximation with the Gibbs sampler 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
1. A Semi-conjugate prior distribution 2. Discrete approximations 3. Sampling from the conditional distributions 4. Gibbs Sampling 5. General properties of the Gibbs sampler 6. Introduction to MCMC diagnostics Conclusion semi-conjugate 분포를 모두 알면 full conditional probability를 아는 것과 거의 같다. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>MVN</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb07/</link>
        <pubDate>Sun, 31 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb07/</guid>
        <description>Chapter 07. The Multivariate Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
1. Multivariate Normal Desity 2. Semiconjugate prior distribution for the mean 3. inverse-Wishart Distribution 4. Gibbs Sampling of the mean and covariance 4-1. Draw yourself Figure 7.1 1 2 3 4 5  library(tidyverse) library(gridExtra) library(MASS) library(reshape2) library(ash)   1 2 3 4 5 6 7 8 9 10 11  # 초기 설정 inv &amp;lt;- solve MU = matrix(c(50,50), ncol=1) SIGMA = matrix(c(64,0,0,144), ncol=2) # MVN pdf calc.</description>
      </item>
      
      <item>
        <title>Bayesian Linear Regression</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb09/</link>
        <pubDate>Thu, 04 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb09/</guid>
        <description>Chapter 09. The Multivariate Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
1. Linear Regression Model 2. Bayesian estimation for a regression model 3. Model Selection 1  library(tidyverse)     Code Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  ## Data load  data = dget(&amp;#39;https://www2.</description>
      </item>
      
      <item>
        <title>Jeffrey&#39;s Prior</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/jeffrey_prior/</link>
        <pubDate>Sun, 07 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/jeffrey_prior/</guid>
        <description>Jeffrey&amp;rsquo;s Prior 1. uninformative prior의 제한점 uninformative prior를 임의로 주게 될 경우, 여러 문제점이 있을 수 있는데 그 중 하나는 변수변환에 취약해질 수 있다는 점이다.
예를 들어, $p(\theta) \propto 1$라고 uninformative prior를 주자. 그리고 $ \phi = exp(\theta)$라고 가정해보자.
\begin{align}p(\phi) &amp;amp;\propto p(\theta) \bigg|\frac{d\theta}{d\phi}\bigg| \\&amp;amp;\propto \frac{1}{\phi} \neq 1\end{align}
변수변환 후에는 prior가 uninformative하지 않게 되어버림을 확인할 수 있다.
2. Jeffrey&amp;rsquo;s prior 그렇다면 어떻게 해야 변수변환에 강건한 prior를 줄 수 있을까?</description>
      </item>
      
      <item>
        <title>Empirical Bayes</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/empricialbayes/</link>
        <pubDate>Sat, 06 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/empricialbayes/</guid>
        <description>Empirical Bayes 베이지안은 기본적으로 prior, 즉 사전확률을 설정한다.
그런데 이때 data density(histogram)에 비슷한 모양을 갖도록 prior를 설정하고자 하는 경우도 있는데, 이를 Empirical Bayes라고 한다.
사전확률인데 왜 데이터를 보고 설정하는지에 대한 의문이 생길 수도 있긴 한다&amp;hellip;
혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>Gibbs Sampler</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/gibbs_sampler/</link>
        <pubDate>Tue, 23 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/gibbs_sampler/</guid>
        <description>Gibbs Sampler 기본 원리 (추후 업데이트)
예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  ## Data load  data = dget(&amp;#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/yX.o2uptake&amp;#39;) y = data[,1] X = data[,-1] inv = solve ## set prior g = length(y) nu0 = 1 s20 = summary(lm(y~-1+X))$sigma^2 n = length(y) p = ncol(X) ## setup S = 1000 set.</description>
      </item>
      
      <item>
        <title>BDA Example</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/</link>
        <pubDate>Wed, 20 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/</guid>
        <description>y &amp;lt;- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114)n &amp;lt;- length(y)s2 &amp;lt;- var(y)my &amp;lt;- mean(y) # helper functions to sample from and evaluate# scaled inverse chi-squared distributionrsinvchisq &amp;lt;- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)dsinvchisq &amp;lt;- function(x, nu, s2){exp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x)}ns &amp;lt;- 1000sigma2 &amp;lt;- rsinvchisq(ns, n-1, s2)mu &amp;lt;- my + sqrt(sigma2/n)*rnorm(length(sigma2))sigma &amp;lt;- sqrt(sigma2)ynew &amp;lt;- rnorm(ns, mu, sigma)t1l &amp;lt;- c(90, 150)t2l &amp;lt;- c(10, 60)nl &amp;lt;- c(50, 185)t1 &amp;lt;- seq(t1l[1], t1l[2], length.</description>
      </item>
      
    
  </channel>
</rss>