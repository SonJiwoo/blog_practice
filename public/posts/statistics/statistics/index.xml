<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>기초통계학 on JW Blog</title>
    <link>https://jiwooblog.netlify.app/posts/statistics/statistics/</link>
    <description>Recent content in 기초통계학 on JW Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <copyright>&amp;copy;{year}, All Rights Reserved</copyright>
    <lastBuildDate>Wed, 20 Jan 2021 10:08:56 +0900</lastBuildDate>
    
        <atom:link href="https://jiwooblog.netlify.app/posts/statistics/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    
      
      <item>
        <title>중심극한정리</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/central_limit_theorem/</link>
        <pubDate>Sun, 21 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/central_limit_theorem/</guid>
        <description>중심극한정리(Central Limit Theorem) $ X_1, ... X_n $을 평균이 $\mu$이고, 분산이 $\sigma^2$인 분포로부터 무작위로 얻은 샘플이라고 하자. (단, $\sigma^2 &amp;lt; \infty$)
그러면 $Y_n = \frac{\sqrt{n}(\bar{X_n} - \mu)}{\sigma}$은 $N(0,1)$을 극한분포로 갖는다.
증명 mgf를 활용하여 증명한다. (추후 추가 예정)</description>
      </item>
      
      <item>
        <title>확률분포</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/probability_distribution/</link>
        <pubDate>Sun, 21 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/probability_distribution/</guid>
        <description>확률분포(Probability Distribution) 출처: https://artificialnetworkforstarters.readthedocs.io/en/latest/_post/chap6.html  -- 연속형(Continuous)  정규 분포 T-분포 감마 분포 베타 분포 카이제곱 분포 F-분포 균일 분포 디리클레 분포 위샤트 분포
&amp;hellip;  이산형(Discrete)  이항 분포 다항 분포 베르누이 분포 포아송 분포 음이항 분포 기하 분포 초기하 분포
&amp;hellip;  정규 분포(Normal Distribution)   정규분포  $$ \text{X~} N(\mu, \sigma^2) \rightarrow f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(x-\mu)}{2\sigma^2}^2) $$
$$ E(X) = \mu, Var(X) = \sigma^2$$   다변수 정규분포(Multivariate Normal Distribution) 관련 포스팅 참고</description>
      </item>
      
      <item>
        <title>MVN</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/mvn/</link>
        <pubDate>Fri, 08 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/mvn/</guid>
        <description>1. Drawing MVN plots with ggplot2 1 2  mu = matrix(c(0,10), ncol=1) invSig = solve(matrix(c(4,10,10,100), ncol=2, byrow=TRUE))   1-1. Vectorize + Outer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  dmvlnorm = function(theta, mu, invSig){ (-nrow(mu)/2) * log(2*pi) + 0.5*log(det(invSig)) - 0.5*(t(theta-mu) %*% invSig %*% (theta-mu)) } calc.dens = Vectorize(function(a,b){ theta = c(a,b) exp(dmvlnorm(theta, mu, invSig)) }) A = seq(-5, 5, length=100) B = seq(-15, 40, length=100) dense = outer(A, B, FUN=calc.</description>
      </item>
      
      <item>
        <title>Exponential Family</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/exponential_family/</link>
        <pubDate>Sun, 07 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/exponential_family/</guid>
        <description>Exponential Family 한국어로는 지수족 또는 지수류라고도 하지만, 영어로 보는 편이 직관적으로 받아들이는 데에 편할 것이다.
$f(x;\theta) = \begin{cases}exp\big[p(\theta)K(x) + s(x) + q(\theta)\big] &amp;amp; x \in S \\0 &amp;amp; o.w\end{cases} $
 S does not depend on $\theta$ $p(\theta)$ is a nontrivial continuous function of $\theta \in \Omega$
3-1. If X is continuous, $K&#39;(x) \neq 0$ and $s(x)$ is continuous function.
3-2. If X is discrete, $K(x)$ is nontrivial function.</description>
      </item>
      
      <item>
        <title>비모수통계학</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/nonparametric/</link>
        <pubDate>Wed, 24 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/nonparametric/</guid>
        <description>비모수통계학 비모수통계학(nonparametric statistics)는 모수적 검정의 가정이 충족되지 못하거나, 데이터 형식이 순서형일 경우처럼 일반적이지 않은 경우에 사용하는 통계적 방법론에 대한 연구를 한다.
t검정에서 독립표본 t-검정과 대응표본 t-검정이 있습니다. 이에 대응하여 비모수통계학에서는 Wilcoxon rank-sum test(Mann-Whitney U-test)와 Wilcoxon signed-rank test가 있습니다.
1. Wilcoxon rank-sum test Mann-Whitney U-test라고도 한다.
독립표본 t-검정의 비모수 버전이다.
2. Wilcoxon signed-rank test 대응표본 t-검정의 비모수 버전이다.
참고 [1] 연세대학교 심리통계 2019-1 수업자료 [2] https://blog.naver.com/istech7/50152096673
[3] http://www.incodom.kr/R%ED%99%9C%EC%9A%A9/Wilcoxon_Signed-Rank_Test
[4] https://dermabae.tistory.com/159</description>
      </item>
      
      <item>
        <title>ANOVA</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/anova/</link>
        <pubDate>Mon, 22 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/anova/</guid>
        <description>ANOVA ANOVA는 Analysis of Variance의 약자로, 한국어로는 분산 분석이라고 한다. 집단 간 분산과 집단 내 분산을 비교하여 처리효과가 있는지 살펴보는 통계방법이다.</description>
      </item>
      
      <item>
        <title>t 검정</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/t_test/</link>
        <pubDate>Mon, 22 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/t_test/</guid>
        <description>t-검정 t-검정은 모집단의 분산이나 표준편차를 알지 못할 때, 모집단을 대표하는 표본으로부터 추정된 분산이나 표준편차를 가지고 검정하는 방법이다.
1. 단일표본 t-검정 모집단의 평균이 특정검정값과 같은지 확인하는 통계빵법이다.
2. 독립표본 t-검정 독립된 두 집단 간 비교하는 방법으로, 서로 다른 두 모집단으로부터 데이터가 추출되었을 때 시행한다.
1 2 3 4 5 6 7 8  # independent t-test x1 = 3.6667 x2 = 6.0667 s1 = 2.60951 s2 = 2.37447 n1 = n2 = 15 sp = sqrt((s1^2*(n1-1) + s2^2*(n2-1)) / ((n1-1) + (n2-1))) t = (x1-x2) / (sp*sqrt(1/n1 + 1/n2))   3.</description>
      </item>
      
      <item>
        <title>신뢰구간, 신용구간</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/interval/</link>
        <pubDate>Sun, 21 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/interval/</guid>
        <description>신뢰구간과 신용구간 간단하게 구분하자면, 신뢰구간은 빈도주의자가, 신용구간은 베이지안이 사용하는 것이다.
일반적으로 신용구간을 신뢰구간으로 착각하는 경우가 많다.
신뢰구간 (Confidence Interval) “If we repeat the experiment infinitely many times, 95% of the experiments will capture the population parameter in their confidence intervals.” 해석하자면, 무수히 많이 반복하여 데이터를 얻고 신뢰구간을 산출한다면, 그 수많은 신뢰구간 중 95%는 모수를 갖고 있을 것으로 신뢰한다는 의미이다. 그렇기 때문에 한번의 실험결과만으로 신뢰구간을 구하고 이를 활용하는 데에는 다소 무리가 있어보인다.</description>
      </item>
      
      <item>
        <title>정규성 검정</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/normality/</link>
        <pubDate>Sun, 21 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/normality/</guid>
        <description>정규성 검정 </description>
      </item>
      
      <item>
        <title>오차와 잔차</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/error_residual/</link>
        <pubDate>Mon, 15 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/error_residual/</guid>
        <description>오차 &amp;amp; 잔차  오차(error): 모집단 회귀식 예측값 - 실제 관측값 잔차(residual): 표본집단 회귀식 예측값 - 실제 관측값  </description>
      </item>
      
      <item>
        <title>편향-분산 Tradeoff</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/bias_variance/</link>
        <pubDate>Wed, 10 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/bias_variance/</guid>
        <description>Bias-Variance Tradeoff $$\text{let } y = f(x) + \varepsilon, \text{ where } \varepsilon \text{ ~ } N(0, \sigma^2)$$
\begin{align}MSE(\hat{y}) &amp;amp;= E[(y-\hat{y})^2] \\&amp;amp;= E[(f(x) + \varepsilon - \hat{f}(x))^2] \\&amp;amp;= E[(f(x)-\hat{f}(x))^2] + E[\varepsilon^2] + 2E[\varepsilon(f(x) - \hat{f}(x))] \\&amp;amp;= E[(f(x) - E(\hat{f}(x)) + E(\hat{f}(x)) - \hat{f}(x) )^2] + \sigma^2 \\&amp;amp;= E[(f(x) - E(\hat{f}(x)))^2] + E[(\hat{f}(x)-E(\hat{f}(x)))^2] + 2(f(x) - E(\hat{f}(x)))E[\hat{f}(x) - E(\hat{f}(x))] + \sigma^2 \\&amp;amp;= {bias[\hat{f}(x)]}^2 + Var(\hat{f}(x)) + \sigma^2 \\&amp;amp;= bias^2(\hat{y}) + Var(\hat{y}) + \sigma^2\end{align}</description>
      </item>
      
      <item>
        <title>F1 Score</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/performance_measure/</link>
        <pubDate>Sun, 07 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/performance_measure/</guid>
        <description>Performance Measure 성능을 평가하는 지표에는 여러 가지가 있다. 그중에서 대표적인 몇 개를 알아보고자 한다.
1. Accuracy $$Accuracy = \frac{\text{correctly predicted}}{\text{all dataset}}$$
Accuracy는 balanced data가 아니라면 좋은 지표로서의 역할을 하기 힘들다. 왜냐하면 A,B,C,D라는 그룹이 있을 때, B~D가 각각 10개의 케이스 밖에 없고 A가 혼자서 500개의 케이스가 있다고 가정한다면 Accuracy 지표는 A 그룹에 의해 좌지우지 될 것이기 때문이다.
2. Precision  Given a class prediction from the classifier, how likely is to be correct?</description>
      </item>
      
      <item>
        <title>Fisher Information</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/fisher_information/</link>
        <pubDate>Sun, 07 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/fisher_information/</guid>
        <description>Fisher Information Fisher information은 score function의 분산이다. 이 점을 명심해서 의미를 생각해보자. 아래는 Fisher information을 다양한 꼴로 표현해본 것이다.
\begin{align}I(\theta;x) &amp;amp;= Var(\frac{\partial}{\partial\theta}logf(x;\theta)) \\&amp;amp;= E\bigg[\Big(\frac{\partial}{\partial\theta}logf(x;\theta)\Big)^2\bigg] \\&amp;amp;= -E\bigg[\frac{\partial^2}{\partial\theta^2}log(f(x;\theta)\bigg]\end{align}
계산 증명 위에 대한 자세한 계산 증명은 아래와 같다.
우선, score function의 평균이 0임을 구하는 것부터 시작한다.
$$\int \frac{\partial}{\partial\theta}logf(x;\theta)f(x;\theta)=0 $$
양변을 $\theta$로 미분해준다.
$$\int\frac{\partial^2}{\partial\theta^2}logf(x;\theta)f(x;\theta)dx + \int\frac{\partial}{\partial\theta}logf(x;\theta)f&#39;(x;\theta)dx = 0 $$
오른쪽 부분을 우변으로 이항하여 정리해주면 아래와 같다.
\begin{align}E\bigg[\frac{\partial^2}{\partial\theta^2}logf(x;\theta)\bigg] &amp;amp;= -\int\frac{\partial}{\partial\theta}logf(x;\theta)f&#39;(x;\theta)dx \\ &amp;amp;= -\int\frac{\partial}{\partial\theta}logf(x;\theta)\frac{f&#39;(x;\theta)}{f(x;\theta)}f(x;\theta)dx \\&amp;amp;= -\int\bigg(\frac{\partial}{\partial\theta}logf(x;\theta)\bigg)^2f(x;\theta)dx \\&amp;amp;= -E\bigg[\Big(\frac{\partial}{\partial\theta}logf(x;\theta)\Big)^2\bigg]\end{align}</description>
      </item>
      
      <item>
        <title>Score Function</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/statistics/score_function/</link>
        <pubDate>Sun, 07 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/statistics/score_function/</guid>
        <description>Score Function $X \text{ ~ } f(x;\theta)$일 때, score function $s(\theta;x)$은 다음과 같이 정의한다.
$$\frac{\partial}{\partial\theta}logf(x;\theta) $$
평균 계산 증명 \begin{align}E\big[s(\theta;x) \big] &amp;amp;= \int\bigg[\frac{\partial}{\partial\theta}logf(x;\theta)\bigg]f(x;\theta)dx \\&amp;amp;= \int\frac{f&#39;(x;\theta)}{f(x;\theta)}f(x;\theta)dx \\&amp;amp;= \int f&#39;(x;\theta)dx \\&amp;amp;= \frac{\partial}{\partial\theta}\int f(x;\theta)dx \\&amp;amp;= \frac{\partial}{\partial\theta}1 = 0\end{align}
의미 score function은 log likelihood의 기울기를 나타낸다는 데에서 의미가 있다.
score function의 평균이 0이라는 것의 의미를 그래프를 likelihood 그래프를 상상하여 생각해보자.</description>
      </item>
      
    
  </channel>
</rss>