<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on JW Blog</title>
    <link>https://jiwooblog.netlify.app/posts/machinelearning/</link>
    <description>Recent content in Machine Learning on JW Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <copyright>&amp;copy;{year}, All Rights Reserved</copyright>
    <lastBuildDate>Thu, 07 Jan 2021 10:08:56 +0900</lastBuildDate>
    
        <atom:link href="https://jiwooblog.netlify.app/posts/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    
      
      <item>
        <title>ML Intro</title>
        <link>https://jiwooblog.netlify.app/posts/machinelearning/intro/</link>
        <pubDate>Thu, 11 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/machinelearning/intro/</guid>
        <description>Machine Learning  주어진 데이터를 통해서 입력변수와 출력변수 간의 관계를 만드는 함수 $f$를 만드는 것 주어진 데이터 속에서 데이터의 특징을 찾아내는 함수 $f$를 만드는 것  1. 기본 개념구분  지도 학습: 회귀(Regression), 분류(Classification) 비지도 학습: PCA, 군집분석 강화 학습: 수많은 시뮬레이션을 통해 현재의 선택이 먼 미래에 보상이 최대로 하는 action을 학습  2. 다양한 머신러닝 기법  선형회귀분석: 선형관계를 가정하여, 독립변수의 중요도와 영향력 파악 DT(Decision Tree): 독립변수의 조건에 따라 종속변수를 분리 KNN(K-Nearest Neighbor): 새로 들어온 데이터의 주변 K개의 데이터의 class로 분류 NN(Neural Network): 입력층/은닉층/출력층 으로 구성된 모형.</description>
      </item>
      
      <item>
        <title>Entropy, KL-Divergence</title>
        <link>https://jiwooblog.netlify.app/posts/machinelearning/entorpy_kld/</link>
        <pubDate>Thu, 11 Mar 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/machinelearning/entorpy_kld/</guid>
        <description>Entropy정보량 = 불확실성\[\begin{align}H(p) &amp;amp;= \sum_{i=1}p_i log\frac{1}{p_i} \\&amp;amp;= -\sum_{i=1}p_i log(p_i)\end{align}\]
여기서 $\frac{1}{p_i}$는 발생확률의 역수로, 다르게 보면 가능한 결과의 수라고 볼 수 있다.
그렇기 때문에 $log\frac{1}{p_i}$는 필요한 질문의 수라고 생각할 수 있다.
합쳐서 생각해보면, 정보량이라고 하는 것은 필요한 질문의 수 x 확률의 총합이라고 생각할 수 있다.
Cross Entropyp에 대해, 전략 Q를 사용했을 때의 불확실성즉, 특정 전략을 쓸 때, 예상되는 질문개수에 대한 기댓값</description>
      </item>
      
    
  </channel>
</rss>