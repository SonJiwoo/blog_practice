[{"content":"Intro 생키 차트(Sankey Chart)는 흐름(Flow)을 보여주기에 최적화된 차트 형태이다. 예를 들어 지역이나 국가 간의 에너지를 표현하는데에 적합하다.\n  기본 전처리 코드(R)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  library(tidyverse) library(xlsx) setwd(\u0026#39;C:/Users/bunga/Desktop/Tableau/data\u0026#39;) data \u0026lt;- read.table(\u0026#39;seoul_park_raw.txt\u0026#39;, header=TRUE, sep=\u0026#39;\\t\u0026#39;) data \u0026lt;- as.tibble(data) data \u0026lt;- data %\u0026gt;% mutate(ym = (ym*100)%%100) data \u0026lt;- data %\u0026gt;% select(-total) data \u0026lt;- data[data[\u0026#39;region\u0026#39;] != \u0026#39;합계\u0026#39;,] data \u0026lt;- data %\u0026gt;% mutate(ord = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, ord)), health = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, health)), bicycle = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, bicycle)), event = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, event)), special = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, special)), etc = as.numeric(gsub(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;, etc))) %\u0026gt;% mutate_all(~ifelse(is.na(.), 0, .)) write.table(data, file = \u0026#34;seoul_park.txt\u0026#34;, sep = \u0026#34;\\t\u0026#34;, row.names = FALSE) write.xlsx(data,file=\u0026#39;seoul_park.xlsx\u0026#39;, sheetName = \u0026#39;Sheet1\u0026#39;)      Reference [1] WeViz 유튜브\n[2] https://qliksense.tistory.com/32\n[3] 서울열린데이터광장\n","description":"","id":0,"section":"posts","tags":null,"title":"생키 차트","uri":"https://jiwooblog.netlify.app/posts/tableau/sankey_chart/"},{"content":"Intro 본 포스팅은 WeViz 유튜브를 적극 참고하였습니다.\n결과물  1. 팔레트 커스텀 추가 내 태블로 리포지토리 Preferences 파일에 색상 커스텀 추가\n본 결과물에는 해당 참고사이트에서 추천한 색상 조합을 적용하였다.\n2. 색상 참고 사이트 2-1. UI gradients (측정값 그라데이션) UI gradients\n2-2. Adobe Color (차원 색조합) Adobe color\nReference [1] WeViz 유튜브\n","description":"","id":1,"section":"posts","tags":null,"title":"한국 픽셀맵","uri":"https://jiwooblog.netlify.app/posts/tableau/korea_pixel_map/"},{"content":"Intro 본 포스팅은 WeViz 유튜브를 적극 참고하였습니다.\n결과물  1. 기본 수정  [맵] - [맵 계층] 스타일 어둡게 변경 기본도, 토지 피복도 체크박스 해제 해안선 체크  2. mapbox 이용하기  mapbox 홈페이지 Studio -\u0026gt; New Style -\u0026gt; Basic(다른 스타일 선택해도 무방)  2-1. 영역 색깔 바꾸기 특정 영역 클릭 후 색깔 변경\n2-2. 폰트 바꾸기 STEP1. 텍스트는 기본적으로 자물쇠 잠금 해제(override) 클릭 후 변경 가능\nSTEP2. 특정 레이블을 선택 -\u0026gt; Components -\u0026gt; Typography\nSTEP3. 원하는 글꼴 검색 후 적용(Noto sans 추천)\n2-3. 영어 레이블을 한글 레이블로 바꾸기 STEP1. 특정 레이블 클릭 -\u0026gt; Layers -\u0026gt; T-country label 클릭\nSTEP2. override를 클릭하여 name_en을 name_ko로 바꿔준다.\n2-4. 배포하기 오른쪽 상단 Publish 클릭 -\u0026gt; Publish as new\n2-5. Tableau로 가져오기 STEP1. mapbox preview only 링크 복사\nSTEP2. Tableau 맵 관리 -\u0026gt; 추가\nSTEP3. 내보내기 꼭 하기! (다음에도 활용하기 위해서!)\nReference [1] WeViz 유튜브\n","description":"","id":2,"section":"posts","tags":null,"title":"맵 커스텀","uri":"https://jiwooblog.netlify.app/posts/tableau/map_custom/"},{"content":"Intro 본 포스팅은 WeViz 유튜브를 적극 참고하였습니다.\n시각화에 관심이 있었는데, 해당 연합동아리 소속의 친구에게 추천을 받아서 참고하게 되었습니다.\n결과물  배운점   구글 스프레드시트 GEOCODE 활용법\n 경도(longitude), 위도(latitude) 자동 추출 기능    MAKELINE, MAKEPOINT 활용법\nMAKELINE(MAKEPOINT(30.602101,114.316826), MAKEPOINT([Latitude],[Longitude]))\n  맵 관리\n 커스텀 맵 디자인 추가    Reference [1] WeViz 유튜브\n[2] 데이터 출처\n","description":"","id":3,"section":"posts","tags":null,"title":"코로나","uri":"https://jiwooblog.netlify.app/posts/tableau/corona_map/"},{"content":"esquisse esquisse는 드래그 앤 드롭(drag \u0026amp; drop)으로 ggplot을 간단하게 그릴 수 있는 획기적인 패키지이다.\n복잡한 커스터마이징은 디테일한 수정이 추가적으로 필요하겠지만, 간단한 특징들을 반복적인 코드수정과 확인과정을 거치기 않고서도 즉각적으로 그래프 모양을 확인할 수 있다는 큰 장점이 있다.\n거의 Tableu 같은 느낌도 든다. 간단한 ggplot 그릴 때 또는 ggplot 입문자가 먼저 거쳐가도 좋을 것 같다.\n1 2 3  library(ggplot2) library(dplyr) library(esquisse)   STEP1. Addins을 클릭하고, ggplot2 builder를 이어서 클릭한다.\n\nSTEP2. validate imported data를 클릭한다.\n\nSTEP3. 드래그 앤 드롭으로 X축, Y축 등을 설정한다.\n\nSTEP4-1. Labels \u0026amp; Title에서는 제목과 축 이름 등을 설정한다.\n\nSTEP4-2. Plot Options에서는 색깔을 포함한 전반적인 테마를 설정한다.\n\nSTEP4-4. Data에서는 표현하고픈 또는 표현하고 싶지 않은 데이터를 필터링한다.\n\nSTEP4-3. Export \u0026amp; Data에서는 Insert code in script를 클릭하면 아래와 같은 코드를 바로 script로 옮겨준다.\n\nSTEP5. 친절하게 라이브러리까지 제시된 코드를 실행한다.\nSTEP6. 추가적으로 더 손 보고 싶은 부분을 개선한다.\n  코드 예시  1 2 3 4 5 6 7 8 9 10 11 12  library(dplyr) library(ggplot2) mpg %\u0026gt;% filter(displ \u0026gt;= 1.6 \u0026amp; displ \u0026lt;= 5.95) %\u0026gt;% filter(year \u0026gt;= 1999 \u0026amp; year \u0026lt;= 2005.2) %\u0026gt;% filter(hwy \u0026gt;= 12L \u0026amp; hwy \u0026lt;= 33L) %\u0026gt;% ggplot() + aes(x = manufacturer, y = cyl) + geom_boxplot(fill = \u0026#34;#18c975\u0026#34;) + scale_y_continuous(trans = \u0026#34;log10\u0026#34;) + theme_gray()     참고 [1] https://www.youtube.com/watch?v=FWLxE-ARuO8\n","description":"","id":4,"section":"posts","tags":null,"title":"esquisse","uri":"https://jiwooblog.netlify.app/posts/r/%EC%8B%9C%EA%B0%81%ED%99%94/esquisse/"},{"content":"naniar 패키지 훑어보기 NA 관련해서 직관적으로 깔끔한 그래프로 훑어볼 수 있게 도와주는 패키지이다.\n본 포스팅은 해당 사이트를 적극참고하여 작성하였다.\n1 2  library(tidyverse) library(naniar)   vis_miss 1  vis_miss(airquality)   gg_miss_var 1  gg_miss_var(airquality)   1  gg_miss_var(airquality, show_pct = TRUE)   1  gg_miss_var(airquality, facet = Month)   gg_miss_case 1  gg_miss_case(airquality)   gg_miss_upset 1  gg_miss_upset(riskfactors)   1  n_var_miss(riskfactors)   ## [1] 24\r1  gg_miss_upset(riskfactors, nsets = n_var_miss(riskfactors))   1  gg_miss_upset(riskfactors, nsets = 4) #nset: 변수 개수   1  gg_miss_upset(riskfactors, nsets = 10, nintersects = 5) #nintersects: 변수조합 수   geom_miss_point ggplot과 응용\n1 2  ggplot(airquality, aes(x = Ozone, y = Solar.R)) + geom_point()   ## Warning: Removed 42 rows containing missing values (geom_point).\r1 2  ggplot(airquality, aes(x = Ozone, y = Solar.R)) + geom_miss_point()   gg_miss_fctfas 1  gg_miss_fct(oceanbuoys, year)   miss_var_summary 1 2 3  riskfactors %\u0026gt;% group_by(marital) %\u0026gt;% miss_var_summary()   ## # A tibble: 231 x 4\r## # Groups: marital [7]\r## marital variable n_miss pct_miss\r## \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Married smoke_stop 120 91.6 ## 2 Married pregnant 117 89.3 ## 3 Married smoke_last 84 64.1 ## 4 Married smoke_days 73 55.7 ## 5 Married drink_average 68 51.9 ## 6 Married health_poor 67 51.1 ## 7 Married drink_days 67 51.1 ## 8 Married weight_lbs 6 4.58\r## 9 Married bmi 6 4.58\r## 10 Married diet_fruit 4 3.05\r## # ... with 221 more rows\rmiss_var_span, gg_miss_span 1  miss_var_span(pedestrian, hourly_counts, span_every = 3000)   ## # A tibble: 13 x 5\r## span_counter n_miss n_complete prop_miss prop_complete\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 0 3000 0 1 ## 2 2 0 3000 0 1 ## 3 3 1 2999 0.000333 1.00 ## 4 4 121 2879 0.0403 0.960\r## 5 5 503 2497 0.168 0.832\r## 6 6 555 2445 0.185 0.815\r## 7 7 190 2810 0.0633 0.937\r## 8 8 0 3000 0 1 ## 9 9 1 2999 0.000333 1.00 ## 10 10 0 3000 0 1 ## 11 11 0 3000 0 1 ## 12 12 745 2255 0.248 0.752\r## 13 13 432 2568 0.144 0.856\r1  gg_miss_span(pedestrian, hourly_counts, span_every = 3000)   1  gg_miss_span(pedestrian, hourly_counts, span_every = 3000, facet = sensor_name)   그외 다양한 1  gg_miss_case_cumsum(airquality)   1  gg_miss_var_cumsum(airquality)   1  gg_miss_which(airquality)   ","description":"","id":5,"section":"posts","tags":null,"title":"naniar","uri":"https://jiwooblog.netlify.app/posts/r/%EC%8B%9C%EA%B0%81%ED%99%94/naniar/"},{"content":"Machine Learning  주어진 데이터를 통해서 입력변수와 출력변수 간의 관계를 만드는 함수 $f$를 만드는 것 주어진 데이터 속에서 데이터의 특징을 찾아내는 함수 $f$를 만드는 것  1. 기본 개념구분  지도 학습: 회귀(Regression), 분류(Classification) 비지도 학습: PCA, 군집분석 강화 학습: 수많은 시뮬레이션을 통해 현재의 선택이 먼 미래에 보상이 최대로 하는 action을 학습  2. 다양한 머신러닝 기법  선형회귀분석: 선형관계를 가정하여, 독립변수의 중요도와 영향력 파악 DT(Decision Tree): 독립변수의 조건에 따라 종속변수를 분리 KNN(K-Nearest Neighbor): 새로 들어온 데이터의 주변 K개의 데이터의 class로 분류 NN(Neural Network): 입력층/은닉층/출력층 으로 구성된 모형. 각 층을 연결하는 노드의 가중치를 업데이트하며 학습 SVM(Support Vector Machine): class 간 거리가 최대가 되도록 decision boundary 만드는 방법 K-means Clustering: Label 없이 데이터의 군집 k개 생성 Ensemble Learning: 여러 개의 모델을 결합하여 사용하는 모델로, 구체적으로는 다양한 알고리즘 종류가 있다.\n7-1. Bagging: 모델을 다양하게 만들기 위해 데이터를 재구성\n7-2. Random Forest: 모델을 다양하게 만들기 위해 데이터뿐만 아니라 변수도 재구성\n7-3. Boosting: 맞추기 어려운 데이터에 대해 좀 더 가중치를 두어 seqeuntial하게 학습하는 개념 (ex. AdaBoost, Gradient Boosting(Xgboost, LightGBM, CatBoost)\n7-4. Stacking: 모델의 output값을 새로운 독립변수로 활용 Deep Learning: 딥러닝은 사실 머신러닝의 부분집합이다. 하지만 워낙 깊고 다양하기에 따로 다루도록 하겠다.  3. 모형의 적합성 평가 및 실험설계 데이터를 Training-Validation-Test, 총 세 가지 세트로 나눈다.\nK-Fold Cross Validation 데이터를 k개 부분으로 나누 뒤, 하나를 검증집합 나머지를 학습집합으로 한다. 이 과정을 k번 반복해서 k개의 성능지표를 구하고 그것들의 평균을 구한다.\nLOOCV(Leave One Out Cross Validation) 데이터를 k개의 부분으로 나누기에 부족할 때, 데이터 한 개씩을 빼가면서 K-fold CV를 하는 방식과 똑같이 한다.\n4. 과적합(Overfitting) 머신러닝에서 가장 주의해야 할 것 중 하나가 바로 과적합이다. 이와 관련해서는 Bias-Variance Tradeoff에 대한 이해가 필요하다. 그리고 아주 간단하게 이해하기 위해서는 아래 두 사진을 참고하면 될 것이다.\n참고 [1] https://medium.com/@cs.sabaribalaji/overfitting-6c1cd9af589\n[2] https://www.researchgate.net/figure/The-overfitting-of-model-a-training-error-and-true-error-b-depiction-of-Eq-33_fig5_333505702\n","description":"머신러닝 개괄","id":6,"section":"posts","tags":null,"title":"ML Intro","uri":"https://jiwooblog.netlify.app/posts/machinelearning/intro/"},{"content":"중심극한정리(Central Limit Theorem) $ X_1, ... X_n $을 평균이 $\\mu$이고, 분산이 $\\sigma^2$인 분포로부터 무작위로 얻은 샘플이라고 하자. (단, $\\sigma^2 \u0026lt; \\infty$)\n그러면 $Y_n = \\frac{\\sqrt{n}(\\bar{X_n} - \\mu)}{\\sigma}$은 $N(0,1)$을 극한분포로 갖는다.\n증명 mgf를 활용하여 증명한다. (추후 추가 예정)\n","description":"","id":7,"section":"posts","tags":null,"title":"중심극한정리","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/central_limit_theorem/"},{"content":"확률분포(Probability Distribution) 출처: https://artificialnetworkforstarters.readthedocs.io/en/latest/_post/chap6.html  -- 연속형(Continuous)  정규 분포 T-분포 감마 분포 베타 분포 카이제곱 분포 F-분포 균일 분포 디리클레 분포 위샤트 분포\n\u0026hellip;  이산형(Discrete)  이항 분포 다항 분포 베르누이 분포 포아송 분포 음이항 분포 기하 분포 초기하 분포\n\u0026hellip;  정규 분포(Normal Distribution)   정규분포  $$ \\text{X~} N(\\mu, \\sigma^2) \\rightarrow f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{(x-\\mu)}{2\\sigma^2}^2) $$\n$$ E(X) = \\mu, Var(X) = \\sigma^2$$   다변수 정규분포(Multivariate Normal Distribution) 관련 포스팅 참고\nT-분포(Student\u0026rsquo;s t-Distribution)   T-분포  $$ \\text{X~} t(n) \\rightarrow f(x) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\Gamma(\\frac{n}{2})\\cdot\\sqrt{\\pi n}}(\\frac{n}{x^2+n})^\\frac{n+1}{2} \\ \\text{ for } -\\infty\u0026lt;x\u0026lt;\\infty$$\n$$ E(X) = 0, Var(X) = \\frac{n}{n-2} $$\nT분포는 기본적으로 통계검정을 하기 위해서 작위적으로 고안된 분포이다.\nT분포의 탄생 과정은 아래와 같다.\n$$ T = \\frac{Z}{\\sqrt{\\frac{V}{\\nu}}} \\text{~ } t(df)$$\nwhere $Z\\text{~ }N(0,1), V\\text{~ } \\chi^2(\\nu)$\n  이항 분포(Binomial Distribution)   이항분포  $$ \\text{X~} Binom(n, p) \\rightarrow f(x) = \\binom{n}{x}p^x(1-p)^{n-x} $$\n$$ E(X) = np, Var(X) = np(1-p) $$   음이항 분포(Negative Binomial Distribution) n번째 시행에서 r번째 성공을 확률을 구하고자 할 때 활용한다.\n음이항 분포에서는 시행 횟수 n이 확률변수이고 성공 횟수 r이 고정되어 있는 반면, 이항분포에서는 시행 횟수 n이 고정되어 있고 성공 횟수 r이 확률변수이다. 그래서 음이항분포라고 이름지어진 것이다.\n  음이항 분포(X : r번째 성공을 얻을 때까지 시행횟수)  $$\\text{X~} Negative \\ Binomial(r, p) $$\n$$\\rightarrow f(x) = \\binom{x-1}{r-1}p^r(1-p)^{x-r}, \\ x=r, r+1, \u0026hellip; $$\n$$ E(X) = \\frac{r}{p}, \\ Var(X) = r \\cdot\\frac{1-p}{p^2}$$  \n여기서 x = r+y로 하여, r번째 성공을 얻을 때까지 실패횟수를 계산하면 아래와 같다.\n해당 확률분포 형태에 따라 음이항분포라고 이름지어진 것임을 다시 한 번 확인할 수 있다.\n  음이항 분포(Y : r번째 성공을 얻을 때까지 실패횟수)  $$\\text{Y~} Negative \\ Binomial(r, p) $$\n$$\\rightarrow f(y) = \\binom{r+y-1}{r-1}p^r(1-p)^y, \\ y=0,1,2,\u0026hellip;$$\n$$\\rightarrow f(y) = (-1)^y\\binom{-r}{y}p^r(1-p)^y, \\ y=0,1,2,\u0026hellip; $$\n$$ E(Y) = r\\cdot\\frac{1-p}{p}, \\ Var(Y) = r \\cdot\\frac{1-p}{p^2}$$  \n감마 분포(Gamma Distribution)   감마분포 (알파: shape, 베타: scale)  $$ \\text{X~} Gamma(\\alpha, \\beta) \\rightarrow f(x) = \\frac{1}{\\beta^\\alpha\\cdot\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\frac{x}{\\beta}}$$\n$\\text{for } x\u0026gt;0, \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X)=\\alpha\\beta, \\ Var(X)=\\alpha\\beta^2 $$  --  Gamma Distribution: k=alpha, theta=beta로 생각하기  -- 참고로, 포아송분포, 지수분포, 카이제곱분포와의 연관성을 생각하면 베타를 rate parameter로 보는 것이 좋다.\n베타를 scale로 보는 방식은 가려두도록 하겠다.\n  감마분포 (알파: shape, 베타: rate)  $$ \\text{X~} Gamma(\\alpha, \\beta) \\rightarrow f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}$$\n$\\text{for } x\u0026gt;0, \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X)=\\frac{\\alpha}{\\beta}, \\ Var(X)=\\frac{\\alpha}{\\beta^2} $$  \n  역감마 분포 (inverse-Gamma)  $$ \\text{X~} inv-Gamma(\\alpha, \\beta) \\rightarrow f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\frac{1}{x}\\right)^{\\alpha+1}exp(-\\frac{\\beta}{x}) $$\n$\\text{for } x\u0026gt;0, \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X)=\\frac{\\beta}{\\alpha-1}, \\ Var(X)=\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)} \\ \\text{for } \\alpha\u0026gt;1$$     스케일된 역감마분포 (scaled inverse-Gamma)  $$\\text{X~} \\chi^{-2}(\\nu, \\tau^2) = \\Gamma^{-1}(\\nu/2, \\nu\\tau^2/2) $$\n$$\\rightarrow f(x) = \\frac{(\\nu\\tau^2/2)^{\\nu/2}}{\\Gamma(\\nu/2)}\\left(\\frac{1}{x}\\right)^{\\nu/2+1}exp(-\\frac{\\nu\\tau^2}{2x}) $$   베타 분포(Beta Distribution)   베타 분포  $$ \\text{X~} Beta(\\alpha, \\beta) \\rightarrow f(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} $$\n$\\text{for } x\\in[0,1], \\ \\alpha\u0026gt;0, \\ \\beta\u0026gt;0 $\n$$ E(X) = \\frac{\\alpha}{\\alpha+\\beta}, \\ Var(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}, \\ mode(X) = \\frac{\\alpha-1}{\\alpha+\\beta-2} (\\text{단}, \\alpha\u0026gt;1, \\ \\beta\u0026gt;1) $$   디리클레 분포(Dirichlet Distribution) 디리클레 분포는 베타분포의 다변량 버전이다.\n  디리클레 분포  $$\\theta \\text{ ~ } Dirichlet(\\alpha) \\rightarrow p(\\theta) = \\frac{1}{B(\\alpha)}\\prod_{j=1}^{k}\\theta_j^{\\alpha_j-1} $$\n$$\\text{for } B(\\alpha) = \\frac{\\prod\\Gamma(\\alpha_j)}{\\Gamma(\\sum \\alpha_j)}, \\sum_{j=1}^{k}\\theta_j=1 $$  \n포아송 분포(Poisson Distribution) 정해진 시간 안에 어떤 사건이 일어날 횟수에 대한 기댓값을 $\\lambda$ 라고 했을 때, 그 사건이 n회 일어날 확률은 다음과 같다.\n  포아송 분포  $$ \\text{X~} Pois(\\lambda) \\rightarrow f(x) = \\frac{\\lambda^x e^\\lambda}{x!}$$\nfor $x$: 0이상의 정수, $\\lambda\u0026gt;0$\n$$ E(X) = Var(X) = \\lambda $$  \n\n지수 분포(Exponential Distribution) 사건이 서로 독립적일 때, 일정 시간동안 발생하는 사건의 횟수가 푸아송 분포를 따른다면, 다음 사건이 일어날 때까지 대기 시간 또는 사건이 한 번 일어날 때까지 걸리는 시간은 지수분포를 따른다. 지수분포는 감마분포의 특수한 형태이다.\n  지수 분포  $$\\text{X~} exp(\\lambda) \\rightarrow f(x) = \\lambda e^{\\lambda x} $$\n$exp(\\lambda) = \\Gamma(1,\\lambda) $ where $\\beta$위치의 모수가 rate parameter를 뜻할 때!\n$$ E(X) = \\frac{1}{\\lambda}, \\ Var(X)=\\frac{1}{\\lambda^2} $$  \n\n카이제곱 분포(Chi-squared Distribution) $\\nu$개의 서로 독립적인 표준정규분포 확률변수를 각각 제곱한 다음 합해서 얻어지는 분포이다.\n이때 $\\nu$를 자유도라고 하며, 카이제곱 분포의 매개변수가 된다.\n  카이제곱 분포  $$\\text{X~} \\chi^2(\\nu) \\rightarrow f(x) = \\frac{(\\frac{1}{2})^\\frac{\\nu}{2}}{\\Gamma(\\frac{\\nu}{2})}x^{\\frac{\\nu}{2}-1}e^{-x/2} $$\n$\\chi^2(\\nu) = \\Gamma(\\frac{\\nu}{2}, \\frac{1}{2}) $ where $\\beta$위치의 모수가 rate parameter를 뜻할 때!\n$$ E(X) = \\nu, \\ Var(X) = 2\\nu$$  \n\n  역카이제곱 분포 (inverse Chi-squared)  $$\\text{X~} \\chi^{-2}(\\nu) = \\Gamma^{-1}(\\nu/2, 1/2) $$\n$$\\rightarrow f(x) = \\frac{(1/2)^{\\nu/2}}{\\Gamma(\\nu/2)}\\left(\\frac{1}{x}\\right)^{\\nu/2+1}exp(-\\frac{1}{2x}) $$     스케일된 역카이제곱분포 (scaled inverse chi-squard)  $$\\text{X~} \\chi^{-2}(\\nu, \\tau^2) = \\Gamma^{-1}(\\nu/2, \\nu\\tau^2/2) $$\n$$\\rightarrow f(x) = \\frac{(\\nu\\tau^2/2)^{\\nu/2}}{\\Gamma(\\nu/2)}\\left(\\frac{1}{x}\\right)^{\\nu/2+1}exp(-\\frac{\\nu\\tau^2}{2x}) $$   라플라스 분포(Laplace Distribution) 지수분포를 두 개 붙여놓은 것 같다고 하여 double-exponential distribution이라고도 불린다.\n  라플라스 분포 (Laplcace Distribution)  $$\\text{X~} Laplace(\\mu, b) \\rightarrow f(x) = \\frac{1}{2b}exp(-\\frac{|x-\\mu|}{b}) $$\n$$ E(X) = \\mu, Var(X) = 2b^2 $$  \n\n사진 출처 [1] https://artificialnetworkforstarters.readthedocs.io/en/latest/_post/chap6.html\n[2] 위키백과\n","description":"","id":8,"section":"posts","tags":null,"title":"확률분포","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/probability_distribution/"},{"content":"Index  통계학이란?  표본분포 모수 검정 \u0026amp; 비모수 검정   기술통계  데이터 유형 분포의 이해 CLT   가설검정  양방향 \u0026amp; 단방향   정규성 검정  Kolmogorov-Smirnov test(표본 50개 이상) Shapiro-Wilk test(표본 50개 이하) 모수적 접근 \u0026amp; 비모수적 접근   단일집단 평균  모수적 접근(one sample t-test) 비모수적 접근(Kolmogorov-Smirnov test, Runs 검정)   독립 두 집단 평균  모수적 접근(independent t-test) 비모수적 접근(Mann-Whitney U-test, Wilcoxon rank-sum test, Kolmogorov-Smirnov test, 중앙값 검정)   대응 두 집단 평균  모수적 접근(paired t-test) 비모수적 접근(sign test, Wilcoxon signed rank test)   독립 세 집단 이상 평균  모수적 접근(ANOVA) 비모수적 접근(Kruskal-Wallis test) 사후분석(Bonferroni)   종속 세 집단 이상 평균  비모수적 접근(Cochran Q test, Friedman test)   카이제곱검정(범주형 데이터)  독립성 검정 적합도 검정 동질성 검정   상관분석  모수적 접근 비모수적 접근(Spearman 상관계수, Kendall 순위 상관계수, Kendall 편 순위 상관계수, Kendall 일치도 계수)    ","description":"","id":9,"section":"posts","tags":null,"title":"개요","uri":"https://jiwooblog.netlify.app/posts/spss/prologue/"},{"content":"미분(Differentiation) 1. Chain Rule $$\\frac{dy}{dx} = \\frac{dy}{du} \\times \\frac{du}{dx}$$\n2. Product Rule $$\\frac{dy}{dx} = \\frac{du}{dx}v + u\\frac{dv}{dx}$$\n이를 다르게 쓰면,\n$$y = f(x)g(x)$$\n$$\\rightarrow y'=f'(x)g(x)+f(x)g'(x)$$\n3. Quotient Rule $$y = \\frac{f(x)}{g(x)}$$\n$$\\rightarrow y' = \\frac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}$$\n4. Implicit Differentiation $$\\frac{d}{dx}[f(y)] = \\frac{d}{dy}[f(y)] \\times \\frac{dy}{dx} $$\n","description":"","id":10,"section":"posts","tags":null,"title":"미분","uri":"https://jiwooblog.netlify.app/posts/statistics/calculus/1_differentiation/"},{"content":"1. 추천 사이트  pandas_exercises  2. 추천 책  Python for Data Analysis (Wes McKinney)  3. 검색 팁 구글링 시, 뒤에 \u0026lsquo;towards data science\u0026rsquo; 또는 \u0026lsquo;medium\u0026rsquo; 붙이기\n하루 열람 제한이 있는데, 더 읽고 싶은 경우는 Chrome 시크릿 모드를 활용하면 제한이 풀린다.\n","description":"","id":11,"section":"posts","tags":null,"title":"Python 꿀팁","uri":"https://jiwooblog.netlify.app/posts/python/python_tip/"},{"content":"To Do List 1. Want to Upload Posts   프로젝트들 정리\n Rhino: 수소차 충전소 입지 선정 [ESC 2020_SUMMER] 빅콘테스트 NS Shop+ 심리성향 예측 AI 경진대회 [ESC 2020_FALL] NLP 논문 따라해보기 [ESC 2020_SPRING] 행복지수 예측\n- 배운 점: 해석 시 Domain Knowledge의 중요성, FA의 효과    R 다양한 기능들 정리\n dpylr: group_by, summarise, mutate, select, filter 시각화\n2-1. ggplot2\n2-2. plotly\n2-3. highcharter\n2-4. g2r\n2-5. rayshader\n2-6. ggmap\n2-7. r4issactoast\n2-8. naniar(gg_miss_upset)\n2-9. gganimate\n2-10. esquisse: 드래그\u0026amp;드롭으로 ggplot 그리기 shiny NA imputation\n4-1. mice visNetwork(visNodes, visEdges) apriori(연관성분석, 장바구니분석) xaringan(R로 PPT 만들기)\n7-1. 참고사이트1\n7-2. 참고사이트2 Rstudio Ligature 적용하기 참고사이트 latex2exp: expression 대신 latex문법 활용하기    SPSS\n 기술통계(데이터 유형, 분포의 이해) 표본분포의 개념 정규성 검정(모수적 접근 \u0026amp; 비모수적 접근) 가설검정 단일집단 평균 독립 두 집단 평균 대응 두 집단 평균 독립 세 집단 이상 평균 카이제곱검정(범주형 데이터)    Statistics\n0. SPSS 내용 대부분\n 분포 간 관계도 정리 mgf(moment generating function)\n2-1. 테일러급수 normal sampling theory 증명    Updates  주식  gitignore나 config.toml ignoreFiles에 추가해두기 매달 초 기록하기 매 매매를 기준으로 계산해보기 코스피, 코스닥, 나스닥, S\u0026amp;P500, 다우존스, 환율 등의 지표와 지수들을 정리하기 슈카월드 정리하기(미정)    Blog  Blogdown으로 블로그 만들기  blogdown 설치 및 사이트 개설\n1-1. 테마 추천(가장 유명한 Academic, Rstudio에서 공식적으로 밀고 있는 Apero, 깔끔한 distill ) github 업로드 방식 (Terminal 사용법)\n2-1. 세부 오류 수정(자격증명 관리자-Windows 자격증명)참고사이트\n2-2. git origin 수정하기 참고사이트\n2-3. .Rprofile 수정하기 (blogdown.knit.on_save = TRUE) 테마 선택하기 / Markdown 문법 기본 소개 세부수정하기 1) baseURL 세부수정하기 2) (ex. config.toml의 baseURL, 이미지 삽입) Comment 기능 추가하기(disqus, gitalk) LaTex 수식 리스트 (md와 RMD 구분하기) Netlify로 배포하기 블로그탭 바꾸기(static-\u0026gt;favicon) favicon 변환사이트 코드 결과물 글씨색깔 바꾸기 (assets / sass / themes / _lightcode.scss 중에서 content-code-in-pre-color 바꿔주기) (단, zdoc theme에 한정된 것일 수도!)\n10-1. 위의 방법이 안된다면, config.toml 중 \u0026lsquo;markup.highlight\u0026rsquo;에 \u0026lsquo;style=\u0026ldquo;fruity\u0026rdquo;\u0026lsquo;와 같은 style 추가하기 style 참고사이트    --- 2. Want to Know  Rmd로 plot 그릴 때, data를 어디에 넣어놔야 하는지  ","description":"","id":12,"section":"updates","tags":null,"title":"To Do List","uri":"https://jiwooblog.netlify.app/updates/todo_list/"},{"content":"2021년 3월    주문날짜 제품명 브랜드 금액 쇼핑몰 링크     3.12 캉골 트로픽 헌팅캡 504 BEIGE (S사이즈) 캉골 39,647원 SSG닷컴 링크   3.19 인사일런스 구르카 팬츠 Indigo (S 사이즈) 인사일런스 63,000원 공홈 링크    2021년 2월    주문날짜 제품명 브랜드 금액 쇼핑몰     2.6 뉴발란스 327 오렌지 뉴발란스 114,000원 크림   2.23 캉골 트로픽 헌팅캡 504 BEIGE (M사이즈, 반품) 캉골 37,850원 인터파크   2.24 19ss easy pants brick 유니폼브릿지 15,000원 에딧에디션   2.24 19fw fisherman cardigan orange 유니폼 브릿지 25,000원 에딧에디션    ","description":"","id":13,"section":"updates","tags":null,"title":"쇼핑 리스트","uri":"https://jiwooblog.netlify.app/updates/shopping_list/"},{"content":"Part 2-1. 데이터 엔지니어 기초 다지기 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Unix 환경 및 커맨드 cd\nmkdir\nls\ncp\n./run.sh\nchmod +x run.sh: Permission denied 되었을 때, 권한 부여하기\nrun.sh 코드(참고용) #!/bin/bash\rpython examply.py 1 \u0026gt; example.txt\rpython examply.py 2 \u0026gt; example.txt\r*주의사항: python 대신에 python3를 쓰면 Windows에서는 오류가 날 수도 있다.\n2. AWS 기초 및 CLI 세팅 AWS CLI (Command Line Interface) 다운로드\n IAM에서 사용자 추가하기 Windows Powershell에다가 aws configure 입력하기 액세스 키 ID와 비밀 액세스 키 입력하기  이는 앞으로 shell에서 aws command를 쳐도 가능하게끔 세팅해놓는 것이다.\n","description":"데이터 엔지니어 기초 다지기","id":14,"section":"posts","tags":null,"title":"Unix \u0026 AWS","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part2_1/"},{"content":"1. Drawing MVN plots with ggplot2 1 2  mu = matrix(c(0,10), ncol=1) invSig = solve(matrix(c(4,10,10,100), ncol=2, byrow=TRUE))   1-1. Vectorize + Outer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  dmvlnorm = function(theta, mu, invSig){ (-nrow(mu)/2) * log(2*pi) + 0.5*log(det(invSig)) - 0.5*(t(theta-mu) %*% invSig %*% (theta-mu)) } calc.dens = Vectorize(function(a,b){ theta = c(a,b) exp(dmvlnorm(theta, mu, invSig)) }) A = seq(-5, 5, length=100) B = seq(-15, 40, length=100) dense = outer(A, B, FUN=calc.dens) rownames(dense) = A colnames(dense) = B dens = reshape2::melt(dense) colnames(dens) = c(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;dens\u0026#39;) ggplot(data=dens, aes(x=a, y=b)) + geom_raster(aes(fill=dens, alpha=dens), interpolate=TRUE) + geom_contour(aes(z=dens), color=\u0026#39;black\u0026#39;, size=0.2) + scale_fill_gradient(low=\u0026#39;cornflowerblue\u0026#39;, high=\u0026#39;steelblue\u0026#39;, guide=FALSE) + scale_alpha(range=c(0,1), guide=FALSE) + labs(title=\u0026#39;MVN density\u0026#39;, x=\u0026#39;alpha\u0026#39;, y=\u0026#39;beta\u0026#39;)   2. Gibbs sampling for MVN draws 1 2 3  Y = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/Y.reading\u0026#39;) ggplot(data.frame(Y)) + geom_point(aes(x=pretest, y=posttest))   Prior specification 1 2 3 4  Mu0 \u0026lt;- c(50,50) Lambda0 = matrix(c(625,312.5,312.5,625), ncol=2) nu0 = 4 S0 = (nu0-nrow(Lambda0)-1) * Lambda0   Gibbs Sampler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  inv = solve n = nrow(Y) ybar = colMeans(Y) Sigma = cov(Y) #initials S = 5000 MU = matrix(NA, nrow=S, ncol=2) SIGMA = matrix(NA, nrow=S, ncol=4) for(s in 1:S){ # update Mu Lambdan = inv(inv(Lambda0) + n*inv(Sigma)) Mun = Lambdan %*% (inv(Lambda0) %*% Mu0 + n*inv(Sigma) %*% ybar) Mu = MASS::mvrnorm(n=1, Mun, Lambdan) # update Sigma Sn = S0 + (t(Y)-c(Mu)) %*% t((t(Y)-c(Mu))) Sigma = inv(rWishart(1, nu0+n, inv(Sn))[,,1]) MU[s,] = Mu SIGMA[s,] = c(Sigma) } disp = tail(1:S, S/2) p1 \u0026lt;- data.frame(mu1=MU[disp,1], mu2=MU[disp,2]) %\u0026gt;% ggplot(aes(x=mu1, y=mu2)) + geom_point(size=0.5, color=\u0026#39;steelblue\u0026#39;) + geom_abline(slope=1, intercept=0) + coord_fixed(ratio=1) + ggtitle(\u0026#39;Posterior darws of MU\u0026#39;) meandiff = MU[disp,2] - MU[disp,1] p2 \u0026lt;- data.frame(meandiff=meandiff) %\u0026gt;% ggplot(aes(x=meandiff)) + geom_histogram(color=\u0026#39;white\u0026#39;, fill=\u0026#39;steelblue\u0026#39;, bins=30) + geom_vline(xintercept=0) + ggtitle(\u0026#39;Posterior draws of Mu2 - Mu1\u0026#39;) grid.arrange(p1, p2, ncol=2)   3. Gibbs Sampling for NA imputation 1 2  Y = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/Y.pima.miss\u0026#39;) head(Y)   ## glu bp skin bmi\r## 1 86 68 28 30.2\r## 2 195 70 33 NA\r## 3 77 82 NA 35.8\r## 4 NA 76 43 47.9\r## 5 107 60 NA NA\r## 6 97 76 27 NA\r1  psych::pairs.panels(Y, method=\u0026#39;pearson\u0026#39;, density=T, breaks=20, hist.col=\u0026#39;steelblue\u0026#39;)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  # priors n = nrow(Y) p = ncol(Y) Mu0 = c(120,64,26,26) sd0 = Mu0/2 L0 = matrix(0.1, p, p) diag(L0) = 1 L0 = L0*outer(sd0,sd0) nu0 = p+2 S0 = (nu0-p-1)*L0 Sigma = S0 Y.full = Y O = 1*(!is.na(Y)) for(j in 1:p){ Y.full[is.na(Y.full)[,j], j] = mean(Y.full[,j], na.rm=TRUE) #mean imputation } inv = solve S = 100 for(s in 1:S){ # update Mu ybar = colMeans(Y.full) Ln = inv(inv(L0) + n*inv(Sigma)) Mun = Ln %*% (inv(L0) %*% Mu0 + n*inv(Sigma) %*% ybar) Mu = MASS::mvrnorm(n=1, Mun, Ln) # update Sigma Sn = S0 + (t(Y.full) - c(Mu)) %*% t((t(Y.full) - c(Mu))) Sigma = inv(rWishart(1, nu0+n, inv(Sn))[,,1]) # update missing data for(i in 1:n){ #iterate over rows b = (O[i,] == 0) #missing at each row a = (O[i,] == 1) #observed at each row if(sum(b)==0) next iSa = inv(Sigma[a,a]) beta.j = Sigma[b,a] %*% iSa Sigma.j = Sigma[b,b] - Sigma[b,a] %*% iSa %*% Sigma[a,b] Mu.j = Mu[b] + beta.j %*% (t(Y.full[i,a]) - Mu[a]) Y.full[i,b] = MASS::mvrnorm(1, Mu.j, Sigma.j) } if(s%% 10 == 1) cat(s, \u0026#39;\\t\u0026#39;) }   ## 1 11 21 31 41 51 61 71 81 91 1  colSums(is.na(Y))   ## glu bp skin bmi ## 15 23 25 22\r1  colSums(is.na(Y.full))   ## glu bp skin bmi ## 0 0 0 0\r","description":"다변량 정규분포","id":15,"section":"posts","tags":null,"title":"MVN","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/mvn/"},{"content":"Vector Space  덧셈에 닫혀있다. 스칼라배에 닫혀있다.  Subspace 벡터공간 $V_n$ 의 부분 집합 $W_n$이 벡터공간이면, $W_n$을 $V_n$의 부분공간이라고 한다. 즉, $W_n$이 부분공간이려면, 덧셈과 스칼라배에 닫혀있으면 된다.\nGauss-Jordan Elimination  확장행렬을 기약행사다리꼴(RREF, reduced row echelon form)로 바꾸는 알고리즘 가우스조던 소거법 = 가감법 + 대입법  선형사상의 특징  가산성 $f(x+y) = f(x) + f(y)$ 동차성 $f(ax) = af(x)$  LU Decomposition $E_{k}...E_{2}E_{1}A = U$\n=\u0026gt; $A = E_{1}^{-1}E_{2}^{-1}...E_{k}^{-1}U = LU$  여기서 $E_{k}...E_{2}E_{1}$는 하삼각행렬이며, $E_{1}^{-1}E_{2}^{-1}...E_{k}^{-1}$도 하삼각행렬이다.\n  근거(1) 가우스 소거법을 시행할 때 사용되는 모든 기본행렬은 항상 하삼각행렬이다. (단, 행교환 제외) 근거(2) 하삼각행렬인 기본행렬의 역행렬은 여전히 하삼각행렬이다. 근거(3) 하삼각행렬 $\\times$ 하삼각행렬 = 하삼각행렬 PLU Decomposition 행교환이 필요한 경우, 행교환을 미리 해주고 LU 분해하는 법 (P: permutation)\n용어 정리  consistent: 해가 적어도 한 개는 있는 경우 homogeneous: 동차 (ex. $A\\vec{x} = \\vec{0}$: 동차 연립선형방정식)  ","description":"Intro","id":16,"section":"posts","tags":null,"title":"Lecture 01","uri":"https://jiwooblog.netlify.app/posts/statistics/linearalgebra/l01/"},{"content":"Chapter 01. Introduction and Examples 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n이번 장을 통해서는 Likelihood and Prior를 살펴보고 Full probability model의 의미를 보는 데에 주목해보쟈.\n베이지안 추론의 목적 우리는 데이터 획득을 통해, 모집단 특성에 대한 불확실성을 줄여나가고자 한다. 이때, 불확실성 정도의 변화 수준을 계량화하는 것이 베이지안 추론통계의 목적이라고 할 수 있다.\n핵심 개념  prior distribution $p(\\theta)$  사전확률 모수에 대해 기존에 갖고 있던 믿음의 정도   sampling model $p(y|\\theta)$  일종의 가능도 함수(likelihood) 사전확률이 참이라는 가정 하에, 특정 데이터가 관찰된 확률   posterior distribution $p(\\theta|y)$  데이터가 관찰되었을 때, 이를 바탕으로 수정된 모수에 대한 믿음의 정도    Bayes' Rule $$p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{\\int_{\\Theta}p(y|\\tilde{\\theta})p(\\tilde{\\theta})d\\tilde{\\theta}}$$\n이는 사후분포가 사전분포와 가능도 함수에 의해 어떻게 업데이트 되는지를 수식적으로 나타난 것이다.\n베이즈 통계의 전부라고 해도 무방하다.\n활용예시  희소사건 확률 추정(Estimation)  감염 확률(infectious probability) 확률론자(frequentist)는 sample이 적을 때 확률추정을 합리적으로 하는 데에 있어서 취약할 수 있다. 예를 들어, 20명만을 대상으로 감염 여부를 확인하고 감염 확률을 추론한다면, 감염확률을 0%라고 제안하는 것은 통계적으로는 그럴 듯하게 계산될 수 있다. 하지만 이는 현실과는 다소 거리가 있을 수 있다. 이에 반해, 베이지안은 감염 확률을 분포로서 제시할 뿐더러 기존의 믿음을 사전확률로서 제안하기 때문에 이러한 부분에 있어서 덜 취약할 수 있다.   예측 모델 구축(Prediction)  당뇨병(diabetes progression) 50% 확률로 변수의 coefficient가 0라고 사전확률을 제안한다면, 변수선택의 효과를 얻을 수 있다. 이와 관련된 자세한 내용은 FCB chapter 09서 Bayesian Linear Regression과 관련하여 설명될 예정이다.    ETC  \u0026lsquo;Adjusted\u0026rsquo; Wald interval\n흔히 알려진 신뢰구간을 베이지안적으로 바꾼 형태이다.  `\\hat{\\theta} \\pm 1.96\\sqrt{\\hat{\\theta}(1-\\hat{\\theta})//n}` , where `\\hat{\\theta} = \\frac{n}{n+4}\\bar{y} + \\frac{4}{n+4}\\frac{1}{2}`\n Lasso\n변수 선택의 한 방법이다. 아래 제시된 SSR를 최소화하는 것을 목표로 한다.\n베이지안의 맥락에서 처음 연구된 방법론은 아니지만, 특정 사전확률을 적용한다면 베이지안의 관점과 일치한다.\n여기서 말하는 그 특정 사전확률분포란, $\\beta_j$가 0에서 첨점을 갖는 라플라스 분포(또는 double-exponential distribution)를 따른다는 것을 의미한다.\n그리고 이때 lasso estimate은 $\\beta$의 사후 최빈값(posterior mode)과 같다.\n$$SSR(\\beta:\\lambda) = \\sum_{i=1}^{n}(y_i-\\boldsymbol{x_i}^T\\boldsymbol{\\beta})^2 + \\lambda\\sum_{j=1}^{n}|\\beta_j|$$  Conclusion \"All models are wrong, but some are useful\" - Box and Draper, 1987 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"Introduction and Examples","id":17,"section":"posts","tags":null,"title":"What is Bayesian","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/"},{"content":"심리학 수강 과목  심리학개론 충동과자기관리 심리학의 실험연구방법 발달심리학 행복의 과학 언어심리학 이론 및 실습 인지신경과학의 기초 심리통계 학습과 기억의 심리학 UT세미나(실패와 좌절의 심리학) 성격심리학 재능과 기술의 심리학 인지공학심리학 산업심리학  ","description":"","id":18,"section":"posts","tags":null,"title":"수강과목","uri":"https://jiwooblog.netlify.app/posts/yonsei/psychology/lecture/"},{"content":"통계학 수강 과목  통계학입문 미분적분학 통계방법론 선형대수 컴퓨터자료처리 심리통계 데이터사이언스를 위한 확률과정 회귀분석 수리통계학(1) 베이즈통계 수리통계학 (2) 시계열분석 금융리스크관리 실무와 통계학 UT세미나(생물통계학) 범주형자료분석 데이터사이언스입문 데이터마이닝 이론통계학(1) 데이터사이언스(2): 네트워크 자료분석  ","description":"","id":19,"section":"posts","tags":null,"title":"수강과목","uri":"https://jiwooblog.netlify.app/posts/yonsei/statistics/lecture/"},{"content":"NH투자증권 Y\u0026amp;Z세대 투자자 프로파일링 본 대회는 NH투자증권에서 주최한 대회로, 2020년 급격하게 늘어난 20~30대 투자자들에 대한 분석을 하고 이를 시각화하는 것이 주목적이었다.\n코드가 상당히 복잡한 관계로 Dacon에 코드공유한 링크와 이미지 일부를 올리는 것으로 포스팅을 대체하고자 한다.\nDacon 코드 공유 Factor Analysis Word Cloud_국내 Word Cloud_해외 Cluster Polygon Cluster Characteristics Idea Table Idea Sample Domain Knowledge 1. 해외주식 소수점 거래  현재 신한금융투자, 한국투자증권에서 실현 중 의결권, 배당권을 빼고 소수점 거래 가능! (ex. 한투 미니스톡)  2. 휴면 고객에 대한 이벤트도 주기적으로 한다. 3. ETP (= ETF + ETN) 금융투자협회: 한눈에 알아보는 레버리지 ETP Guide\n  괴리율\n 순자산가치(NAV) \u0026amp; 지표가치(IV)은 장중 실시간으로 산출한다.  전일 종가로 확정된 순자산 가치 + 당일의 가격 움직임   괴리율이 플러스(+): 추적대상 지수보다 고평가되어 있다(=비싸게 거래되고 있다). 괴리율이 마이너스(-): 추적대상 지수보다 저평가되어 있다(=싸게 거래되고 있다.) 괴리율이 너무 커지면, 한국거래소에서 단일가 매매 또는 매매거래정지를 시킬 수 있다. 유동성공급자(Liquidity Provider, LP)  괴리율이 과도하게 높을 경우: ETP를 매도하거나 대상자산을 매입해야겠다! 괴리율이 과도하게 낮을 경우: ETP를 매수하거나 대상자산을 매도해야겠다!      복리 효과\n 레버리지 ETP의 운용방식: \u0026lsquo;일별\u0026rsquo; 수익률의 ±2배 상승장일 경우, 2X의 상승률 \u0026gt; 인버스2X 하락률 하락장일 경우, 2X의 하락률 \u0026lt; 인버스2X 상승률 횡보장일 경우, 2X와 인버스2X 모두 손해볼 가능성이 높다. 즉, 장기투자에 적합하지 않다. 상식: 레버리지 상품을 만들기 위해서는 파생상품을 집어넣는다.    롤오버(roll-over)\n 파생상품의 거래월물을 교체하는 것(파생상품의 만기에 발생) 즉, 롤오버 과정에서 거래월물의 가겨차이에 따라 ETP의 자산가치 변동이 이루어질 수 있다. 선물 \u0026lsquo;매수\u0026rsquo;포지션을 보유하고 있는 레버리지(2X) ETP  거래월물의 가격차이가 (+)상태이면 이득, (-)상태이면 손해   선물 \u0026lsquo;매도\u0026rsquo;포지션을 보유하고 있는 레버리지(2X) ETP  거래월물의 가격차이가 (+)상태이면 손해, (-)상태이면 이득      지수의 방법론\n 괴리율, 복리효과, 롤오버만큼의 중요성은 아니지만, 간과해서는 안되는 POINT ex) 2020년 4월 유례 없는 마이너스 유가 폭락으로 인한 원유선물의 거래월물 편입대상과 롤오버 방식 변화    배운 점 1. 크롤링 능력  크롤링은 Python을 통해서 진행하였다. BeautifulSoup, selenium의 webdriver 등의 라이브러리를 활용하였다. (1) 해외사이트 크롤링의 어려움  느리고, 자잘한 오류가 생긴다. 예를 들어, 특정 단어를 검색을 해도 나오지 않는다.   (2) 크롤링 권한 접근  접근 권한의 제한으로 단순 크롤링을 할 수 없는 경우\nNetwork-XHR의 Request Headers 활용하기 에러 핸들링 관점에서 데이터 엔지니어링의 중요성 체감    2. 파생변수 제작의 어려움  run_away_cd(휴면 여부) 라는 변수를 새롭게 만들었다. 총 세가지 기준에 의해 각 고객의 휴면 여부를 판단하였는데, 그 기준은 다음과 같다.  (1) 거래주기에 비해 거래휴식기가 지나치게 긴 경우 (2) 예상 거래횟수에 비해 실제 거래횟수가 눈에 띄게 적은 경우 (3) 최근 2달 이내에 계좌개설한 사람들은 제외   해당 변수를 만들고 유의성을 검토하는 데에 적지 않은 시간을 투자하였던 경험👍 해당 변수를 만들기 위해, 중간과정에서 orr_prd, orr_cyl, orr_idx_1, orr_idx2를 만들었다.  3. 요인분석 Factor 조합 노하우  어려운 변수보다는 직관적으로 간단한 변수들의 조합으로 구성하는 것이 좋다.. 왜냐하면 요인분석 자체가 해석을 목적으로 하기 때문이다.  ","description":"","id":20,"section":"posts","tags":null,"title":"YZ 투자자 프로파일링","uri":"https://jiwooblog.netlify.app/posts/dacon/nh_yz/"},{"content":"lubridate 패키지 훑어보기 lubridate는 날짜 데이터를 처리하기 위한 패키지입니다.\n1  library(lubridate)    목차  parse datetimes  1. parse datetimes   다양한 형태 #1  1 2 3 4 5 6 7 8 9 10 11  # 다양한 형태가 있다. # ymd_hms(), ymd_hm(), ymd_h() # ydm_hms(), ydm_hm(), ydm_h() # mdy_hms(), mdy_hm(), mdy_h() # dmy_hms(), dmy_hm(), dmy_h() # ymd(), ydm() # mdy(), myd() # dmy(), dym() # yq() Q for quarter ymd_hms(\u0026#34;2017-11-28T14:02:00\u0026#34;)   ## [1] \u0026quot;2017-11-28 14:02:00 UTC\u0026quot;\r1  ydm_hms(\u0026#34;2017-22-12 10:00:00\u0026#34;)   ## [1] \u0026quot;2017-12-22 10:00:00 UTC\u0026quot;\r1  mdy_hms(\u0026#34;11/28/2017 1:02:03\u0026#34;)   ## [1] \u0026quot;2017-11-28 01:02:03 UTC\u0026quot;\r1  dmy_hms(\u0026#34;1 Jan 2017 23:59:59\u0026#34;)   ## [1] \u0026quot;2017-01-01 23:59:59 UTC\u0026quot;\r1  ymd(20170131)   ## [1] \u0026quot;2017-01-31\u0026quot;\r1  mdy(\u0026#34;July 4th, 2000\u0026#34;)   ## [1] \u0026quot;2000-07-04\u0026quot;\r1  dmy(\u0026#34;4th of July \u0026#39;99\u0026#34;)   ## [1] \u0026quot;1999-07-04\u0026quot;\r1  yq(\u0026#34;2001: Q3\u0026#34;)   ## [1] \u0026quot;2001-07-01\u0026quot;\r     다양한 형태 #2  1  today()   ## [1] \u0026quot;2021-03-23\u0026quot;\r1  now()   ## [1] \u0026quot;2021-03-23 11:24:45 KST\u0026quot;\r1  date_decimal(2021.5) #2021년 5월이 아니라, 2021년의 절반   ## [1] \u0026quot;2021-07-02 12:00:00 UTC\u0026quot;\r1  fast_strptime(\u0026#39;9/1/01\u0026#39;, \u0026#39;%y/%m/%d\u0026#39;)   ## [1] \u0026quot;2009-01-01 UTC\u0026quot;\r1  parse_date_time(\u0026#34;9/1/01\u0026#34;, \u0026#34;ymd\u0026#34;) # fast_strptime과 달리, 조금 더 디테일하게 써줘야한다.   ## Warning: All formats failed to parse. No formats found.\r## [1] NA\r1  parse_date_time(\u0026#34;2009/1/01\u0026#34;, \u0026#34;ymd\u0026#34;)   ## [1] \u0026quot;2009-01-01 UTC\u0026quot;\r   ","description":"","id":21,"section":"posts","tags":null,"title":"lubridate","uri":"https://jiwooblog.netlify.app/posts/r/lubridate/"},{"content":"Entropy 정보량 = 불확실성\n$$H(p) = -\\sum_{i=1}p_i log(p_i) $$\nCross-Entropy p에 대해, 전략 Q를 사용했을 때의 불확실성\n$$H(p,q) = -\\sum_{i=1}p_i log(q_i) $$\nKL-Divergence $$\n\\begin{aligned}\nKL(p||q) \u0026amp;= \\sum_{i=1}p_i log\\frac{p_i}{q_i} \\\n\u0026amp;= H(p,q) - H(p)\n\\end{aligned}\n$$\n","description":"Entropy, Cross-Entropy, KL-Divergence","id":22,"section":"posts","tags":null,"title":"Entropy, KL-Divergence","uri":"https://jiwooblog.netlify.app/posts/machinelearning/entropy/"},{"content":"tidyr 패키지 훑어보기 tidyr은 tidy data를 형성하기 위해 고안된 패키지입니다. tidy data에서 1) 열은 변수를 의미하고, 2) 행은 하나의 케이스를 의미하며, 3) 하나의 셀은 하나의 값을 의미합니다.\n1  library(tidyverse)    목차  nest  nest 예시를 통해, 단순히 group_by를 하는 것과 group_by 이후 nest를 한 후에 어떻게 데이터가 정리되는지 확인해보자.\n  nest 예시  1 2  iris %\u0026gt;% group_by(Species)   ## # A tibble: 150 x 5\r## # Groups: Species [3]\r## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows\r1 2 3  iris %\u0026gt;% group_by(Species) %\u0026gt;% nest()   ## # A tibble: 3 x 2\r## # Groups: Species [3]\r## Species data ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026gt; ## 1 setosa \u0026lt;tibble [50 x 4]\u0026gt;\r## 2 versicolor \u0026lt;tibble [50 x 4]\u0026gt;\r## 3 virginica \u0026lt;tibble [50 x 4]\u0026gt;\r  \n참고 [1] https://gomguard.tistory.com/229\n","description":"","id":23,"section":"posts","tags":null,"title":"tidyr","uri":"https://jiwooblog.netlify.app/posts/r/tidyr/"},{"content":"2021.02.26  hugo theme 중 zzo와 zdoc은 다른 것이다. zzo 참고사이트 zdoc 참고사이트  ","description":"","id":24,"section":"updates","tags":null,"title":"February 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_02/"},{"content":"대회 리스트  빅콘테스트 데이콘(Dacon) 서울특별시 빅데이터 캠퍼스 공모전 한국정보화진흥원 데이터 크리에이터 캠프 상권분석 빅데이터 경진대회 KB 국민은행 Future Finance Ai Challenge 빅데이터 활용정책 아이디어 공모전 Big Data Competition 미래에셋대학생디지털 금융페스티벌  ","description":"","id":25,"section":"updates","tags":null,"title":"대회 리스트","uri":"https://jiwooblog.netlify.app/updates/contest_list/"},{"content":"dplyr 패키지 훑어보기 1 2  library(tidyverse) library(MASS)   목차  rowwise\n1-1. pmax slice relocate lag, lead between, near coalsece recode first, last, nth rownames_to_column, column_to_rownames bind_rows, bind_cols mutate_all, mutate_if inner_join, left_join, right_join, full_join semi_join, anti_join  데이터   data  1 2 3 4  tmp \u0026lt;- tibble(x=round(rnorm(n=5, mean=5, sd=1)), y=round(rnorm(n=5, mean=5, sd=3)), z=round(rnorm(n=5, mean=5, sd=5))) tmp   ## # A tibble: 5 x 3\r## x y z\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1\r## 2 5 4 -2\r## 3 5 2 6\r## 4 6 2 12\r## 5 5 4 2\r1 2  data(survey) glimpse(survey)   ## Rows: 237\r## Columns: 12\r## $ Sex \u0026lt;fct\u0026gt; Female, Male, Male, Male, Male, Female, Male, Female, Male, ...\r## $ Wr.Hnd \u0026lt;dbl\u0026gt; 18.5, 19.5, 18.0, 18.8, 20.0, 18.0, 17.7, 17.0, 20.0, 18.5, ...\r## $ NW.Hnd \u0026lt;dbl\u0026gt; 18.0, 20.5, 13.3, 18.9, 20.0, 17.7, 17.7, 17.3, 19.5, 18.5, ...\r## $ W.Hnd \u0026lt;fct\u0026gt; Right, Left, Right, Right, Right, Right, Right, Right, Right...\r## $ Fold \u0026lt;fct\u0026gt; R on L, R on L, L on R, R on L, Neither, L on R, L on R, R o...\r## $ Pulse \u0026lt;int\u0026gt; 92, 104, 87, NA, 35, 64, 83, 74, 72, 90, 80, 68, NA, 66, 60,...\r## $ Clap \u0026lt;fct\u0026gt; Left, Left, Neither, Neither, Right, Right, Right, Right, Ri...\r## $ Exer \u0026lt;fct\u0026gt; Some, None, None, None, Some, Some, Freq, Freq, Some, Some, ...\r## $ Smoke \u0026lt;fct\u0026gt; Never, Regul, Occas, Never, Never, Never, Never, Never, Neve...\r## $ Height \u0026lt;dbl\u0026gt; 173.00, 177.80, NA, 160.00, 165.00, 172.72, 182.88, 157.00, ...\r## $ M.I \u0026lt;fct\u0026gt; Metric, Imperial, NA, Metric, Metric, Imperial, Imperial, Me...\r## $ Age \u0026lt;dbl\u0026gt; 18.250, 17.583, 16.917, 20.333, 23.667, 21.000, 18.833, 35.8...\r   1. rowwise() 행별로 최대값 구하기\n  rowwise 예시  1 2 3 4  #올바른 버전 tmp %\u0026gt;% rowwise() %\u0026gt;% mutate(max = max(x,y,z))   ## # A tibble: 5 x 4\r## # Rowwise: ## x y z max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1 7\r## 2 5 4 -2 5\r## 3 5 2 6 6\r## 4 6 2 12 12\r## 5 5 4 2 5\r1 2 3  #잘못된 버전 tmp %\u0026gt;% mutate(max = max(x,y,z))   ## # A tibble: 5 x 4\r## x y z max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1 12\r## 2 5 4 -2 12\r## 3 5 2 6 12\r## 4 6 2 12 12\r## 5 5 4 2 12\r  \n1-1. pmax 그런데 사실은 여기서 rowwise를 사용하지 않고, pmax를 사용하면 보다 간단하게 구할 수 있기도 하다.\n  pmax 예시  1 2 3  #간단한 버전 tmp %\u0026gt;% mutate(max = pmax(x,y,z))   ## # A tibble: 5 x 4\r## x y z max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 7 -1 7\r## 2 5 4 -2 5\r## 3 5 2 6 6\r## 4 6 2 12 12\r## 5 5 4 2 5\r  \n2. slice() 행 선택\n  slice 예시  1 2 3  # choose 10 rows with smallest 10 pulse values. survey %\u0026gt;% slice_min(Pulse, n = 10)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Male 20.0 20.0 Right Neither 35 Right Some Never 165.00 Metric\r## 2 Female 16.5 17.0 Right L on R 40 Left Freq Never 167.64 Imperial\r## 3 Female 18.0 17.5 Right R on L 48 Neither Freq Never 165.00 Metric\r## 4 Male 21.0 21.0 Right L on R 48 Neither Freq Never 174.00 Metric\r## 5 Female 18.0 17.9 Right R on L 50 Left None Never 165.00 Metric\r## 6 Female 15.5 15.5 Right Neither 50 Right Some Regul NA \u0026lt;NA\u0026gt;\r## 7 Male 18.0 19.0 Right L on R 54 Neither Some Regul NA \u0026lt;NA\u0026gt;\r## 8 Male 22.0 21.5 Left R on L 55 Left Freq Never 200.00 Metric\r## 9 Male 20.5 19.5 Right L on R 56 Right Freq Never 179.00 Metric\r## 10 Male 19.8 20.0 Left L on R 59 Right Freq Never 180.00 Metric\r## Age\r## 1 23.667\r## 2 17.417\r## 3 18.667\r## 4 21.333\r## 5 30.750\r## 6 18.500\r## 7 17.750\r## 8 18.500\r## 9 17.417\r## 10 17.417\r1 2 3  # choose 10 columns with greatest 10 pulses values. survey %\u0026gt;% slice_max(Pulse, n = 10)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Male 19.5 20.5 Left R on L 104 Left None Regul 177.8 Imperial\r## 2 Female 19.0 18.5 Left L on R 104 Left Freq Never 170.0 Metric\r## 3 Female 18.5 18.0 Left L on R 100 Neither Some Never 171.0 Metric\r## 4 Male 21.0 20.4 Right L on R 100 Right Freq Heavy 184.0 Metric\r## 5 Female 17.5 17.5 Right R on L 98 Left Freq Never NA \u0026lt;NA\u0026gt;\r## 6 Male 17.5 17.0 Left L on R 97 Neither None Never 165.0 Metric\r## 7 Male 22.5 23.0 Right R on L 96 Right None Never 170.0 Metric\r## 8 Male 21.4 21.0 Right L on R 96 Neither Some Never 180.0 Metric\r## 9 Female 17.5 17.8 Right R on L 96 Right Some Never NA \u0026lt;NA\u0026gt;\r## 10 Female 18.5 18.0 Right R on L 92 Left Some Never 173.0 Metric\r## 11 Female 18.0 17.7 Left R on L 92 Left Some Never NA \u0026lt;NA\u0026gt;\r## 12 Female 18.5 18.0 Right R on L 92 Right Freq Never 172.0 Metric\r## 13 Female 18.0 18.0 Right L on R 92 Neither Freq Never 165.0 Metric\r## 14 Female 16.3 16.2 Right L on R 92 Right Some Regul 152.4 Imperial\r## 15 Male 20.0 19.5 Right R on L 92 Right Some Never 179.1 Imperial\r## Age\r## 1 17.583\r## 2 17.250\r## 3 18.917\r## 4 20.083\r## 5 17.667\r## 6 19.500\r## 7 19.417\r## 8 19.000\r## 9 18.667\r## 10 18.250\r## 11 17.583\r## 12 17.500\r## 13 20.000\r## 14 23.500\r## 15 18.917\r1 2 3  # randomly select 10 rows. survey %\u0026gt;% slice_sample(n = 10)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Female 18.3 18.5 Right R on L 75 Left Freq Never 170.00 Metric\r## 2 Male 20.0 19.5 Right R on L 92 Right Some Never 179.10 Imperial\r## 3 Female 17.5 18.0 Right R on L 68 Neither Freq Never 157.48 Imperial\r## 4 Female 18.5 18.2 Right R on L 72 Neither Freq Never 167.64 Imperial\r## 5 Female 20.5 20.5 Right R on L NA Left Freq Regul NA \u0026lt;NA\u0026gt;\r## 6 Male 18.0 16.0 Right R on L NA Right Some Never 180.34 Imperial\r## 7 Female 17.7 17.0 Right R on L 76 Right Some Never 167.00 Metric\r## 8 Male 16.0 15.5 Right Neither 71 Right Freq Never 154.94 Imperial\r## 9 Male 18.5 18.5 Right L on R NA Neither Freq Never 171.00 Metric\r## 10 Male 20.5 19.5 Left L on R 80 Right Some Occas 182.88 Imperial\r## Age\r## 1 18.750\r## 2 18.917\r## 3 17.750\r## 4 17.333\r## 5 19.250\r## 6 20.750\r## 7 17.250\r## 8 17.167\r## 9 18.333\r## 10 18.667\r1 2 3  # randomly select 10% of the data observations. survey %\u0026gt;% slice_sample(prop = .05)   ## Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height M.I\r## 1 Female 17.0 17.0 Right L on R 79 Right Some Never 163.00 Metric\r## 2 Female 17.7 17.0 Right R on L 76 Right Some Never 167.00 Metric\r## 3 Female 16.7 15.1 Right Neither NA Right None Never 157.48 Imperial\r## 4 Male 19.5 20.5 Left R on L 104 Left None Regul 177.80 Imperial\r## 5 Male 19.5 20.2 Right R on L 60 Neither Freq Never 185.42 Imperial\r## 6 Female 13.0 13.0 \u0026lt;NA\u0026gt; L on R 70 Left Freq Never 180.34 Imperial\r## 7 Female 18.0 17.8 Right L on R 68 Right Some Never 168.90 Imperial\r## 8 Male 20.5 20.0 Right L on R 68 Right Freq Never 190.00 Metric\r## 9 Male 20.5 21.0 Right R on L 60 Right Freq Never 185.00 Metric\r## 10 Female 19.5 19.2 Right R on L 70 Right Some Never 170.00 Metric\r## 11 Female 16.2 16.4 Right R on L NA Right Freq Occas 172.00 Metric\r## Age\r## 1 24.667\r## 2 17.250\r## 3 18.167\r## 4 17.583\r## 5 32.667\r## 6 17.417\r## 7 17.083\r## 8 17.500\r## 9 17.917\r## 10 18.167\r## 11 17.000\r  \n3. relocate() relocate: changes the order of the columns.\n  relocate 예시  1 2 3  # move columns with factor variables to the front survey %\u0026gt;% relocate(where(is.factor)) %\u0026gt;% colnames()   ## [1] \u0026quot;Sex\u0026quot; \u0026quot;W.Hnd\u0026quot; \u0026quot;Fold\u0026quot; \u0026quot;Clap\u0026quot; \u0026quot;Exer\u0026quot; \u0026quot;Smoke\u0026quot; \u0026quot;M.I\u0026quot; \u0026quot;Wr.Hnd\u0026quot;\r## [9] \u0026quot;NW.Hnd\u0026quot; \u0026quot;Pulse\u0026quot; \u0026quot;Height\u0026quot; \u0026quot;Age\u0026quot;\r1 2 3  # move Pulse before Height survey %\u0026gt;% relocate(Pulse, .before = Height) %\u0026gt;% colnames()   ## [1] \u0026quot;Sex\u0026quot; \u0026quot;Wr.Hnd\u0026quot; \u0026quot;NW.Hnd\u0026quot; \u0026quot;W.Hnd\u0026quot; \u0026quot;Fold\u0026quot; \u0026quot;Clap\u0026quot; \u0026quot;Exer\u0026quot; \u0026quot;Smoke\u0026quot; ## [9] \u0026quot;Pulse\u0026quot; \u0026quot;Height\u0026quot; \u0026quot;M.I\u0026quot; \u0026quot;Age\u0026quot;\r1 2 3  # move Pulse to the end survey %\u0026gt;% relocate(Pulse, .after = last_col()) %\u0026gt;% colnames()   ## [1] \u0026quot;Sex\u0026quot; \u0026quot;Wr.Hnd\u0026quot; \u0026quot;NW.Hnd\u0026quot; \u0026quot;W.Hnd\u0026quot; \u0026quot;Fold\u0026quot; \u0026quot;Clap\u0026quot; \u0026quot;Exer\u0026quot; \u0026quot;Smoke\u0026quot; ## [9] \u0026quot;Height\u0026quot; \u0026quot;M.I\u0026quot; \u0026quot;Age\u0026quot; \u0026quot;Pulse\u0026quot;\r  \n4. lag(), lead()   lag, lead 예시  1  lag(1:5)   ## [1] NA 1 2 3 4\r1  lag(1:5, n = 2)   ## [1] NA NA 1 2 3\r1  lag(1:5, default = 0)   ## [1] 0 1 2 3 4\r1  lead(1:5)   ## [1] 2 3 4 5 NA\r1  lead(1:5, default = 6)   ## [1] 2 3 4 5 6\r   5. between(), near()   between, near 예시  1 2  # between: \u0026gt;=, \u0026lt;= 조건을 한번에 사용하기 between(1:12, 7, 9)   ## [1] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE\r1 2  # near: ==의 안전한 버전(특히 소수점 계산시) sqrt(2) ^ 2 == 2   ## [1] FALSE\r1  near(sqrt(2) ^ 2, 2)   ## [1] TRUE\r   6. coalesce() 각 위치별로 NA가 아닌 값을 첫번째 값을 반환\n  coalesce 예시  1 2 3 4 5  library(tidyverse) a \u0026lt;- NA b \u0026lt;- 3 c \u0026lt;- 5 coalesce(a,b,c)   ## [1] 3\r1 2 3  # 응용: coalesce를 NA imputation으로 활용하기 x \u0026lt;- sample(c(1:5, NA, NA, NA)) coalesce(x, 0)   ## [1] 0 2 0 3 1 4 0 5\r1 2 3  y \u0026lt;- c(1, 2, NA, NA, 5) z \u0026lt;- c(NA, NA, 3, 4, 5) coalesce(y, z)   ## [1] 1 2 3 4 5\r1 2 3 4 5  vecs \u0026lt;- list( c(1, 2, NA, NA, 5), c(NA, NA, 3, 4, 5) ) coalesce(!!!vecs)   ## [1] 1 2 3 4 5\r  \n7. recode() case_when의 특수한 형태로서 데이터를 교체할 때 사용할 수 있을 것이다.\n  recode 예시  1 2  tmp_char \u0026lt;- sample(c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;), 10, replace = TRUE) recode(tmp_char, a = \u0026#34;Apple\u0026#34;)   ## [1] \u0026quot;b\u0026quot; \u0026quot;b\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;b\u0026quot; \u0026quot;b\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;b\u0026quot; \u0026quot;Apple\u0026quot;\r## [10] \u0026quot;c\u0026quot;\r1  recode(tmp_char, a = \u0026#34;Apple\u0026#34;, b = \u0026#34;Banana\u0026#34;)   ## [1] \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot;\r## [9] \u0026quot;Apple\u0026quot; \u0026quot;c\u0026quot;\r1  recode(tmp_char, a = \u0026#34;Apple\u0026#34;, b = \u0026#34;Banana\u0026#34;, .default = NA_character_)   ## [1] \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Banana\u0026quot; \u0026quot;Apple\u0026quot; \u0026quot;Banana\u0026quot;\r## [9] \u0026quot;Apple\u0026quot; NA\r1 2 3  # 숫자형은 아래와 같이 ``표시가 들어가야 한다. tmp_num \u0026lt;- sample(c(1,2,3), 10, replace=TRUE) recode(tmp_num, `1`=5)   ## [1] 3 5 3 5 2 3 3 2 5 3\r1 2 3  # !!!을 활용하면, python에서 dictionary 형태로 활용하는 것처럼 쓸 수 있다. level_key \u0026lt;- c(a = \u0026#34;apple\u0026#34;, b = \u0026#34;banana\u0026#34;, c = \u0026#34;carrot\u0026#34;) recode(tmp_char, !!!level_key)   ## [1] \u0026quot;banana\u0026quot; \u0026quot;banana\u0026quot; \u0026quot;apple\u0026quot; \u0026quot;apple\u0026quot; \u0026quot;banana\u0026quot; \u0026quot;banana\u0026quot; \u0026quot;apple\u0026quot; \u0026quot;banana\u0026quot;\r## [9] \u0026quot;apple\u0026quot; \u0026quot;carrot\u0026quot;\r  \n8. first(), last(), nth() 첫번째, 마지막 또는 특정 위치에 있는 요소를 반환하는 함수이다.\n  first, last, nth 예시  1 2 3 4  x \u0026lt;- 1:10 y \u0026lt;- 10:1 first(x)   ## [1] 1\r1  last(y)   ## [1] 1\r1  nth(x, 3)   ## [1] 3\r1  nth(y, 4)   ## [1] 7\r  \n9. rownames_to_column(), column_to_rownames()   rownames_to_column, column_to_rownames 예시  1 2  a \u0026lt;- rownames_to_column(iris, var = \u0026#34;C\u0026#34;) head(a)   ## C Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 1 5.1 3.5 1.4 0.2 setosa\r## 2 2 4.9 3.0 1.4 0.2 setosa\r## 3 3 4.7 3.2 1.3 0.2 setosa\r## 4 4 4.6 3.1 1.5 0.2 setosa\r## 5 5 5.0 3.6 1.4 0.2 setosa\r## 6 6 5.4 3.9 1.7 0.4 setosa\r1 2  b \u0026lt;- column_to_rownames(a, var = \u0026#34;C\u0026#34;) head(b)   ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 5.1 3.5 1.4 0.2 setosa\r## 2 4.9 3.0 1.4 0.2 setosa\r## 3 4.7 3.2 1.3 0.2 setosa\r## 4 4.6 3.1 1.5 0.2 setosa\r## 5 5.0 3.6 1.4 0.2 setosa\r## 6 5.4 3.9 1.7 0.4 setosa\r   10. bind_rows(), bind_cols() 기존의 rbind랑 cbind 대신에 활용하면 될 것 같다.\n  bind_rows, bind_cols 예시  1 2 3 4 5  # bind_rows one_r \u0026lt;- starwars[1:4, ] two_r \u0026lt;- starwars[9:12, ] three_r \u0026lt;- starwars[9:12, 3] bind_rows(one_r, two_r)   ## # A tibble: 8 x 14\r## name height mass hair_color skin_color eye_color birth_year sex gender\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke~ 172 77 blond fair blue 19 male mascu~\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold yellow 112 none mascu~\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, bl~ red 33 none mascu~\r## 4 Dart~ 202 136 none white yellow 41.9 male mascu~\r## 5 Bigg~ 183 84 black light brown 24 male mascu~\r## 6 Obi-~ 182 77 auburn, w~ fair blue-gray 57 male mascu~\r## 7 Anak~ 188 84 blond fair blue 41.9 male mascu~\r## 8 Wilh~ 180 NA auburn, g~ fair blue 64 male mascu~\r## # ... with 5 more variables: homeworld \u0026lt;chr\u0026gt;, species \u0026lt;chr\u0026gt;, films \u0026lt;list\u0026gt;,\r## # vehicles \u0026lt;list\u0026gt;, starships \u0026lt;list\u0026gt;\r1  bind_rows(one_r, three_r) # 에러 안 뜸. NA로 채움   ## # A tibble: 8 x 14\r## name height mass hair_color skin_color eye_color birth_year sex gender\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke~ 172 77 blond fair blue 19 male mascu~\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold yellow 112 none mascu~\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, bl~ red 33 none mascu~\r## 4 Dart~ 202 136 none white yellow 41.9 male mascu~\r## 5 \u0026lt;NA\u0026gt; NA 84 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 \u0026lt;NA\u0026gt; NA 77 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 \u0026lt;NA\u0026gt; NA 84 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 \u0026lt;NA\u0026gt; NA NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; NA \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # ... with 5 more variables: homeworld \u0026lt;chr\u0026gt;, species \u0026lt;chr\u0026gt;, films \u0026lt;list\u0026gt;,\r## # vehicles \u0026lt;list\u0026gt;, starships \u0026lt;list\u0026gt;\r1 2 3 4 5  # bind_cols one_c \u0026lt;- starwars[,1:4] two_c \u0026lt;- starwars[,7:9] three_c \u0026lt;- starwars[10:50,7:9] bind_cols(one_c, two_c)   ## # A tibble: 87 x 7\r## name height mass hair_color birth_year sex gender ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke Skywalker 172 77 blond 19 male masculine\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; 112 none masculine\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; 33 none masculine\r## 4 Darth Vader 202 136 none 41.9 male masculine\r## 5 Leia Organa 150 49 brown 19 female feminine ## 6 Owen Lars 178 120 brown, grey 52 male masculine\r## 7 Beru Whitesun lars 165 75 brown 47 female feminine ## 8 R5-D4 97 32 \u0026lt;NA\u0026gt; NA none masculine\r## 9 Biggs Darklighter 183 84 black 24 male masculine\r## 10 Obi-Wan Kenobi 182 77 auburn, white 57 male masculine\r## # ... with 77 more rows\r1  # bind_cols(one_c, three_c) # 에러 뜸, bind_rows와 차이점     \n11. mutate_all, mutate_if 모든 변수를 다 특정 함수를 거친 형태로 바꾸거나, 조건을 주어서 특정 함수를 거친 형태로 바꾸는 함수이다.\n  mutate_all, mutate_if 예시  1  iris %\u0026gt;% mutate_all(as.integer) %\u0026gt;% head() # Species까지 integer로 바꿔버림   ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 5 3 1 0 1\r## 2 4 3 1 0 1\r## 3 4 3 1 0 1\r## 4 4 3 1 0 1\r## 5 5 3 1 0 1\r## 6 5 3 1 0 1\r1  iris %\u0026gt;% mutate_if(is.double, as.integer) %\u0026gt;% head()   ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r## 1 5 3 1 0 setosa\r## 2 4 3 1 0 setosa\r## 3 4 3 1 0 setosa\r## 4 4 3 1 0 setosa\r## 5 5 3 1 0 setosa\r## 6 5 3 1 0 setosa\r  \n12. inner_join, left_join, right_join, full_join SQL에서 봤던 join 함수가 역시나 그대로 있다. 그런데 친절하게 무엇을 기준으로 join 했는지도 위에 알려줘서 좋은 것 같다.\n  join 예시  1  band_members   ## # A tibble: 3 x 2\r## name band ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones ## 2 John Beatles\r## 3 Paul Beatles\r1  band_instruments   ## # A tibble: 3 x 2\r## name plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John guitar\r## 2 Paul bass ## 3 Keith guitar\r1  band_members %\u0026gt;% inner_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 2 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles guitar\r## 2 Paul Beatles bass\r1  band_members %\u0026gt;% left_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 3 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones \u0026lt;NA\u0026gt; ## 2 John Beatles guitar\r## 3 Paul Beatles bass\r1  band_members %\u0026gt;% right_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 3 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles guitar\r## 2 Paul Beatles bass ## 3 Keith \u0026lt;NA\u0026gt; guitar\r1  band_members %\u0026gt;% full_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 4 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones \u0026lt;NA\u0026gt; ## 2 John Beatles guitar\r## 3 Paul Beatles bass ## 4 Keith \u0026lt;NA\u0026gt; guitar\r  \n13. semi_join, anti_join   semi_join, anti_join 예시  1 2  #이미 조인된 결과값 band_members %\u0026gt;% inner_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 2 x 3\r## name band plays ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles guitar\r## 2 Paul Beatles bass\r1 2  #band_members 중 join될 값 확인 band_members %\u0026gt;% semi_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 2 x 2\r## name band ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 John Beatles\r## 2 Paul Beatles\r1 2  #band_members 중 join되지 않을 값 확인 band_members %\u0026gt;% anti_join(band_instruments)   ## Joining, by = \u0026quot;name\u0026quot;\r## # A tibble: 1 x 2\r## name band ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mick Stones\r   참고 [1] slice와 relocate 예시는 slack 슬기로운통계생활을 참고하였습니다.\n","description":"","id":26,"section":"posts","tags":null,"title":"dplyr","uri":"https://jiwooblog.netlify.app/posts/r/dplyr/"},{"content":"purrr 패키지 훑어보기 purrr는 R에서 깔끔하게 반복 작업 처리하는 패키지입니다. Purrr 을 이용하면 반복작업을 Apply family 에 비해 더욱 직관적이고 쉽게 할 수 있습니다. purrr는 고양이 울음소리와 R의 합성어로, 로고는 아래와 같습니다.\n1  library(tidyverse)   목차  map, map2 pmap, invoke_map rerun every, some, none reduce, accumulate  map, map2   map, map2 예시  1 2 3 4 5  num \u0026lt;- c(1,2,4,5,7) num2 \u0026lt;- c(3,5,6,8,9) #list map(num, function(x){x^2})   ## [[1]]\r## [1] 1\r## ## [[2]]\r## [1] 4\r## ## [[3]]\r## [1] 16\r## ## [[4]]\r## [1] 25\r## ## [[5]]\r## [1] 49\r1  map2(num, num2, sum)   ## [[1]]\r## [1] 4\r## ## [[2]]\r## [1] 7\r## ## [[3]]\r## [1] 10\r## ## [[4]]\r## [1] 13\r## ## [[5]]\r## [1] 16\r1 2  #numeric vector map_dbl(num, function(x){x^2})   ## [1] 1 4 16 25 49\r1  map2_dbl(num, num2, sum)   ## [1] 4 7 10 13 16\r     map 실전활용- iris data  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  n_iris \u0026lt;- iris %\u0026gt;% group_by(Species) %\u0026gt;% nest() mod_fun \u0026lt;- function(df){ lm(Sepal.Length ~ ., data = df) } m_iris \u0026lt;- n_iris %\u0026gt;% mutate(model = map(data, mod_fun)) b_fun \u0026lt;- function(mod){ coefficients(mod)[[1]] } m_iris %\u0026gt;% transmute(Species, beta = map_dbl(model, b_fun))   ## # A tibble: 3 x 2\r## # Groups: Species [3]\r## Species beta\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 setosa 2.35 ## 2 versicolor 1.90 ## 3 virginica 0.700\r  \npmap, invoke_map   pmap, invoke_map 예시  1 2 3 4 5 6  x \u0026lt;- list(3, 6, 9) y \u0026lt;- list(10, 21, 30) z \u0026lt;- list(100, 200, 300) # pmap은 3개 이상의 리스트일 때 사용한다. pmap(list(x, y, z), sum)   ## [[1]]\r## [1] 113\r## ## [[2]]\r## [1] 227\r## ## [[3]]\r## [1] 339\r1 2  # invoke_map은 각각의 리스트에 다른 함수를 적용시키고 싶을 때 활용한다. invoke_map(list(runif, rnorm), list(list(n = 10), list(n = 5)))   ## [[1]]\r## [1] 0.07250356 0.88643385 0.53457501 0.45493290 0.95970419 0.57638199\r## [7] 0.62800763 0.63266467 0.64451000 0.77471082\r## ## [[2]]\r## [1] 0.1348207 -0.5144428 -0.8763132 0.8955774 0.3386345\r   rerun rerun은 샘플 데이터를 리스트 형식으로 형성하는 데에 효율적인 방법이다.\n  rerun 예시  1 2 3 4 5  # 예시 set.seed(2021) a \u0026lt;- 10 %\u0026gt;% rerun(rnorm(5)) a   ## [[1]]\r## [1] -0.1224600 0.5524566 0.3486495 0.3596322 0.8980537\r## ## [[2]]\r## [1] -1.92256952 0.26174436 0.91556637 0.01377194 1.72996316\r## ## [[3]]\r## [1] -1.0822049 -0.2728252 0.1819954 1.5085418 1.6044701\r## ## [[4]]\r## [1] -1.841476 1.623310 0.131389 1.481122 1.513318\r## ## [[5]]\r## [1] -0.9424433 -0.1856850 -1.1011246 1.2081153 -1.6249385\r## ## [[6]]\r## [1] 0.10537833 -1.45544335 -0.35401614 -0.09370004 1.10066863\r## ## [[7]]\r## [1] -1.9638251 -1.4479444 1.0194434 -1.4214171 -0.6045321\r## ## [[8]]\r## [1] -1.58347390 -1.28593235 -1.45468488 -0.08707112 0.50473644\r## ## [[9]]\r## [1] 0.11638871 1.76021373 -0.34511646 2.12000016 -0.03437749\r## ## [[10]]\r## [1] -0.7921541 1.4755152 -0.7255572 0.3123790 0.6919641\r1 2 3 4 5 6 7  # 위 함수는 아래의 함수와 같은 결과를 산출함을 알 수 있다. set.seed(2021) b \u0026lt;- list() for(i in 1:10){ b[[i]] \u0026lt;- rnorm(5) print(b[i]) }   ## [[1]]\r## [1] -0.1224600 0.5524566 0.3486495 0.3596322 0.8980537\r## ## [[1]]\r## [1] -1.92256952 0.26174436 0.91556637 0.01377194 1.72996316\r## ## [[1]]\r## [1] -1.0822049 -0.2728252 0.1819954 1.5085418 1.6044701\r## ## [[1]]\r## [1] -1.841476 1.623310 0.131389 1.481122 1.513318\r## ## [[1]]\r## [1] -0.9424433 -0.1856850 -1.1011246 1.2081153 -1.6249385\r## ## [[1]]\r## [1] 0.10537833 -1.45544335 -0.35401614 -0.09370004 1.10066863\r## ## [[1]]\r## [1] -1.9638251 -1.4479444 1.0194434 -1.4214171 -0.6045321\r## ## [[1]]\r## [1] -1.58347390 -1.28593235 -1.45468488 -0.08707112 0.50473644\r## ## [[1]]\r## [1] 0.11638871 1.76021373 -0.34511646 2.12000016 -0.03437749\r## ## [[1]]\r## [1] -0.7921541 1.4755152 -0.7255572 0.3123790 0.6919641\r1 2 3  for(i in 1:10){ print(a[[i]] == b[[i]]) }   ## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r## [1] TRUE TRUE TRUE TRUE TRUE\r1 2  # 참고로, base에 있는 replicate와 어떻게 다른지 한번 살펴보자! replicate(10, rnorm(5))   ## [,1] [,2] [,3] [,4] [,5] [,6]\r## [1,] -0.50029080 0.1037663 0.01604353 -0.9836134 -0.34823176 -0.2369450\r## [2,] -2.25586935 0.4272891 -0.18536431 0.5650808 -0.04298997 -0.9991415\r## [3,] 0.04374133 -0.1704815 0.39193326 1.6167519 -1.39755396 -1.3925426\r## [4,] -0.36881809 -1.5491403 -0.75671092 -0.2519641 1.49021633 0.9820053\r## [5,] -0.96022240 -1.5055999 0.23141761 -1.0558786 -1.03938712 0.3609409\r## [,7] [,8] [,9] [,10]\r## [1,] -0.3375092 -1.2400271 0.81061837 -0.1220018\r## [2,] -0.6433876 0.5339593 -0.29366457 -0.6467737\r## [3,] -2.1668853 -1.5882648 -0.05345832 -0.8678583\r## [4,] 0.6332890 -0.9909645 0.73518450 -0.5087003\r## [5,] -0.1449141 0.4832608 0.01498499 -2.0775844\r1  typeof(a)   ## [1] \u0026quot;list\u0026quot;\r1  typeof(replicate(10, rnorm(5)))   ## [1] \u0026quot;double\u0026quot;\r  \nevery, some, none 리스트형식을 summarise하는 데에 효율적인 방법이다.\n  every, some, none 예시  1 2  y \u0026lt;- list(0:10, 5.5) y   ## [[1]]\r## [1] 0 1 2 3 4 5 6 7 8 9 10\r## ## [[2]]\r## [1] 5.5\r1  y %\u0026gt;% every(is.numeric)   ## [1] TRUE\r1  y %\u0026gt;% every(is.integer)   ## [1] FALSE\r1  y %\u0026gt;% some(is.integer)   ## [1] TRUE\r1  y %\u0026gt;% none(is.character)   ## [1] TRUE\r  \nreduce, accumulate 함수를 재귀적으로(recursively) 적용시키는 효율적인 방법이다.\n  reduce, accumulate 예시  1 2  #1. reduce: reduce(1:10, sum)   ## [1] 55\r1 2  #2. accumulate accumulate(1:10, sum)   ## [1] 1 3 6 10 15 21 28 36 45 55\r1 2 3  #reduce 응용 버전 paste2 \u0026lt;- function(x, y, sep = \u0026#34;.\u0026#34;) paste(x, y, sep = sep) letters[1:4] %\u0026gt;% reduce(paste2)   ## [1] \u0026quot;a.b.c.d\u0026quot;\r  \n","description":"","id":27,"section":"posts","tags":null,"title":"purrr","uri":"https://jiwooblog.netlify.app/posts/r/purrr/"},{"content":"1. 추천 사이트  R for data science  2. 검색 팁 구글링 시, 뒤에 \u0026lsquo;Rpubs\u0026rsquo; 붙이기\n3. blogdown Auto-Knit 끄기 Ctrl+S 단축키로 수시로 저장하는 습관 때문에, Rmd 파일을 작업할 때 knit가 수시로 되어 작업속도에 영향을 미친다.\n이럴 때는 .Rprofile이라는 이름의 파일을 찾아서\n blogdown.knit.on_save = TRUE\n 라는 코드에서 TRUE를 FALSE로 바꿔주어야 한다.\n4. 자동정렬 단축키 ctrl + I\n5. python의 dictionary처럼 사용하기 1 2 3 4 5  # recode: case_when의 특수한 형태로서 데이터를 교체할 때 사용할 수 있을 것이다. tmp_char \u0026lt;- sample(c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;), 10, replace = TRUE) # !!!을 활용하면, python에서 dictionary 형태로 활용하는 것처럼 쓸 수 있다. level_key \u0026lt;- c(a = \u0026#34;apple\u0026#34;, b = \u0026#34;banana\u0026#34;, c = \u0026#34;carrot\u0026#34;) recode(tmp_char, !!!level_key)   ","description":"","id":28,"section":"posts","tags":null,"title":"R 꿀팁","uri":"https://jiwooblog.netlify.app/posts/r/r_tip/"},{"content":"\r\r데이터 설명\rdataset containing demographic data and laboratory data of 857 patients with acute coronary syndrome(ACS).\n# 변수별 NA값 확인\rcolSums(is.na(acs))\r## age sex cardiogenicShock entry ## 0 0 0 0 ## Dx EF height weight ## 0 134 93 91 ## BMI obesity TC LDLC ## 93 0 23 24 ## HDLC TG DM HBP ## 23 15 0 0 ## smoking ## 0\rcolSums(is.na(acs))[colSums(is.na(acs))\u0026gt;0]\r## EF height weight BMI TC LDLC HDLC TG ## 134 93 91 93 23 24 23 15\rna.var \u0026lt;- names(colSums(is.na(acs))[colSums(is.na(acs))\u0026gt;0])\r# 그래프로 보기\raggr(acs, prop=FALSE) \r# 상관관계\racs.na \u0026lt;- is.na(acs[,na.var])\rround(cor(acs.na),2)\r## EF height weight BMI TC LDLC HDLC TG\r## EF 1.00 0.46 0.45 0.46 0.13 0.12 0.13 0.11\r## height 0.46 1.00 0.99 1.00 0.20 0.19 0.20 0.21\r## weight 0.45 0.99 1.00 0.99 0.20 0.19 0.20 0.21\r## BMI 0.46 1.00 0.99 1.00 0.20 0.19 0.20 0.21\r## TC 0.13 0.20 0.20 0.20 1.00 0.98 1.00 0.75\r## LDLC 0.12 0.19 0.19 0.19 0.98 1.00 0.98 0.73\r## HDLC 0.13 0.20 0.20 0.20 1.00 0.98 1.00 0.75\r## TG 0.11 0.21 0.21 0.21 0.75 0.73 0.75 1.00\r\rMissing Data 종류\rMCAR (missing completely at random): 변수의 종류와 값 모두와 무관한 경우\rMAR (missing at random): 누락이 변수와는 관련있지만 그 값과는 관계 없는 경우\rMNAR (missing at not random): 누락의 원인이 있는 경우\r\r# na.omit과 complete.cases는 같은 역할을 한다.\rnrow(na.omit(acs)) == nrow(acs[complete.cases(acs),])\r## [1] TRUE\r\r추가로 알아볼 만한 주제\rNA imputation with Gibbs Sampler\rNA imputation with GAN(Generative Adversarial Network)\r\r\n\r\n참고사이트: https://rstudio-pubs-static.s3.amazonaws.com/192402_012091b9adac42dbbd22c4d07cb00d36.html\n\r\r","description":"","id":29,"section":"posts","tags":null,"title":"NA Imputation","uri":"https://jiwooblog.netlify.app/posts/r/na_imputation/"},{"content":"2021.1.30  또 다시 보니 11.18달러 과금으로 계속 증가하고 있어서 이것저것 최대한 삭제하고 계정도 해지했다. 90일 동안 또 다시 과금이 되는지 지속적으로 살펴봐야겠다. 그리고 해지한 메일 계정으로는 다시 계정 개설이 안된다길래 hanmail.net으로 수정한 후 삭제했다. 참고사이트는 두 곳을 참고했다. 참고사이트1, 참고사이트2 제발 다음에는 과금 안 되길\u0026hellip;AWS는 너무 어려운 것 같다. 나중에 AWS에 조금 더 관심이 생기면 참고사이트2의 다양한 포스트들을 참고해도 좋을 것 같다.  2021.1.15  다시 보니 10.68달러 과금으로 증가해있어서 얼른 RDS를 삭제했다. 교훈: 돌다리도\u0026hellip; 다음주부터는 주식 관련해서도 update 해야겠다.  2021.1.14  FastCampus 따라하다가 AWS 과금 4.13달러 발생했다\u0026hellip; Free Tier라고 모든게 다 공짜는 아니었다\u0026hellip;  2021.1.11  Data Engineering에 관심이 생겼다.  2021.1.8  First Course in Bayesian Statistical Methods(이하 FCB) 1장을 정리해서 올렸다. 오래된 Github Repository를 정리했는데\u0026hellip;이들을 삭제하면 각 repository와 연관된 commit 기록들도 사라지기 때문에 잔디 심어둔 것들이 사라져버리는 사태가 발생했다\u0026hellip;아깝다\u0026hellip; 교훈: repository는 신중하게 만들고, 가능하면 지우지 말자\u0026hellip;  2021.1.7  LaTex 수식 삽입기능 추가 ( 참고사이트 )  2021.1.4  baseURL 수정 댓글 기능 확인  블로그 제작일: 2021년 1월 2일 토요일  ","description":"","id":30,"section":"updates","tags":null,"title":"January 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_01/"},{"content":"Part 2-2. 데이터 엔지니어 기초 다지기 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n3. SQLite Studio SQLite Studio 다운로드\n데이터 다운로드  editor 여는 법: Tools \u0026gt; Open SQL Editor (or Alt + E) SQLite과 MySQL을 포함한 다른 프로그램들과 코드가 다른 것들이 사소하게 있을 수 있다.  SQL 기본 문법 (1) SELECT 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  SELECT * FROM Salaries LIMIT 10; SELECT * FROM Salaries ORDER BY salary DESC LIMIT 10; SELECT * FROM Salaries WHERE yearID = \u0026#39;2010\u0026#39; AND lgID = \u0026#39;AL\u0026#39; ORDER BY salary DESC LIMIT 20; --SUM, AVG SELECT SUM(salary) FROM Salaries WHERE playerID = \u0026#39;rodrial01\u0026#39;; --Concat, Count, Group By SELECT nameFirst || \u0026#39; \u0026#39; || nameLast AS name FROM People Limit 10; SELECT nameFirst || \u0026#39; \u0026#39; || nameLast AS name FROM People Where playerID = \u0026#39;rodrial01\u0026#39;; SELECT COUNT(DISTINCT(nameFirst || \u0026#39; \u0026#39; || nameLast)) FROM People; SELECT nameFirst || \u0026#39; \u0026#39; || nameLast AS name, COUNT(*) FROM People GROUP BY name HAVING COUNT(*) \u0026gt; 1; SELECT teamID, SUM(Salary) as total_salary FROM Salaries GROUP BY teamID ORDER BY total_salary DESC;   SQL 기본 문법 (2) JOIN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  --Join SELECT t2.nameFirst ||\u0026#39; \u0026#39;||t2.nameLast AS name, t1.salary FROM Salaries t1 JOIN People t2 ON t2.playerID = t1.playerID ORDER BY salary DESC LIMIT 20; --Quiz. Top paid player for each team in 2010 SELECT t1.teamID, t2.nameFirst||\u0026#39; \u0026#39;||t2.nameLast AS name, t1.salary --using MAX(salary) instead of ORDER BY would be more efficient FROM Salaries t1 JOIN People t2 ON t2.playerID = t1.playerID WHERE t1.yearID = \u0026#39;2010\u0026#39; GROUP BY teamID ORDER BY salary DESC; -- Left Join, Right Join SELECT t1.playerID, COUNT(*) FROM People t1 LEFT JOIN AllstarFull t2 ON t2.playerID = t1.playerID GROUP BY 1 ORDER BY COUNT(*) DESC LIMIT 20;   SQL 기본 (3) 데이터 타입들 및 키 값들, 테이블 생성 Primary Key: 빠른 처리, 중복 처리를 위해서 설정하기도 함!\nForeign Key: 다른 테이블에서 온 칼럼 처리\nUnique: 중복 처리 방지\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  -- 데이터베이스 테이블 생성 CREATE TABLE mytable (id INT, name VARCHAR(255), debut DATE); CREATE TABLE mytable2 (id INTEGER PRIMARY KEY AUTOINCREMENT, name VARCHAR(255), debut DATE); INSERT INTO mytable2 (name, debut) VALUES (\u0026#39;jiwoo\u0026#39;, \u0026#39; 2000-09-01\u0026#39;); SELECT * FROM mytable2; --INSERT INSERT INTO mytable2 (name, debut) VALUES (\u0026#39;jiwoo\u0026#39;, \u0026#39;2000-09-05\u0026#39;); SELECT * FROM mytable2; --Update UPDATE mytable2 SET debut = \u0026#39;2010-09-01\u0026#39; WHERE id = 1; --Replace REPLACE INTO mytable2 (id, name, debut) VALUES (5, \u0026#39;jiwoo2\u0026#39;, \u0026#39;2015-09-01\u0026#39;); -- Update는 기존의 값이 없다면 아무런 행동도 하지 않지만, Replace는 기존의 값이 없다면 새로 만들어버린다는 차이점이 있다.  --Insert Or Ignore INSERT OR IGNORE INTO mytable2 (id, name, debut) VALUES (1, \u0026#39;jiwoo3\u0026#39;, \u0026#39;2010-09-11\u0026#39;); --이미 id가 1인 행이 있을 경우, 그냥 insert into만 하면 \u0026#39;unique constraint failed\u0026#39;가 뜨지만 or ignore을 추가해주면 괜찮다.  -- Delete, ALter, Drop -- 아주아주 신중하게 써야하는 커맨드들이다! SELECT * FROM mytable2; Delete FROM mytable2 WHERE id=1; ALTER TABLE mytalbe2 RENAME TO players; ALTER TABLE players ADD COLUMN DOB date; SELECT * FROM players; DROP TABLE mytable;  \nSQL 기본 (4) Functions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  -- Functions(1) 기본처리 및 연산 SELECT * FROM players; SELECT SUBSTR(name, 1, 3) FROM players; SELECT UPPER(name) FROM players; SELECT AVG(LENGTH(name)) FROM players; --MAX, AVG, COUNT, SUM  -- Functions(2) 날짜데이터, Case When SELECT CURRENT_TIMESTAMP; --UTC기준 SELECT DATE(\u0026#39;NOW\u0026#39;); SELECT DATETIME(CURRENT_TIMESTAMP, \u0026#39;+1 DAY\u0026#39;); SELECT id, name, CASE WHEN name = \u0026#39;jiwoo\u0026#39; THEN \u0026#39;OK\u0026#39; WHEN name = \u0026#39;jiwoo2\u0026#39; THEN \u0026#39;OK2\u0026#39; ELSE \u0026#39;No OK\u0026#39; END AS ok_name, --CASE WHEN부터 여기까지가 variable 하나!  debut FROM players;   ","description":"데이터 엔지니어 기초 다지기","id":31,"section":"posts","tags":null,"title":"SQL 기초 (SQLite)","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part2_2/"},{"content":"\r\rChapter 02. Belief, Probability and Exchangeability\r본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\r이번 장의 목표는 independence와 exchangeability를 이해하는 것이다. 이를 바탕으로 de Finetti’s theorem이 Bayesian에 갖는 의의를 이해한다면, 베이즈 통계를 공부할 준비가 된 것이다.\nBelief functions and Probabilities\r$Be()$는 belief function이라고 하자. 예를 들어, $Be(F) \u0026gt; Be(G)$는 G보다 F를 더 믿는다고 해석하면 된다. F, G, H를 아래와 같은 각각의 상황이라고 가정해보자.\n\rF : 좌파 후보자를 투표하는 경우 G : 소득이 하위 10%에 속하는 경우 H : 대도시에 거주하는 경우\n\rAxioms of beliefs\r$Be($not $H|H) \\le Be(F|H) \\le Be(H|H)$\r$Be(F $ or $G|H) \\ge max(Be(F|H), Be(G|H))$\r$Be(F $ and $G|H)$ can be drvied from $Be(G|H)$ and $Be(F|G $ and $H)$\r\r\rAxioms of probability\r$0 = Pr($not $H|H) \\le Pr(F|H) \\le Pr(H|H) \\le = 1$\r$Pr(F \\cup G|H) = Pr(F|H) + Pr(G|H)$ if $F \\cap G = \\emptyset$\r$Pr(F \\cap G|H) = Pr(G|H)Pr(F|g \\cap H)$\r\rbelief와 probability에 대한 각각의 공리들이 매칭되므로, 우리는 믿음의 정도를 계산할 때 확률함수를 계산하는 것처럼 다뤄도 무방하다고 결론낼 수 있다.\n\r\rConditional Independence\r사건 F와 G는 아래와 같은 상황에서 조건부 독립(conditional independence)이라고 한다.\r\\[Pr(F \\cap G|H) = Pr(F|H)Pr(G|H)\\]\r이를 풀어서 해석해보자면, H를 알고 있는 상황에서, 추가적으로 G에 대해서 알게 되는 것은 F에 대한 믿음을 변화시키는 데에 영향이 없다는 것이다. 위를 통해서 아래를 알 수 있는 것이다.\r\\[Pr(F|H \\cap G) = Pr(F|H) \\]\n\rExchangeability\r$Y_1, ..., Y_n$이 있을 때, 이 순서를 어떻게 섞더라도 결합확률은 바꾸지 않을 때 exchangeable하다고 한다. 이는 직관적으로 풀어쓴 것이며, 다시 한 번 수학적 정의로 자세히 써보자면 아래와 같다.\rLet $p(y_1, ... y_n)$ be the joint density of $Y_1, ..., Y_n$. If $p(y_1, ..., y_n) = p(y_{\\pi_1}, ..., y_{\\pi_n})$ for all permutations $\\pi$ of {1, …, n}, then $Y_1, ..., Y_n$ are exchangeable.\n\\[\\begin{equation}\r\\left.\\begin{aligned}\rY_1, ..., Y_n|\\theta \\text{ i.i.d} \\\\ \\theta \\sim p(\\theta)\r\\end{aligned}\\right\\} \\Rightarrow Y_1, ... Y_n \\text{ are exchangeable}\r\\end{equation}\\]\n\rde Finetti’s Theorem\r만약 $Y_1, ..., Y_n$이 exchangeability를 만족한다면, 아래와 같이 말할 수 있다.\n\\[p(y_1, ..., y_n) = \\int{\\Bigg\\{\\prod_{1}^{n}p(y_i|\\theta)\\Bigg\\} \\:p(\\theta)d\\theta} \\\\\r\\text{for some parameter} \\: \\theta\\]\n이는 확률변수 $Y_1, ..., Y_n$에 대해서 exchangeability를 만족한다면, $p(y_1, ..., y_n)$에 대해 $\\theta$라는 parameter를 활용하여 위와 같은 수식으로 나타낼 수 있다는 것이다. 그렇다면 이 정리가 베이지안에게 어떤 의미를 갖는 것일까? 이는 사전확률분포(prior model)와 가능도함수(sampling model)가 belief model $p(y_1, ..., y_n)$에 의존함을 의미한다. 풀어서 이야기하자면, parameter $\\theta$가 확률론자가 주장하는 것처럼 미지의 고정된 값이 아니라, 어떤 분포를 갖는 확률변수로 볼 수 있다는 것이다. (그리고 그것을 우리는 사전확률분포 prior distribution이라고 부른다.)\n\r주의사항\rBayes’ rule은 데이터를 접한 이후, 우리의 믿음이 어떻게 업데이트되는지에 대한 수식이다.\r여기서 헷갈리면 안되는 것이 있다. Bayes’ rule은 우리의 믿음이 어때야 하는지(should be)에 대해서 이야기하고 있는 것이 아니라 어떻게 변해야 하는지(should change)에 대해서 이야기하는 것이다.\n\rConclusion\r믿음(Belief)도 확률(Probability)로써 이야기할 수 있다.\rparamter $\\theta$는 분포를 갖는 확률변수이다.\r\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다.\r\n\r\r","description":"Belief, Probability and Exchangeability","id":32,"section":"posts","tags":null,"title":"Exchangeability","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/"},{"content":"Exponential Family 한국어로는 지수족 또는 지수류라고도 하지만, 영어로 보는 편이 직관적으로 받아들이는 데에 편할 것이다.\n$f(x;\\theta) = \\begin{cases}\rexp\\big[p(\\theta)K(x) + s(x) + q(\\theta)\\big] \u0026amp; x \\in S \\\\\r0 \u0026amp; o.w\r\\end{cases} $\n S does not depend on $\\theta$ $p(\\theta)$ is a nontrivial continuous function of $\\theta \\in \\Omega$\n3-1. If X is continuous, $K'(x) \\neq 0$ and $s(x)$ is continuous function.\n3-2. If X is discrete, $K(x)$ is nontrivial function.  또는\n$ f(y|\\phi) = \\begin{cases}\rh(y)c(\\phi)exp\\big[\\phi K(y)\\big] \u0026amp; y \\in S \\\\\r0 \u0026amp; o.w\r\\end{cases} $\n S does not depend on $\\phi$ $\\phi$ is a nontrivial continuous function of $\\theta \\in \\Omega$\n3-1. If X is continuous, $K'(y) \\neq 0$ and $h(y)$ is continuous function.\n3-2. If X is discrete, $K(y)$ is nontrivial function.  첫번째는 Hogg 책을 기준으로 서술한 것이며, 두번째는 FCB 기준으로 서술한 것이다.\n즉 위는 Frequentist 입장, 아래는 Bayesian 입장이라고 보면 된다. 수식에 있어서 큰 차이는 없지만, parameter가 given인지 아닌지가 차이라고 볼 수 있다.\nSufficient Statistic 우선은 Hogg책 서술을 기준으로 이야기해보자.\n$X_1, ..., X_n \\text{ ~ iid } f(x;\\theta)$이고 $f(x;\\theta)$가 exponential family라고 한다면,\n$Y_1 = \\sum_{i=1}^{n}K(X_i)$는 $\\theta$에 대한 완전충분통계량(complete sufficient statistic)이다.\n그렇다면 Sufficient 하다는 것의 의미는 무엇일까? Defintion: $p(X_1, ..., X_n|Y_1=y_1)$가 $\\theta$에 의존하지 않는다.\n즉 $\\frac{p(X_1, ..., X_n, Y_1=y_1;\\theta)}{p(Y_1=y_1; \\theta)}$를 계산할 때, $\\theta$에 의해 값이 좌지우지 되지 않는다는 것이다.\n이를 풀어서 말하자면, 각각 $X_1,...,X_n$ 데이터를 직접 알지는 못하더라도 이들에 대한 정보가 $Y_1$에 들어가있다는 것이다. 그렇기 때문에 $Y_1$ 값을 알게 되면 $X_1,...,X_n$의 joint probability를 계산할 수 있다. 그래서 **충분(sufficient)**하다는 것이다.\n","description":"지수족","id":33,"section":"posts","tags":null,"title":"Exponential Family","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/exponential_family/"},{"content":"2021.03.06  문득 빈도론자와 베이지안의 논쟁이 쿤과 포퍼의 과학철학 논쟁과 비슷하다는 생각을 했다. 빈도론자는 가설검정이라는 것을 하며 귀무가설과 대립가설을 놓고 검정통계량을 통해 기각하거나 채택을 한다. 반면, 베이지안은 철저하게 베이즈 이론만을 기반으로 하여 분포를 제시하여 가능성을 제시한다. 포퍼가 반증가능성을 이야기했다면, 쿤은 이를 비판하며 패러다임의 전환에 대해 이야기했다.  2021.03.28  대학원 진학을 하기 위해 어느 정도 고민을 마쳤지만, 생각보다 마음이 덜 잡힌 것 같다. 열품타를 사용하고 있는데 생각보다 좋은 것 같다. 더 잘 활용해보도록 해야겠다.  ","description":"","id":34,"section":"updates","tags":null,"title":"March 2021","uri":"https://jiwooblog.netlify.app/updates/diary/2021_03/"},{"content":"비모수통계학 비모수통계학(nonparametric statistics)는 모수적 검정의 가정이 충족되지 못하거나, 데이터 형식이 순서형일 경우처럼 일반적이지 않은 경우에 사용하는 통계적 방법론에 대한 연구를 한다.\nt검정에서 독립표본 t-검정과 대응표본 t-검정이 있습니다. 이에 대응하여 비모수통계학에서는 Wilcoxon rank-sum test(Mann-Whitney U-test)와 Wilcoxon signed-rank test가 있습니다.\n1. Wilcoxon rank-sum test Mann-Whitney U-test라고도 한다.\n독립표본 t-검정의 비모수 버전이다.\n2. Wilcoxon signed-rank test 대응표본 t-검정의 비모수 버전이다.\n참고 [1] 연세대학교 심리통계 2019-1 수업자료 [2] https://blog.naver.com/istech7/50152096673\n[3] http://www.incodom.kr/R%ED%99%9C%EC%9A%A9/Wilcoxon_Signed-Rank_Test\n[4] https://dermabae.tistory.com/159\n","description":"","id":35,"section":"posts","tags":null,"title":"비모수통계학","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/nonparametric/"},{"content":"ANOVA ANOVA는 Analysis of Variance의 약자로, 한국어로는 분산 분석이라고 한다. 집단 간 분산과 집단 내 분산을 비교하여 처리효과가 있는지 살펴보는 통계방법이다.\n","description":"분산분석","id":36,"section":"posts","tags":null,"title":"ANOVA","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/anova/"},{"content":"t-검정 t-검정은 모집단의 분산이나 표준편차를 알지 못할 때, 모집단을 대표하는 표본으로부터 추정된 분산이나 표준편차를 가지고 검정하는 방법이다.\n1. 단일표본 t-검정 모집단의 평균이 특정검정값과 같은지 확인하는 통계빵법이다.\n2. 독립표본 t-검정 독립된 두 집단 간 비교하는 방법으로, 서로 다른 두 모집단으로부터 데이터가 추출되었을 때 시행한다.\n1 2 3 4 5 6 7 8  # independent t-test x1 = 3.6667 x2 = 6.0667 s1 = 2.60951 s2 = 2.37447 n1 = n2 = 15 sp = sqrt((s1^2*(n1-1) + s2^2*(n2-1)) / ((n1-1) + (n2-1))) t = (x1-x2) / (sp*sqrt(1/n1 + 1/n2))   3. 대응표본 t-검정 한 집단 내 비교하는 방법으로, 하나의 모집단으로부터 데이터를 반복 추출하였을 때 시행한다.\n1 2 3 4 5 6 7  # paired t-test xa \u0026lt;- c(1,10,2,6,5,2,3,4,2,1,2,2,3,4,8) xb \u0026lt;- c(2,10,5,7,6,5,6,9,6,7,8,5,4,2,9) n \u0026lt;- length(xa) d_bar = (sum(xa)-sum(xb))/n d_sd = sqrt(sum((xa-xb-d_bar)^2)/(n-1)) t = d_bar / (d_sd/sqrt(n))   참고 [1] 연세대학교 심리통계 2019-1 수업자료 [2] https://wikidocs.net/34009\n","description":"t-test","id":37,"section":"posts","tags":null,"title":"t 검정","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/t_test/"},{"content":"신뢰구간과 신용구간 간단하게 구분하자면, 신뢰구간은 빈도주의자가, 신용구간은 베이지안이 사용하는 것이다.\n일반적으로 신용구간을 신뢰구간으로 착각하는 경우가 많다.\n신뢰구간 (Confidence Interval) “If we repeat the experiment infinitely many times, 95% of the experiments will capture the population parameter in their confidence intervals.”\r 해석하자면, 무수히 많이 반복하여 데이터를 얻고 신뢰구간을 산출한다면, 그 수많은 신뢰구간 중 95%는 모수를 갖고 있을 것으로 신뢰한다는 의미이다. 그렇기 때문에 한번의 실험결과만으로 신뢰구간을 구하고 이를 활용하는 데에는 다소 무리가 있어보인다. 하지만 중심극한정리를 통해 정규성을 확보함으로써 어느 정도의 논리적 비약은 막는다고 빈도론자들은 생각한다.\n신용구간 (Credential Interval) “There is 95% probability/plausibility/likelihood that the population parameter lies in the interval.”\r 해당 구간에 모수가 있을 확률을 구한다. 95%가 되는 구간은 무수히 많이 잡을 수 있겠지만, 그중에서 HPD(Highest posterior Density) region을 구하여 활용한다. 이는 x축에 평행한 선을 위에서부터 내려오면서 적용하여 그 사이 영역의 넓이가 95%가 되는지 확인하는 방법으로 계산한다.\n신용구간을 사용함으로써 얻을 수 있는 장점은 크게 두 가지로 요약 된다.\n 사후분포가 정규분포가 아니더라도 확률계산을 할 수 있다. 이는 정의역에 대한 전제를 고려할 수 있다는 장점으로 이어진다. 사전확률을 고려함으로써 신뢰구간보다 빠르게 신용구간을 구할 수 있다.  참고사이트 [1] https://towardsdatascience.com/do-you-know-credible-interval-e5b833adf399\n[2] FCB figure 3.6\n","description":"","id":38,"section":"posts","tags":null,"title":"신뢰구간, 신용구간","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/interval/"},{"content":"정규성 검정 ","description":"","id":39,"section":"posts","tags":null,"title":"정규성 검정","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/normality/"},{"content":"삼각함수 공식(Trigonometric Functions) $\\sin^2{x} + \\cos^2{x} = 1 $\n$\\tan^2{x} + 1 = \\sec^2{x} $\n$\\cos{2x} = 1 - 2\\sin^2{x} = 2\\cos^2{x} -1 $\n$\\frac{d}{dx}\\tan{x} = \\sec^2{x} $\n$\\frac{d}{dx}\\sec{x} = \\sec{x}\\tan{x} $\n","description":"","id":40,"section":"posts","tags":null,"title":"삼각함수공식","uri":"https://jiwooblog.netlify.app/posts/statistics/calculus/3_trigonometry/"},{"content":"Pygame 기초 본 포스팅은 해당 사이트(https://kkamikoon.tistory.com/129)를 적극참고 하였습니다.\n코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  import pygame pygame.init() BLACK = (0,0,0) WHITE = (255,255,255) BLUE = (0,0,255) GREEN = (0,255,0) RED = (255,0,0) size = [400,300] screen = pygame.display.set_mode(size) pygame.display.set_caption(\u0026#39;Game Title\u0026#39;) done = False clock = pygame.time.Clock() while not done: clock.tick(10) for event in pygame.event.get(): if event.type == pygame.QUIT: done = True screen.fill(WHITE) pygame.draw.polygon(screen, GREEN, [[30,150], [125,100], [220,150]], 5) pygame.draw.polygon(screen, GREEN, [[30,150], [125,100], [220,150]],0) pygame.draw.lines(screen, RED,False, [[50,150], [50,250], [200,250], [200,150]],5) pygame.draw.rect(screen, BLACK, [75,175,75,50],5) pygame.draw.rect(screen, BLUE, [75,175,75,50],0) pygame.draw.line(screen, BLACK, [112,175], [112,225],5) pygame.draw.line(screen, BLACK, [75,200], [150,200],5) pygame.display.flip() #출처: https://kkamikoon.tistory.com/129   결과물 ","description":"pygame 기초","id":41,"section":"posts","tags":null,"title":"pygame[기초]","uri":"https://jiwooblog.netlify.app/posts/python/pygame_1/"},{"content":"오차 \u0026amp; 잔차  오차(error): 모집단 회귀식 예측값 - 실제 관측값 잔차(residual): 표본집단 회귀식 예측값 - 실제 관측값  ","description":"오차와 잔차의 차이","id":42,"section":"posts","tags":null,"title":"오차와 잔차","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/error_residual/"},{"content":"국내/해외 지수 및 환율 1월 넷째주    INDEX 1.18(월) 1.19(화)     코스피 3013.93 ▼71.97(2.33%) 3092.66 ▲78.73(2.61%)   코스닥 944.67 ▼19.77(2.05%) 957.75 ▲13.08(1.38%)   S\u0026amp;P500 3768.25 ▼27.29(0.72%) 3768.25 ▼27.27(0.72%)   나스닥 12998.50 ▼114.14(0.87%) 12998.50 ▼114.14(0.87%)   환율 1103.90 ▲4.50(0.41%) 1102.90 ▼1.00(0.09%)    1월 셋째주 1.15(금) 오후 10:36 기준    INDEX VALUE Change Rate     코스피 3085.90 ▼64.03(2.03%)   코스닥 964.44 ▼15..85(1.62%)   S\u0026amp;P500 3795.54 ▼14.30(0.38%)   나스닥 13112.65 ▼16.31(0.12%)   환율 1099.40 ▲1.40(0.13%)    ","description":"","id":43,"section":"updates","tags":null,"title":"주식","uri":"https://jiwooblog.netlify.app/updates/stock/"},{"content":"Part 3. API는 무엇인가 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. API 정의  Application Programming Interface 두 개의 시스템이 서로 상호작용하기 위한 인터페이스(데이터 주고 받기!) 일반적으로 API는 REST API를 지칭한다. ex) Web API: 웹을 통해 외부 서비스들로부터 정보를 불러오는 API  2. API 접근 권한  Authentication: Identity가 맞다는 증명 Authorization: API를 통한 어떠한 액션을 허용 둘은 다르다! Athentication을 하였다고 하더라도 Authorization을 허용하지 않을 수 있다! Security 이슈가 중요하다.  API Key?  보통 Request URL 혹은 Request Header에 포함되는 긴 스트링 ex) Google Maps Platform \u0026gt; Geocoding API  https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA\u0026amp;key=YOUR_API_KEY 여기서 YOUR_API-KEY 이부분을 채워주지 않으면, request denied가 뜨게 된다.    Baisc Auth  username:password와 같은 credential을 Base64로 인코딩한 값을 Request Header 안에 포함  OAuth 2.0  End User \u0026lt;=\u0026gt; My App \u0026lt;=\u0026gt; Server(ex. Spotify) (1) App에서 End User한테 생일, 전화번호, 플레이리스트와 같은 정보를 가져가는 것에 대해서 동의를 받는다. (2) 동의서를 받아왔으니 Server한테 API 요청을 하고 그에 맞는 데이터를 요구한다.  3. Spotify Web API  Spotify 직접 방문해보기  4. Endpoints \u0026amp; Methods  Resource: API를 통해 리턴된 정보 Endpoint: Resource 안에는 여러 개의 Endpoints가 존재 Method: 자원 접근에 허용된 행위(GET, POST, PUT, DELETE)     Method Action     GET 해당 리소스를 조회하고 정보를 가져온다.   HEAD 응답코드와 HEAD만 가져온다.   POST 요청된 리소스를 생성한다.   PUT 요청된 리소스를 업데이트 한다.   DELETE 요청된 리소스를 삭제한다.    5. Parameters  Parameters: Endpoint를 통해 Request할 때 같이 전달하는 옵션들     Type 내용     Header Request Header에 포함. 주로 Authorization에 관련   Path Query String(?) 이전에 Endpoint Path 안에 포함. ex) id   Query String Query STring(?) 이후에 포함. ex) ?utm_source=facebook\u0026amp;\u0026hellip;   Request Body Request Body 안에 포함. 주로 JSON 형태    \n","description":"API는 무엇인가","id":44,"section":"posts","tags":null,"title":"API","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part3_1/"},{"content":"Chapter 03. One-parameter Models 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\nBinomial Model Prior: $\\theta \\text{ ~ } Beta(a,b)$\nLikelihood: $Y|\\theta \\text{ ~ } Binomial(n, \\theta) $\nPosterior: $\\theta|y \\text{ ~ } Beta(a+y, b+n-y) $ a: prior 성공횟수, b: prior 실패횟수, $\\omega$=a+b: concentration $E[\\theta|y] = \\frac{a+y}{a+b+n} = \\frac{n}{a+b+n}\\times\\frac{y}{n} + \\frac{a+b}{a+b+n}\\times\\frac{a}{a+b}$ where $\\frac{y}{n}$ = sample mean, $\\frac{a}{a+b}$ = prior expectation Posterior Predictive\n$n^* = 1$일 때 : $\\tilde{Y}|y \\text{ ~ } Ber(\\frac{a+y}{a+b+n})$\n$n^* \\geq 2$일 때 : $p(\\tilde{Y}=y^*|y) = \\binom{n^*}{y^*}\\frac{B(a+y+y^*, b+n+n^*-y-y^*)}{B(a+y, b+n-y)}$ where $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} $\nPoisson Model Prior: $\\theta \\text{ ~ } Gamma(a,b) $\nLikelihood: $Y_1, ..., Y_n \\text{ ~ iid. } Poisson(\\theta)$\nPosterior: $\\theta|y_1, ..., y_n \\text{ ~ } Gamma(a+\\sum_{i=1}^{n}{y_i}, b+n) $ a: sum of counts from b prior observations, b: number of prior observations $E[\\theta|y_1, ..., y_n] = \\frac{a+\\sum y_i}{b+n} = \\frac{b}{b+n}\\frac{a}{b} + \\frac{n}{b+n}\\frac{\\sum y_i}{n}$ Posterior Predictive: $\\tilde{Y}=y^*|y_1, ..., y_n \\text{ ~ } NB(a+\\sum y_i+y^*, \\frac{b+n}{b+n+1}) $\n단, 여기서 $Negative Binomial$은 성공이 아닌 실패횟수를 세는 분포 형태이다. 자세한 내용은 확률분포 포스팅에서 확인하자.\nExponential Family exponential family(지수족)의 pdf 또는 pmf는 다음과 같은 형식으로 표현될 수 있어야 한다.\n$ p(y_i|\\phi) = h(y)c(\\phi)exp\\big[\\phi K(y)\\big]$\nexponential family 자체에 대해서 보다 자세한 것은 해당 포스팅을 참고하자.\nPrior\n$$\\begin{align}\rp(\\phi) \u0026amp;= k(n_0, t_0)c(\\phi)^n_0e^{n_0t_0\\phi} \\\\\r\u0026amp;\\propto c(\\phi)^n_0e^{n_0t_0\\phi}\r\\end{align}$$\nLikelihood\n$$L(\\phi|y_1,...,y_n) \\propto c(\\phi)^n exp(\\phi \\sum_{i=1}^{n}K(y_i))$$\nPosterior\n$$\\begin{align}\rp(\\phi|y) \u0026amp;\\propto p(\\phi)f(y|\\phi) \\\\\r\u0026amp;\\propto c(\\phi)^{n_0}e^{n_0t_0\\phi} \\cdot c(\\phi)^n exp(\\phi \\sum_{i=1}^{n}K(y_i)) \\\\\r\u0026amp;\\propto c(\\phi)^{n_0}exp\\big[n_0t_0\\phi + \\phi \\sum_{i=1}^{n}K(y_i) \\big] \\\\\r\u0026amp;\\propto c(\\phi)^{n_0}exp\\big[ \\phi \\big( n_0t_0 + n\\frac{\\sum_{i=1}^{n}K(y_i)}{n} \\big)\\big]\r\\end{align}$$\n여기서 $n_0$와 $t_0$은 각각 prior sample size와 prior guess of $K(Y)$를 뜻한다.\n  참고: FCB 책 표현  Prior: $p(\\theta) \\propto g(\\theta)^\\eta \\ exp(\\phi(\\theta)^T \\ \\nu)$\nLikelihood: $p(y|\\theta) = \\prod_{i=1}^{N} f(y_i) \\ g(\\theta)^N \\ exp(\\phi(\\theta)^T \\ \\sum_{i=1}^{N}s(y_i))$ where $\\sum_{i=1}^{N}s(y_i))$ is sufficient statistics $t(y)$\nPosterior: $p(\\theta|y) \\propto g(\\theta)^{\\eta+N} \\ exp(\\phi(\\theta)^T \\ (\\nu + t(y)) $   Conjugate Prior prior와 posterior의 확률분포형태가 같을 수 있도록 prior을 설정하면 이를 conjugate prior라고 한다.\n위의 예시 외에도 Normal model 등이 있는데, 이들에 대해서는 다음에 이어서 살펴보도록 하겠다.\n다양한 예시들은 위키백과에 자세히 나와있으니 궁금한 사람들은 추가적으로 살펴보아도 좋겠다.\n주의사항 사후확률분포가 차이가 많이 나는 것과 사후예측치가 차이가 많이 나는 것의 차이를 알아두어야 한다. 즉, {${\\theta_1 \u0026gt; \\theta_2}$}와 {$\\tilde{Y_1} \u0026gt; \\tilde{Y_2}$}는 다르다.\n Strong evidence of a difference between two populations does not mean that the difference itself is large.\n Conclusion Conjugacy를 잘 알아두자. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"One-parameter Models","id":45,"section":"posts","tags":null,"title":"Conjugacy","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/"},{"content":"base 함수 중 유용한 함수 훑어보기 1 2  library(tidyverse) library(palmerpenguins)   1. split   split 예시  1 2 3 4 5 6 7 8 9  #1col기준으로 분리 penguins %\u0026gt;% split(.$species) -\u0026gt; split1 #2col기준으로 분리 penguins %\u0026gt;% split(list(.$species, .$island)) -\u0026gt; split2 #10row기준으로 분리(row개수 안맞으면 error) splitrow \u0026lt;- rep(1:35, c(rep(10, 34), 4)) splitrow   ## [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3\r## [26] 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5\r## [51] 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8\r## [76] 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 10 10 10 10 10 10 10 10 10 10\r## [101] 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13\r## [126] 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15\r## [151] 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18\r## [176] 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20\r## [201] 21 21 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23\r## [226] 23 23 23 23 23 24 24 24 24 24 24 24 24 24 24 25 25 25 25 25 25 25 25 25 25\r## [251] 26 26 26 26 26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28\r## [276] 28 28 28 28 28 29 29 29 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30\r## [301] 31 31 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33\r## [326] 33 33 33 33 33 34 34 34 34 34 34 34 34 34 34 35 35 35 35\r1  nrow(penguins)   ## [1] 344\r1  penguins %\u0026gt;% split(splitrow) -\u0026gt; split3      ","description":"","id":46,"section":"posts","tags":null,"title":"base","uri":"https://jiwooblog.netlify.app/posts/r/base/"},{"content":"Pygame 응용 본 포스팅은 해당 영상을 적극참고 하였습니다.\n코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282  import os import pygame ############################################################################## # 기본 초기화 (반드시 해야 하는 것들) pygame.init() # 초기화 # 화면 크기 설정 screen_width = 640 # 가로 크기 screen_height = 480 # 세로 크기 screen = pygame.display.set_mode((screen_width, screen_height)) # 화면 타이틀 설정 pygame.display.set_caption(\u0026#39;Jiwoo Pang\u0026#39;) # FPS clock = pygame.time.Clock() ############################################################################## # 1. 사용자 게임 초기화 (배경화면, 게임 이미지, 좌표, 속도, 폰트 등) current_path = os.path.dirname(__file__) # 현재 파일의 위치 반환 image_path = os.path.join(current_path, \u0026#39;images\u0026#39;) # images 폴더 위치 반환 # 배경 만들기 background = pygame.image.load(os.path.join(image_path, \u0026#39;game_background.jpg\u0026#39;)) # 스테이지 만들기 stage = pygame.image.load(os.path.join(image_path, \u0026#39;stage.png\u0026#39;)) stage_size = stage.get_rect().size stage_height = stage_size[1] # 스테이지 높이 위에 캐릭터를 두기 위해 사용 # 캐릭터 만들기 character = pygame.image.load(os.path.join(image_path, \u0026#39;cat.png\u0026#39;)) character_size = character.get_rect().size character_width = character_size[0] character_height = character_size[1] character_x_pos = (screen_width/2) - (character_width/2) character_y_pos = screen_height - character_height - stage_height # 캐릭터 이동 방향 character_to_x = 0 # 캐릭터이동 속도 character_speed = 5 # 무기 만들기  weapon = pygame.image.load(os.path.join(image_path, \u0026#39;weapon.png\u0026#39;)) weapon_size = weapon.get_rect().size weapon_width = weapon_size[0] # 무기는 한 번에 여러발 발 사 가능 weapons = [] # 무기 이동 속도 weapon_speed = 10 # 공 만들기 (4개 크기에 대해 따로 처리) ball_images = [ pygame.image.load(os.path.join(image_path, \u0026#39;balloon1.png\u0026#39;)), pygame.image.load(os.path.join(image_path, \u0026#39;balloon2.png\u0026#39;)), pygame.image.load(os.path.join(image_path, \u0026#39;balloon3.png\u0026#39;)), pygame.image.load(os.path.join(image_path, \u0026#39;balloon4.png\u0026#39;))] # 공 크기에 따른 최초 스피드 ball_speed_y = [-18, -15, -12, -9] # 공들  balls = [] # 최초 발생하는 큰 공 추가 balls.append({ \u0026#39;pos_x\u0026#39; : 50, # 공의 x좌표 \u0026#39;pos_y\u0026#39; : 50, # 공의 y좌표 \u0026#39;img_idx\u0026#39; : 0, # 공의 이미지 인덱스 \u0026#39;to_x\u0026#39; : 3, # x축 이동방향  \u0026#39;to_y\u0026#39; : -6, # y축 이동방향 \u0026#39;init_spd_y\u0026#39; : ball_speed_y[0]}) # y 최초 속도 # 사라질 무기, 공 정보 저장 변수 weapon_to_remove = -1 ball_to_remove = -1 # 폰트 정의 game_font = pygame.font.Font(None, 40) total_time = 100 start_ticks = pygame.time.get_ticks() # 시작 시간 정의 # 게임 종료 메세지 game_result = \u0026#39;Game Over\u0026#39; running = True while running: dt = clock.tick(30) # 2. 이벤트 처리 (키보드. 마우스 등) for event in pygame.event.get(): if event.type == pygame.QUIT: running = False if event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: character_to_x -= character_speed elif event.key == pygame.K_RIGHT: character_to_x += character_speed elif event.key == pygame.K_SPACE: # 무기 발사 weapon_x_pos = character_x_pos + (character_width / 2) - (weapon_width / 2) weapon_y_pos = character_y_pos weapons.append([weapon_x_pos, weapon_y_pos]) if event.type == pygame.KEYUP: if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT: character_to_x = 0 # 3. 게임 캐릭터 위치 정의 character_x_pos += character_to_x if character_x_pos \u0026lt; 0: character_x_pos = 0 elif character_x_pos \u0026gt; screen_width - character_width: character_x_pos = screen_width - character_width # 무기 위치 조정 weapons = [ [w[0], w[1] - weapon_speed] for w in weapons] # 무기 위치를 위로 # 천장에 닿은 무기 없애기 weapons = [ [w[0], w[1]] for w in weapons if w[1] \u0026gt; 0] # 공 위치 정의 for ball_idx, ball_val in enumerate(balls): ball_pos_x = ball_val[\u0026#39;pos_x\u0026#39;] ball_pos_y = ball_val[\u0026#39;pos_y\u0026#39;] ball_img_idx = ball_val[\u0026#39;img_idx\u0026#39;] ball_size = ball_images[ball_img_idx].get_rect().size ball_width = ball_size[0] ball_height = ball_size[1] # 가로벽에 닿았을 때 공 이동 위치 변경 (튕겨나오는 효과) if ball_pos_x \u0026lt; 0 or ball_pos_x \u0026gt; screen_width - ball_width: ball_val[\u0026#39;to_x\u0026#39;] = ball_val[\u0026#39;to_x\u0026#39;] * (-1) # 세로 위치 # 스테이지에 튕겨서 올라가는 처리 if ball_pos_y \u0026gt;= screen_height - stage_height - ball_height: ball_val[\u0026#39;to_y\u0026#39;] = ball_val[\u0026#39;init_spd_y\u0026#39;] else: # 그외의 모든 경우에는 속도를 증가 ball_val[\u0026#39;to_y\u0026#39;] += 0.5 ball_val[\u0026#39;pos_x\u0026#39;] += ball_val[\u0026#39;to_x\u0026#39;] ball_val[\u0026#39;pos_y\u0026#39;] += ball_val[\u0026#39;to_y\u0026#39;] # 4. 충돌 처리 # 캐릭터 rect 정보 업데이트 character_rect = character.get_rect() character_rect.left = character_x_pos character_rect.top = character_y_pos for ball_idx, ball_val in enumerate(balls): ball_pos_x = ball_val[\u0026#39;pos_x\u0026#39;] ball_pos_y = ball_val[\u0026#39;pos_y\u0026#39;] ball_img_idx = ball_val[\u0026#39;img_idx\u0026#39;] # 공 rect 정보 업데이트 ball_rect = ball_images[ball_img_idx].get_rect() ball_rect.left = ball_pos_x ball_rect.top = ball_pos_y # 공과 캐릭터 충돌 처리 if character_rect.colliderect(ball_rect): running = False break # 공과 무기들 충돌 처리 for weapon_idx, weapon_val in enumerate(weapons): weapon_pos_x = weapon_val[0] weapon_pos_y = weapon_val[1] # 무기 rect 정보 업데이트 weapon_rect = weapon.get_rect() weapon_rect.left = weapon_pos_x weapon_rect.top = weapon_pos_y # 충돌 체크 if weapon_rect.colliderect(ball_rect): weapons_to_remove = weapon_idx # 해당 무기 없애기 위한 값 설정 ball_to_remove = ball_idx if ball_img_idx \u0026lt; 3: # 현재 공 크기 정보를 가지고 옴 ball_width = ball_rect.size[0] ball_height = ball_rect.size[1] # 나눠진 공 정보 small_ball_rect = ball_images[ball_img_idx + 1].get_rect() small_ball_width = small_ball_rect.size[0] small_ball_height = small_ball_rect.size[1] # 왼쪽으로 튕겨나가는 작은 공 balls.append({ \u0026#39;pos_x\u0026#39; : ball_pos_x + (ball_width / 2) - (small_ball_width / 2), # 공의 x좌표 \u0026#39;pos_y\u0026#39; : ball_pos_y + (ball_height / 2) - (small_ball_height / 2), # 공의 y좌표 \u0026#39;img_idx\u0026#39; : ball_img_idx + 1, # 공의 이미지 인덱스 \u0026#39;to_x\u0026#39; : -3, # x축 이동방향  \u0026#39;to_y\u0026#39; : -6, # y축 이동방향 \u0026#39;init_spd_y\u0026#39; : ball_speed_y[ball_img_idx + 1]}) # y 최초 속도 # 오른쪽으로 튕겨나가는 작은 공 balls.append({ \u0026#39;pos_x\u0026#39; : ball_pos_x + (ball_width / 2) - (small_ball_width / 2), # 공의 x좌표 \u0026#39;pos_y\u0026#39; : ball_pos_y + (ball_height / 2) - (small_ball_height / 2), # 공의 y좌표 \u0026#39;img_idx\u0026#39; : ball_img_idx + 1, # 공의 이미지 인덱스 \u0026#39;to_x\u0026#39; : 3, # x축 이동방향  \u0026#39;to_y\u0026#39; : -6, # y축 이동방향 \u0026#39;init_spd_y\u0026#39; : ball_speed_y[ball_img_idx + 1]}) # y 최초 속도  break else: # 계속 게임을 진행 continue # 안쪽 for문 조건이 맞지 않으면 continue. 바깥 for문 계속 수행 break # 안쪽 for문에서 break를 만나면 여기로 진입 가능. 2중 for문을 한번에 통과하는 트릭! # for 바깥 조건: # 바깥 동작 # for 안쪽 조건: # 안쪽 동작 # if 충돌하면: # break # else: # continue # break # 충돌된 공 or 무기 없애기 if ball_to_remove \u0026gt; -1: del balls[ball_to_remove] ball_to_remove = -1 if weapon_to_remove \u0026gt; -1: del weapons[weapon_to_remove] weapon_to_remove = -1 # 모든 공을 없앤 경우 게임 종료(성공) if len(balls) == 0: game_result = \u0026#39;Mission Complete\u0026#39; running = False # 5. 화면에 그리기 screen.blit(background, (0, 0)) for weapon_x_pos, weapon_y_pos in weapons: screen.blit(weapon, (weapon_x_pos, weapon_y_pos)) for idx, val in enumerate(balls): ball_pos_x = val[\u0026#39;pos_x\u0026#39;] ball_pos_y = val[\u0026#39;pos_y\u0026#39;] ball_img_dx = val[\u0026#39;img_idx\u0026#39;] screen.blit(ball_images[ball_img_idx], (ball_pos_x, ball_pos_y)) screen.blit(stage, (0, screen_height - stage_height)) screen.blit(character, (character_x_pos, character_y_pos)) # 경과 시간 계산 elapsed_time = (pygame.time.get_ticks() - start_ticks)/ 1000 timer = game_font.render(\u0026#39;Time: {}\u0026#39;.format(int(total_time - elapsed_time)), True, (255, 255, 255)) screen.blit(timer, (10, 10)) # 시간 초과했다면 if total_time - elapsed_time \u0026lt;= 0: game_result = \u0026#39;Time Over\u0026#39; running = False pygame.display.update() # 게임화면을 다시 그리기 # 게임 오버 메세지 msg = game_font.render(game_result, True, (255, 0, 0)) msg_rect = msg.get_rect(center=(int(screen_width/2), int(screen_height/2))) screen.blit(msg, msg_rect) pygame.display.update() # 잠시 대기 pygame.time.delay(2000) # 2초 정도 대기(ms 단위) # pygame 종료 pygame.quit()   결과물 ","description":"pygame 응용","id":47,"section":"posts","tags":null,"title":"pygame[응용]","uri":"https://jiwooblog.netlify.app/posts/python/pygame_2/"},{"content":"Chapter 04. Monte Carlo Approximation 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\nMonte Carlo Method Monte Carlo Method는 이름은 거창해보이지만 사실 그 방법은 매우 간단하다.\n우선, 사후분포($p(\\theta|y_1,...,y_n)$)로부터 S개의 random sample을 뽑는다.\n$$\\theta^{1}, \u0026hellip;, \\theta^{S} \\ \\stackrel{iid}{\\sim} \\ p(\\theta|y_1, \u0026hellip;, y_n) $$\n그러면 S가 커질수록 {$\\theta^{1}, ..., \\theta^{S}$}는 근사적으로 사후분포($p(\\theta|y_1,...,y_n)$)를 따른다.\n이를 통해 $E[\\theta|y_1, ..., y_n]$, $Var[\\theta|y_1, ..., y_n]$부터 중앙값, $\\alpha$ percentile 등의 통계량값들을 근사적으로 계산할 수 있다.\n이때 approximate Monte Carlo Standard error은 $\\sqrt{\\hat{\\sigma}^2/S}$이다. 그렇기 때문에, 가령 $E[\\theta|y_1, ..., y_n]$와 Monte Carlo 추정치의 차이가 0.01이하로 하고 싶다고 한다면, Monte Carlo Sample Size를 조정해주면 된다. 이때 예를 들어서 $\\hat{\\sigma}^2$가 0.024라고 한다면, Sample Size는 $2\\sqrt{0.024/S} \u0026lt; 0.01$로 계산해서 sample을 960개보다는 많이 뽑아야 함을 알 수 있다.\nMonte Carlo Method를 활용하면 다양한 것들을 할 수 있는데, 그 대표적인 예시로 아래 세 개를 이해해보자.\n1. Posterior Inference for Arbitrary Functions $\\theta$ 그 자체가 아니라 임의의 $f(\\theta)$의 posterior distribution이 궁금할 수 있다. 예를 들어서, log odds와 같은 것 말이다. 하지만 결국 $\\gamma = f(\\theta)$도 $\\theta$처럼 Monte Carlo Method로 사후분포을 추정할 수 있다.\n하나의 parameter만 있는 것이 아니라, 심지어 $Pr(\\theta_1 \u0026gt; \\theta_2 | Y_{1,1} = y_{1,1}, ..., Y_{n_2,2}=y_{n_2,2})$나 $Pr(\\theta_1/\\theta_2 | Y_{1,1} = y_{1,1}, ..., Y_{n_2,2}=y_{n_2,2})$처럼 parameter가 두 개인 경우도 구할 수 있다.\n2. Sampling from Predictive Distributions Step1. sample $\\theta^{(1)},...,\\theta^{(S)} \\text{ ~ i.i.d} \\ p(\\theta|y_1,...,y_n)$\nStep2. approximate $p(\\tilde{y}|y_1,...,.y_n)$ with $\\sum_{s=1}^{S}p(\\tilde{y}|\\theta^{(s)})/S$\n위 방법을 통해 $Pr(\\tilde{Y_1}\u0026gt;\\tilde{Y_2}|\\sum Y_{i,1}=217,\\sum Y_{i,2}=66)$를 근사하는 것도 가능하다. 왜냐하면 $Pr(\\tilde{Y_1})$와 $Pr(\\tilde{Y_2})$은 posterior independent하기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12  set.seed(1) a\u0026lt;-2 ; b\u0026lt;-1 sy1\u0026lt;-217 ; n1\u0026lt;-111 sy2\u0026lt;-66 ; n2\u0026lt;-44 theta1.mc\u0026lt;-rgamma(10000,a+sy1, b+n1) theta2.mc\u0026lt;-rgamma(10000,a+sy2, b+n2) y1.mc\u0026lt;-rpois(10000,theta1.mc) y2.mc\u0026lt;-rpois(10000,theta2.mc) mean(theta1.mc\u0026gt;theta2.mc)   ## [1] 0.9708\r1  mean(y1.mc\u0026gt;y2.mc)   ## [1] 0.4846\r위 결과를 통해서 주의깊게 살펴보아야 할 것은, 예측치의 차이와 모수의 차이가 같지 않다는 점이다.\n아래 세 개 구분하기  (1) sampling model: $Pr(\\tilde{Y}=\\tilde{y}|\\theta)$ (2) prior predictive model: $Pr(\\tilde{Y}=\\tilde{y})$  $\\theta$에 대한 사전확률분포가 관측가능한 데이터 $\\tilde{Y}$에 대해 합리적인 믿음을 나타낼 수 있는지 확인해보는 용도로 활용가능하다.   (3) posterior predictive model: $Pr(\\tilde{Y}=\\tilde{y}|Y_1=y_1, ..., Y_n=y_n)$  3. Posterior Predictive Model Checking  We should at least make sure that our model generates predictive datasets $\\tilde{Y}$ that resemble the observed dataset in terms of features that are of interest\n   Code  1 2 3  load(\u0026#34;gss.RData\u0026#34;) table(gss$DEG[gss$YEAR==1998])   ## ## 0 1 2 3 4 ## 430 1500 209 478 205\r1 2 3 4 5 6  y1\u0026lt;-gss$PRAYER[gss$YEAR==1998 \u0026amp; gss$RELIG==1 ] y1\u0026lt;-1*(y1==1) y1\u0026lt;-y1[!is.na(y1) ] sy1\u0026lt;-sum(y1) n1\u0026lt;-length(y1) sy1/n1   ## [1] 0.3616803\r1 2 3 4 5 6  y2\u0026lt;-gss$PRAYER[gss$YEAR==1998 \u0026amp; gss$RELIG!=1 ] y2\u0026lt;-1*(y2==1) y2\u0026lt;-y2[!is.na(y2) ] sy2\u0026lt;-sum(y2) n2\u0026lt;-length(y2) sy2/n2   ## [1] 0.5471464\r1  table(gss$FEMALE[gss$YEAR==1998])   ## ## 0 1 ## 1232 1600\r1 2 3 4 5 6  y\u0026lt;-gss$FEMALE[gss$YEAR==1998] y\u0026lt;-1*(y==1) y\u0026lt;-y[!is.na(y) ] sy\u0026lt;-sum(y) n\u0026lt;-length(y) sy/n   ## [1] 0.5649718\r1 2 3  sy\u0026lt;-sy2 n\u0026lt;-n2 sy/n   ## [1] 0.5471464\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  #### MC approximations set.seed(1) a\u0026lt;-1 ; b\u0026lt;-1 theta.prior.sim\u0026lt;-rbeta(10000,a,b) gamma.prior.sim\u0026lt;- log( theta.prior.sim/(1-theta.prior.sim) ) n0\u0026lt;-860-441 ; n1\u0026lt;-441 theta.post.sim\u0026lt;-rbeta(10000,a+n1,b+n0) gamma.post.sim\u0026lt;- log( theta.post.sim/(1-theta.post.sim) ) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0)) par(mfrow=c(2,3)) par(cex=.8) par(mfrow=c(1,2),mar=c(3,3,1,1), mgp=c(1.75,.75,.0)) plot(density(gamma.prior.sim,adj=2),xlim=c(-5,5),main=\u0026#34;\u0026#34;, xlab=expression(gamma), ylab=expression(italic(p(gamma))),col=\u0026#34;gray\u0026#34;) plot(density(gamma.post.sim,adj=2),xlim=c(-5,5),main=\u0026#34;\u0026#34;,xlab=expression(gamma), ylab=expression(paste(italic(\u0026#34;p(\u0026#34;),gamma,\u0026#34;|\u0026#34;,y[1],\u0026#34;...\u0026#34;,y[n],\u0026#34;)\u0026#34;, sep=\u0026#34;\u0026#34;)) ) lines(density(gamma.prior.sim,adj=2),col=\u0026#34;gray\u0026#34;)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  set.seed(1) a\u0026lt;-2 ; b\u0026lt;-1 sy1\u0026lt;-217 ; n1\u0026lt;-111 sy2\u0026lt;-66 ; n2\u0026lt;-44 theta1.mc\u0026lt;-rgamma(10000,a+sy1, b+n1) theta2.mc\u0026lt;-rgamma(10000,a+sy2, b+n2) y1.mc\u0026lt;-rpois(10000,theta1.mc) y2.mc\u0026lt;-rpois(10000,theta2.mc) #### Posterior predictive check  y1\u0026lt;-gss$CHILDS[gss$FEMALE==1 \u0026amp; gss$YEAR\u0026gt;=1990 \u0026amp; gss$AGE==40 \u0026amp; gss$DEG\u0026lt;3 ] y1\u0026lt;-y1[!is.na(y1)] set.seed(1) a\u0026lt;-2 ; b\u0026lt;-1 t.mc\u0026lt;-NULL for(s in 1:10000) { theta1\u0026lt;-rgamma(1,a+sum(y1), b+length(y1)) y1.mc\u0026lt;-rpois(length(y1),theta1) t.mc\u0026lt;-c(t.mc,sum(y1.mc==2)/sum(y1.mc==1)) } t.obs\u0026lt;-sum(y1==2)/sum(y1==1) mean(t.mc\u0026gt;=t.obs)   ## [1] 0.0049\r   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  par(mar=c(3,3,1,1),mgp=c(1.75,.75,0)) par(mfrow=c(1,2)) ecdf\u0026lt;-(table(c(y1,0:9))-1 )/sum(table(y1)) #ecdf.mc\u0026lt;-(table(c(y1.mc,0:9))-1 )/sum(table(y1.mc)) ecdf.mc\u0026lt;- dnbinom(0:9,size=a+sum(y1),mu=(a+sum(y1))/(b+length(y1))) plot(0:9+.1,ecdf.mc,type=\u0026#34;h\u0026#34;,lwd=5,xlab=\u0026#34;number of children\u0026#34;, ylab=expression(paste(\u0026#34;Pr(\u0026#34;,italic(Y[i]==y[i]),\u0026#34;)\u0026#34;,sep=\u0026#34;\u0026#34;)),col=\u0026#34;gray\u0026#34;, ylim=c(0,.35)) points(0:9-.1, ecdf,lwd=5,col=\u0026#34;black\u0026#34;,type=\u0026#34;h\u0026#34;) legend(1.8,.35, legend=c(\u0026#34;empirical distribution\u0026#34;,\u0026#34;predictive distribution\u0026#34;), lwd=c(2,2),col= c(\u0026#34;black\u0026#34;,\u0026#34;gray\u0026#34;),bty=\u0026#34;n\u0026#34;,cex=.8) hist(t.mc,prob=T,main=\u0026#34;\u0026#34;,ylab=\u0026#34;\u0026#34;,xlab=expression(t(tilde(Y))) ) segments(t.obs,0,t.obs,.25,col=\u0026#34;black\u0026#34;,lwd=3)   실제(empirical) 분포와 예측 분포의 모습이 다소 차이가 남을 확인할 수 있다.\n참고 [1] FCB code\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":48,"section":"posts","tags":null,"title":"Monte Carlo Method","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/"},{"content":"Part 3. API는 무엇인가 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Spotify App 생성 및 토큰 발급 Client Credentials Flow 1 2 3 4 5  { \u0026#34;access_token\u0026#34;: \u0026#34;NgCXRKc...MzYjw\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, }    client id, client secret을 제공하면 우리는 3600초, 즉 1시간동안 사용할 수 있다.  2. Python 기본 1 2 3 4 5 6 7 8 9 10 11  import sys def main(): print(\u0026#39;fastcampus\u0026#39;) #python으로 실행했을 때, 해당 py파일 이름이 전달되면, main()을 실행하라 if __name__ == \u0026#39;__main__\u0026#39;: main() #직접 py파일이 실행 안되고, import spotify_api와 같이 모듈처럼 import되면, ~~를 print하라. else: print(\u0026#39;this script i being imported\u0026#39;)    Windows는 Windows Powershell을 통해서 진행하면 된다. 기본적으로 위처럼 코딩을 시작하게 된다.  3. Python Requests 패키지 requests python library \u0026gt; Developer Interface 참고하기\npowershell에서 pip install requests 실행하기\n4. API를 통한 데이터 요청 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  import sys import requests import base64 import json import logging client_id = \u0026#39;\u0026#39; # client_id 입력 client_secret = \u0026#39;\u0026#39; # client_secret 입력 def main(): headers = get_headers(client_id, client_secret) params = { \u0026#39;q\u0026#39;: \u0026#39;BTS\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: 5 } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) # print(r.status_code) # 200이면 이상 없는 것 # print(r.text) # sys.exit(0) def get_headers(client_id, client_secret): # 1시간만 있으면 expire되기 때문에 추가로 function 하나를 만들어두는 것이다. endpoint = \u0026#39;https://accounts.spotify.com/api/token\u0026#39; encoded = base64.b64encode(\u0026#34;{}:{}\u0026#34;.format(client_id, client_secret).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;ascii\u0026#39;) headers = { \u0026#39;Authorization\u0026#39;: \u0026#39;Basic {}\u0026#39;.format(encoded) } payload = { \u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39; } r = requests.post(endpoint, data=payload, headers=headers) # 중간에 잘 되는지 확인해보는 코드 # print(r.status_code) # print(r.text) # print(type(r.text)) #string으로 출력되므로 아래에서 json.loads를 통해 dictionary로 만들어줘야 한다. # sys.exit(0) access_token = json.loads(r.text)[\u0026#39;access_token\u0026#39;] headers = { \u0026#39;Authorization\u0026#39;: \u0026#34;Bearer {}\u0026#34;.format(access_token) } return headers if __name__ == \u0026#39;__main__\u0026#39;: main()   5. Status Code  Status Code를 알아야 하는 이유: 데이터 엔지니어의 잘못이 아닌, Spotify 서버의 오류 등으로 인한 문제인지 체크할 수 있다. Spotify Web API 기준이지만, RFC 2616와 RFC 6585에 의해 일반적으로 통용되는 기준이다.     STATUS CODE DESCRIPTION     200 OK - The request has succeeded. The client can read the result of the request in the body and the headers of the response.   201 Created - The request has been fulfilled and resulted in a new resource being created.   202 Accepted - The request has been accepted for processing, but the processing has not been completed.   204 No Content - The request has succeeded but returns no message body.   304 Not Modified. See Conditional requests.   400 Bad Request - The request could not be understood by the server due to malformed syntax. The message body will contain more information; see Response Schema.   401 Unauthorized - The request requires user authentication or, if the request included authorization credentials, authorization has been refused for those credentials.   403 Forbidden - The server understood the request, but is refusing to fulfill it.   404 Not Found - The requested resource could not be found. This error can be due to a temporary or permanent condition.   429 Too Many Requests - Rate limiting has been applied.   500 Internal Server Error. You should never receive this error because our clever coders catch them all … but if you are unlucky enough to get one, please report it to us through a comment at the bottom of this page.   502 Bad Gateway - The server was acting as a gateway or proxy and received an invalid response from the upstream server.   503 Service Unavailable - The server is currently unable to handle the request due to a temporary condition which will be alleviated after some delay. You can choose to resend the request again.    6. 에러 핸들링 sys.exit(0)과 sys.exit(1) 차이 1 2 3 4  # 프로그램을 정상적으로 종료시키고 싶을 때 sys.exit(0) # 프로그램을 강제적으로 종료시키고 싶을 때 sys.exit(1)   Status Code 401, 409 에러 핸들링 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) if r.status_code != 200: logging.error(r.text) ## Too many requests if r.status_code == 429: retry_after = json.loads(r.headers)[\u0026#39;Retry-After\u0026#39;] time.sleep(int(retry_after)) r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) ## access_token expireed elif r.status_code == 401: headers = get_headers(client_id, client_secret) r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) else: sys.exit(1) #강제종료   7. 페이지네이션 핸들링 참고사이트\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  import sys import requests import base64 import json import logging client_id = \u0026#39;\u0026#39; # client_id 입력 client_secret = \u0026#39;\u0026#39; # client_secret 입력 def main(): headers = get_headers(client_id, client_secret) # Get BTS\u0026#39; Albums r = requests.get(\u0026#39;https://api.spotify.com/v1/artists/3Nrfpe0tUJi4K4DXYWgMUX/albums\u0026#39;, headers=headers) raw = json.loads(r.text) # total = raw[\u0026#39;total\u0026#39;] # 총 104개가 있음을 확인 # offset = raw[\u0026#39;offset\u0026#39;] # 시작은 0 # limit = raw[\u0026#39;limit\u0026#39;] # 20개씩 뽑겠다. next = raw[\u0026#39;next\u0026#39;] albums = [] albums.extend(raw[\u0026#39;items\u0026#39;]) ## 난 200개만 뽑아 오겠다. count = 0 while count \u0026lt; 200 and next: # while next: 라고만 하면 끝까지 가져오게 된다. r = requests.get(raw[\u0026#39;next\u0026#39;], headers=headers) raw = json.loads(r.text) next = raw[\u0026#39;next\u0026#39;] # print(next) # 맨 마지막에는 none이 나오게 된다. 이에 대해서는 Spotify 페이지를 참고하면 된다. albums.extend(raw[\u0026#39;items\u0026#39;]) count = len(albums) print(len(albums)) def get_headers(client_id, client_secret): endpoint = \u0026#39;https://accounts.spotify.com/api/token\u0026#39; encoded = base64.b64encode(\u0026#34;{}:{}\u0026#34;.format(client_id, client_secret).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;ascii\u0026#39;) headers = {\u0026#39;Authorization\u0026#39;: \u0026#39;Basic {}\u0026#39;.format(encoded)} payload = {\u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39;} r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)[\u0026#39;access_token\u0026#39;] headers = {\u0026#39;Authorization\u0026#39;: \u0026#34;Bearer {}\u0026#34;.format(access_token)} return headers if __name__ == \u0026#39;__main__\u0026#39;: main()   \n","description":"API는 무엇인가","id":49,"section":"posts","tags":null,"title":"Spotify API","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part3_2/"},{"content":"Chapter 05. Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods와 Bayesian Data Analysis를 참고하였다.\nWarm up!  Gamma Distribution Inverse Gamma Distribution Scaled Inverse Chi-squared Distribution  1. Single Parameter Conjugacy 평균이나 분산 중 하나만을 모르는 경우\n1-1. 평균을 모르는 경우 Prior: $\\mu \\text{ ~ } N(\\mu_0, \\tau_0^{2})$\nLikelihood: $y|\\mu \\text{ ~ } N(\\mu, \\sigma^2)$\nPosterior: $\\mu|y \\text{ ~ } N(\\mu_n, \\tau_n^{2})$\nwhere $\\frac{1}{\\tau_n^{2}} = \\frac{1}{\\tau_0^{2}} + \\frac{n}{\\sigma^2}$ and $\\mu_n = \\frac{\\frac{1}{\\tau_0^{2}}}{\\frac{1}{\\tau_0^{2}} + \\frac{n}{\\sigma^2}}\\mu_0 + \\frac{\\frac{n}{\\sigma^2}}{\\frac{1}{\\tau_0^{2}} + \\frac{n}{\\sigma^2}}\\bar{y} $\nPosterior Predictive: $\\tilde{y}|y \\text{ ~ } N(\\mu_n, \\sigma^2+\\tau_n^{2})$\n1-2. 분산을 모르는 경우 Prior: $\\sigma^2 \\text{ ~ } \\chi^{-2}(\\nu_0, \\sigma_0^2)$\nLikelihood: $y|\\sigma^2 \\text{ ~ } N(\\mu, \\sigma^2)$ Posterior: $\\sigma^2|y \\text{ ~ } \\chi^{-2}(\\nu_n, \\sigma_n^2)$\nwhere $\\nu_n = \\nu_0 + n$ and $\\sigma_n^2 = \\frac{\\nu_0\\sigma_0^2 + ns(y)}{\\nu_0 + n}$\nc.f. $s(y) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\mu)^2$, 이는 MLE이다(biased estimator). 참고로, 베이지안은 frequentist들의 기준인 unbiasedness를 중요하게 생각하지 않는다.\n2. Two Parameter marginal distribution 얻는 두 가지 방법  Integreation: joint posterior distribution을 구한 후, 관심 없는 모수(nuisance parameter)에 대해 적분 Simulation: joint posterior distribution에서 sample을 구한 후, 관심 있는 모수의 분포만 고려(나머지는 무시)  그렇다면 joint posterior distribution은 어떻게 구할까?  marginal and conditional simulation을 통해서 구할 수 있다.\n$\\theta_2 \\text{ ~ } \\theta_2|y$ and $\\theta_1 | \\theta_2, y$\n$\\rightarrow (\\theta_1, \\theta_2) \\text{ ~ } (\\theta_1, \\theta_2|y)$  2-1. noninformative prior Prior: $p(\\mu, \\sigma^2) = p(\\mu)p(\\sigma^2) \\propto (\\sigma^2)^{-1} $ (독립 가정, improper prior)\nLikelihood: $p(y|\\mu, \\sigma^2) \\propto \\sigma^{-n}exp(\\frac{-1}{2}\\sigma^2\\sum_{i=1}^{n}(y_i - \\mu)^2) $\nPosterior: $\\mu, \\sigma^2 |y \\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n}) \\times \\chi^{-2}(n-1, s^2)$\nPosterior Predictive: $\\tilde{y}|y \\text{ ~ } t_{n-1}(\\bar{y}, (1+\\frac{1}{n}s^2))$\n이는 posterior과 비교해서, data의 uncertainty($s^2$)이 추가된 형태라고 해석할 수 있다.\nPosterior Distribution 구하기 (Noninformative) 해당 Posterior Distribution을 구하는 과정은 다소 복잡하기 때문에 자세하게 서술해보도록 하겠다.\n우선 시작하기에 앞서, 한마디로 이 과정을 요악한다면 Conditional Posterior X Marginal일 것이다.\nSTEP1. $p(\\mu|\\sigma^2,y)$ $p(\\sigma^2|y)$의 형태를 파악한다.\n  $\\mu|\\sigma^2,y \\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n})$\n이부분은 위의 평균을 모르지만, 분산을 아는 경우에서 prior precision $\\frac{1}{\\tau^2}=0$으로 주면 위와 같이 나온다. prior precision을 0으로 주는 이유는, non-informative prior를 가정하고 있기 때문이다.\n  $\\sigma^2|y \\text{ ~ } \\chi^{-2}(n-1, s^2)$\n이는 아래의 수식을 계산해서 얻을 수 있다.\n  \\begin{align}\rp(\\mu, \\sigma^2|y) \u0026amp;\\propto p(\\mu, \\sigma^2) \\times p(y|\\mu, \\sigma^2) \\\\\r\u0026amp;\\propto \\sigma^{-n-2}exp\\bigg(\\frac{-1}{2\\sigma^2}\\big[(n-1)s^2 + n(\\bar{y}-\\mu)^2\\big]\\bigg) \\\\\r\\rightarrow p(\\sigma^2|y) \u0026amp;= \\int p(\\mu,\\sigma^2|y)d\\mu \\end{align}\nSTEP2. 베이즈룰을 이용하여 posterior distribution을 계산해준다.\n위의 과정을 거친다면, 그 결과는 아래와 같이 정리할 수 있다.\n\\begin{align}\r\\mu|\\sigma^2,y \u0026amp;\\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n}) \\\\\r\\sigma^2|y \u0026amp;\\text{ ~ } \\chi^{-2}(n-1, s^2) \\\\\r\\mu, \\sigma^2 |y \u0026amp;\\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n}) \\times \\chi^{-2}(n-1, s^2)\r\\end{align}\nPosterior Mean의 Marginal Distribution 구하기 번외로, $\\mu$의 marginal posterior distribution $p(\\mu|y)$은 $\\int p(\\mu,\\sigma^2)d\\sigma^2$를 통해서 구할 수 있다. 그리고 형태는 아래와 같다.\n$$p(\\mu|y) \\text{ ~ } t_{n-1}(\\bar{y}, \\frac{s^2}{n})$$\nPosterior Prediction 구하는 과정 \\begin{align}\rp(\\tilde{y}|y) \u0026amp;= \\int\\int p(\\tilde{y}|\\mu,\\sigma^2) p(\\mu, \\sigma^2|y)\\ d\\mu \\ d\\sigma^2 \\\\\r\u0026amp;= \\int\\int p(\\tilde{y}|\\mu,\\sigma^2) \\ p(\\mu|\\sigma^2,y)\\ d\\mu \\cdot p(\\sigma^2|y) \\ d\\sigma^2 \\\\ \u0026amp;= \\int p(\\tilde{y}|\\sigma^2) \\ p(\\sigma^2|y) \\ d\\sigma^2\r\\end{align}\nPosterior Predictive: $\\tilde{y}|y \\text{ ~ } t_{n-1}(\\bar{y}, (1+\\frac{1}{n}s^2))$\n이 결과를 바로 위의 Posterior Mean의 marginal 분포와 비교해보는 것이 중요하다.\n왜냐하면 prediction을 할 때에 $s^2$, 즉 uncertainty가 추가된다고 해석할 수 있기 때문이다.\nTwo parameter Normal model이 중요한 이유는 다음 3. Frequentist와 Bayesian의 차이을 보면 명확하다. Frequentist와 Bayesian의 기본적인 전제와 입장 차이를 이해한다면, 정보가 없는 prior가 결국 어떠한 결론으로 이어가는지 이해할 수 있다.\n2-2. conjugate prior Prior: $p(\\mu, \\sigma^2) = p(\\mu|\\sigma^2) \\times p(\\sigma_0^2) \\text{ ~ N-Inv-} \\chi^2(\\mu_0, \\frac{\\sigma^2}{k_0}; v_0, \\sigma_0^2)$\n\\begin{align}\r\\mu|\\sigma^2 \u0026amp;\\text{ ~ } N(\\mu_0, \\frac{\\sigma^2}{k_0}) \\\\\r\\sigma^2 \u0026amp;\\text{ ~ } \\chi^{-2}(v_0, \\sigma^2_0) \\\\\r\\rightarrow \\mu, \\sigma^2 \u0026amp;\\propto \\sigma^{-1}(\\sigma^2)^{-(\\frac{v_0}{2}+1)}exp\\bigg(\\frac{-1}{2\\sigma^2}\\big[v_0\\sigma_0^2 + k_0(\\mu_0 - \\mu)^2\\big]\\bigg)\r\\end{align}\nLikelihood: $p(y|\\mu, \\sigma^2) \\propto \\sigma^{-n}exp\\bigg(\\frac{-1}{2\\sigma^2}\\sum_{i=1}{n}(y_i-\\mu)^2\\bigg)$\nPosterior: $p(\\mu, \\sigma^2|y) \\text{ ~ N-Inv-}\\chi^2(\\mu_n, \\frac{\\sigma_n^2}{k_n}; v_n, \\sigma_n^2) $\n\\begin{align}\r\\mu_n \u0026amp;= \\frac{k_0}{k_0+n}\\mu_0 + \\frac{n}{k_0+n}\\bar{y} \\\\\rk_n \u0026amp;= k_0 +n \\\\\rv_n \u0026amp;= v_o + n \\\\\rv_n\\sigma_n^2 \u0026amp;= v_0\\sigma_0^2 + (n-1)s^2 + \\frac{k_0n}{k_0+n}(\\bar{y}-\\mu_0)^2 \\\\\r\\rightarrow \\text{posterior ss} \u0026amp;= \\text{prior ss} + \\text{sample ss} + \\text{additional uncertainty}(\\bar{y}-\\mu_0)\r\\end{align}\n3. Frequentist와 Bayesian의 차이 Frequentist: parameter를 알 때, 통계량의 분포에 대해 이야기한다.\nlet $y \\text{ ~ } N(\\mu, \\sigma^2)$\n $\\bar{y} \\text{ ~ } N(\\mu, \\frac{\\sigma^2}{n}) $ $\\frac{(n-1)s^2}{\\sigma^2} \\text{ ~ } \\chi^2(n-1)$ $\\frac{\\bar{y}-\\mu}{s/\\sqrt{n}}|\\mu,\\sigma^2 \\text{ ~ } t_{n-1}$  Bayesian: data를 알 때, parameter의 분포에 대해 이야기한다.\n $\\mu \\text{ ~ } N(\\bar{y}, \\frac{\\sigma^2}{n})$ $\\sigma^2 \\text{ ~ } \\chi^{-2}(n-1, s^2)$ $\\frac{\\mu-\\bar{y}}{s/\\sqrt{n}}|y \\text{ ~ } t_{n-1} $  만약 Bayesian이 noninformative prior를 가정한다면, 즉 prior가 거의 없다고 생각한다면 frequetist랑 결과가 비슷하게 나오는 것은 당연하다.\n4. Multinomial Model Likelihood: $y|\\theta \\text{ ~ Multinomial}(\\theta) \\propto \\prod_{j=1}^{k}\\theta_j^{y_j}$\nPrior: $\\theta \\text{ ~ } Dir(\\alpha) \\propto \\prod_{j=1}^{k}\\theta_j^{\\alpha_j-1}$\nPosterior: $\\theta|y \\text{ ~ } Dir(\\alpha +y) \\propto \\prod_{j=1}^{k}\\theta_j^{\\alpha_j-y_j-1}$\n참고로 Multinomial distribution은 이항분포의 확장이며, Dirichlet distribution은 베타분포의 확장이라고 생각하면 쉽다. 왜냐하면 Beta-Binomial 모델에 대해서는 Chapter3에서 이미 충분히 다루었기 때문이다.\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"Normal Model","id":50,"section":"posts","tags":null,"title":"Normal Model","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n0. Data Type  numeric data/time character/string unicode character/string binary miscellaneous  1. Relational Database(RDB)  모든 데이터를 2차원의 테이블로 표현 하나 이상의 테이블로 구성 Entity-Relationship 모델 Normalization (Reduce Redundacy)  2. AWS 클라우드 MySQL 데이터베이스 생성  aws.amazon.com \u0026gt; RDS \u0026gt; 데이터 생성 Templates \u0026gt; Free Tier로 설정 (과금 예방) Public Access 허용하기 VPC에서 인바운드 규칙에 MySql 추가하기  3. 터미널에서 데이터베이스 연결하기 (Windows 기준)\n mysql client workbench 다운로드 MySQL Workbench랑 AWS를 연결하고, 그것을 termianl(powershell)로 연결하는 법 termianl에서 아래의 커맨드를 작성하고, 이어서 비밀번호를 입력해주면 된다.  mysql -h {hostname} -P 3306 -D {Default Schema} -u {username} -p 4. MySQL 데이터베이스 안에서 테이블 생성 1 2  CREATE TABLE people (first_name VARCHAR(20), last_name VARCHAR(20), age INT); SHOW TABLES;   5. 엔터티 관계도(ERD)  Entity Relationship Diagram 데이터 모델링 설계 과정에서 사용하는 모델 약속된 기호를 이용하여 데이터베이스의 구조를 쉽게 이해하기 위함이다.  ERD의 기본요소  Entities: 개체 Attributes: 엔터티의 속성 Relationship: 엔터티 간의 관계  6. Primary Key \u0026amp; Unique Key Primary Key  테이블에 하나 밖에 없는 유니크한 구별 값 Null 값 안 됨  Foreign Key  한 개 이상 가능 NULL 값도 가능  Unique Key  Primary Key처럼 유니크하긴 하다. 하지만, Null 값은 하나는 가질 수 있다. 그리고 하나 이상의 유니크 키를 가질 수 있다. Primary Key보다는 index로서의 성능은 낮다. ex) Primary Key: 수험번호, Unique Key: 주민등록번호  ","description":"데이터의 이해와 데이터베이스","id":51,"section":"posts","tags":null,"title":"RDBMS","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_1/"},{"content":"Chapter 06. Posterior Approximation with the Gibbs sampler 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n1. A Semi-conjugate prior distribution 2. Discrete approximations 3. Sampling from the conditional distributions 4. Gibbs Sampling 5. General properties of the Gibbs sampler 6. Introduction to MCMC diagnostics Conclusion semi-conjugate 분포를 모두 알면 full conditional probability를 아는 것과 거의 같다. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"Gibbs Sampling","id":52,"section":"posts","tags":null,"title":"Gibbs Sampling","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb06/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Spotify 데이터 이해 Spotify Web API \u0026gt; get an artist\nartist object 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  { \u0026#34;external_urls\u0026#34; : { \u0026#34;spotify\u0026#34; : \u0026#34;https://open.spotify.com/artist/0OdUWJ0sBjDrqHygGUXeCF\u0026#34; }, \u0026#34;followers\u0026#34; : { \u0026#34;href\u0026#34; : null, \u0026#34;total\u0026#34; : 306565 }, \u0026#34;genres\u0026#34; : [ \u0026#34;indie folk\u0026#34;, \u0026#34;indie pop\u0026#34; ], \u0026#34;href\u0026#34; : \u0026#34;https://api.spotify.com/v1/artists/0OdUWJ0sBjDrqHygGUXeCF\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;0OdUWJ0sBjDrqHygGUXeCF\u0026#34;, \u0026#34;images\u0026#34; : [ { \u0026#34;height\u0026#34; : 816, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/eb266625dab075341e8c4378a177a27370f91903\u0026#34;, \u0026#34;width\u0026#34; : 1000 }, { \u0026#34;height\u0026#34; : 522, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/2f91c3cace3c5a6a48f3d0e2fd21364d4911b332\u0026#34;, \u0026#34;width\u0026#34; : 640 }, { \u0026#34;height\u0026#34; : 163, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/2efc93d7ee88435116093274980f04ebceb7b527\u0026#34;, \u0026#34;width\u0026#34; : 200 }, { \u0026#34;height\u0026#34; : 52, \u0026#34;url\u0026#34; : \u0026#34;https://i.scdn.co/image/4f25297750dfa4051195c36809a9049f6b841a23\u0026#34;, \u0026#34;width\u0026#34; : 64 } ], \u0026#34;name\u0026#34; : \u0026#34;Band of Horses\u0026#34;, \u0026#34;popularity\u0026#34; : 59, \u0026#34;type\u0026#34; : \u0026#34;artist\u0026#34;, \u0026#34;uri\u0026#34; : \u0026#34;spotify🧑‍🎨0OdUWJ0sBjDrqHygGUXeCF\u0026#34; }   2. Spotify 데이터 모델 예시 ","description":"데이터의 이해와 데이터베이스","id":53,"section":"posts","tags":null,"title":"Spotify Data","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_2/"},{"content":"Chapter 07. The Multivariate Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n1. Multivariate Normal Desity univariate model에 대해서 앞선 챕터에서 이야기를 많이 했지만, 사실 현실세계에서는 multivariate인 경우가 훨씬 많다.\n1-1. Bivariate Normal   Bivariate Case  1 2 3 4 5  library(tidyverse) library(gridExtra) library(MASS) library(reshape2) library(ash)   1 2 3 4 5 6 7 8 9 10 11 12 13  #### 4-1. Draw yourself Figure 7.1 # 초기 설정 inv \u0026lt;- solve MU = matrix(c(50,50), ncol=1) SIGMA = matrix(c(64,0,0,144), ncol=2) # MVN pdf calc.dmvn = Vectorize(function(a,b, mu=MU, sigma=SIGMA){ y \u0026lt;- c(a,b) log.p \u0026lt;- (-nrow(mu)/2)*log(2*pi) - 0.5*log(det(sigma)) - 0.5*(t(y-mu) %*% inv(sigma) %*% (y-mu)) exp(log.p) })   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  # do it at once allInOne \u0026lt;- function(corr){ SIGMA = matrix(c(64,0,0,144), ncol=2) s1 \u0026lt;- sqrt(SIGMA[1,1]); s2 \u0026lt;- sqrt(SIGMA[2,2]) SIGMA[1,2] \u0026lt;- s1*s2*corr; SIGMA[2,1] \u0026lt;- s1*s2*corr # MVN density function calc.dmvn = Vectorize(function(a,b, mu=MU, sigma=SIGMA){ y \u0026lt;- c(a,b) log.p \u0026lt;- (-nrow(mu)/2)*log(2*pi) - 0.5*log(det(sigma)) - 0.5*(t(y-mu) %*% inv(sigma) %*% (y-mu)) exp(log.p) }) # sample sample = mvrnorm(n=30, mu=MU, Sigma=SIGMA) sample = data.frame(sample) colnames(sample) = c(\u0026#39;y1\u0026#39;,\u0026#39;y2\u0026#39;) # calculate density xLim = seq(20, 80, length=101) yLim = seq(20, 80, length=101) density.mvn \u0026lt;- outer(xLim, yLim, FUN=calc.dmvn) rownames(density.mvn) \u0026lt;- xLim colnames(density.mvn) \u0026lt;- yLim density.mvn \u0026lt;- melt(density.mvn) # graph density.mvn %\u0026gt;% ggplot(aes(x=Var1, y=Var2)) + geom_tile(aes(fill=value, alpha=value)) + geom_contour(aes(z=value), color=\u0026#39;white\u0026#39;, size=0.1) + geom_point(data=sample, mapping=aes(x=y1, y=y2, color=\u0026#39;red\u0026#39;), show.legend=FALSE) + scale_fill_gradient(low=\u0026#39;grey\u0026#39;, high=\u0026#39;steelblue\u0026#39;, guide=FALSE) + scale_alpha(guide=FALSE) + theme(legend.position=\u0026#39;None\u0026#39;) + theme_bw() + ggtitle(paste0(\u0026#39;corr=\u0026#39;,corr)) + xlab(\u0026#39;y1\u0026#39;) + ylab(\u0026#39;y2\u0026#39;) }   1 2 3 4  p1 \u0026lt;- allInOne(corr=-0.5) p2 \u0026lt;- allInOne(corr=0) p3 \u0026lt;- allInOne(corr=0.5) grid.arrange(p1,p2,p3, nrow=1)     1-2. Multivariate Normal Model $$p(\\boldsymbol{y}|\\boldsymbol{\\mu}, \\Sigma) = (2\\pi)^{-p/2}|\\Sigma|^{-1/2}exp\\Big(-\\frac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu}) \\Big) $$\nwhere\n$$\\boldsymbol{y} = \\begin{pmatrix}\ry_1 \\\\\ry_2 \\\\\r\\vdots \\\\\ry_p\r\\end{pmatrix}$$\n$$\\boldsymbol{\\mu} = \\begin{pmatrix}\r\\mu_1 \\\\\r\\mu_2 \\\\\r\\vdots \\\\\r\\mu_p\r\\end{pmatrix}$$\n$$\\Sigma = \\begin{pmatrix}\r\\sigma^2_{1} \u0026amp; \\sigma_{1,2} \u0026amp; \\cdots \u0026amp; \\sigma_{1,p} \\\\\r\\sigma_{2,1} \u0026amp; \\sigma^2_{2} \u0026amp; \\cdots \u0026amp; \\sigma_{2,p} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\\r\\sigma_{p,1} \u0026amp; \\cdots \u0026amp; \\cdots \u0026amp; \\sigma^2_{p}\r\\end{pmatrix}$$\n2. Semiconjugate prior distribution for the mean (known covariance matrix) Semiconjugate라고 하는 것은, 두 모수 중 하나가 주어졌을 경우에 conjugate한 경우를 뜻한다.\n여기서는 공분산 행렬이 주어졌을 때, 평균 벡터의 semiiconjugate prior를 구하는 것(조금 더 쉬움)을 먼저 보고 이어서 공분산 행렬의 semiconjugate prior를 구하는 것을 살펴볼 것이다.\nPrior: $\\boldsymbol{\\mu} \\text{ ~ } MVN(\\boldsymbol{\\mu_0}, \\Lambda_0)$\n\\begin{align}\rp(\\boldsymbol{\\mu}) \u0026amp;= (2\\pi)^{-p/2}|\\Lambda_0|^{-1/2}exp\\Big(-\\frac{1}{2}(\\boldsymbol{\\mu}-\\boldsymbol{\\mu_0})^T\\Lambda_0^{-1}(\\boldsymbol{\\mu}-\\boldsymbol{\\mu_0})\\Big) \\\\\r\u0026amp;\\propto exp(-\\frac{1}{2}\\boldsymbol{\\mu}^T\\Lambda^{-1}\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\Lambda^{-1}_0\\boldsymbol{\\mu_0}) \\\\\r\u0026amp;= exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_0\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_0})\r\\end{align}\nwhere $A_0 = \\Lambda^{-1}_0, \\boldsymbol{b_0} = \\Lambda^{-1}_0\\boldsymbol{\\mu_0}$\nLikelihood: $Y_1, ..., Y_n|\\boldsymbol{\\mu},\\Sigma \\text{ ~ iid } MVN(\\boldsymbol{\\mu}, \\Sigma)$\n\\begin{align}\rp(\\boldsymbol{y_1}, ...,\\boldsymbol{y_n}|\\boldsymbol{\\mu},\\Sigma) \u0026amp;= \\prod^{n}_{i=1} (2\\pi)^{-p/2}|\\Sigma|^{-1/2}exp\\Big(-\\frac{1}{2}(\\boldsymbol{y_i}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\boldsymbol{y_i}-\\boldsymbol{\\mu}) \\Big) \\\\\r\u0026amp;= (2\\pi)^{-np/2}|\\Sigma|^{-n/2}exp\\Big(-\\frac{1}{2}\\sum_{i=1}^{n}(\\boldsymbol{y_i}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\boldsymbol{y_i}-\\boldsymbol{\\mu}) \\Big) \\\\\r\u0026amp;\\propto exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_1\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_1})\r\\end{align}\nwhere $A_1 = n\\Sigma^{-1}, \\boldsymbol{b_1} = n\\Sigma^{-1}\\boldsymbol{\\bar{y}}$\nPosterior: $\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma \\text{ ~ } MVN(\\boldsymbol{\\mu_n}, \\Lambda_n)$\n$$\n\\begin{align}\rp(\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma) \u0026amp;\\propto exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_0\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_0}) \\times exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_1\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_1}) \\\\\r\u0026amp;= exp(-\\frac{1}{2}\\boldsymbol{\\mu}^TA_n\\boldsymbol{\\mu} \\ + \\ \\boldsymbol{\\mu}^T\\boldsymbol{b_n}) \\\\\r\\\\\r\\text{where } A_n \u0026amp;= A_0 + A_1 = \\Lambda_0^{-1}+n\\Sigma^{-1} \\\\\r\\boldsymbol{b_n} \u0026amp;= \\boldsymbol{b_0} + \\boldsymbol{b_1} = \\Lambda_0^{-1}\\boldsymbol{\\mu_0}+n\\Sigma^{-1}\\boldsymbol{\\bar{y}} \\\\\r\\\\\r\\rightarrow \\Lambda_n^{-1} \u0026amp;= \\Lambda_0^{-1}+n\\Sigma^{-1} \\\\\r\\boldsymbol{\\mu_n} \u0026amp;= (\\Lambda_0^{-1}+n\\Sigma^{-1})^{-1}(\\Lambda_0^{-1}\\boldsymbol{\\mu_0}+n\\Sigma^{-1}\\boldsymbol{\\bar{y}})\r\\end{align}\n$$\n3. Semiconjugate prior distribution for the covariance matrix (known mean) 이는 상대적으로 복잡한데, 그 이유는 이전과 달리 matrix 형태에다가 prior을 주어야하기 때문이다.\n그래서 구체적인 prior와 likelihood를 이야기하기 앞서서 필요한 두 가지 개념에 대해서 짚고 넘어가도록 하자.\n첫 번째는 inverse-Wishart 분포이며, 다음은 Positive Definite이라는 선형대수 개념이다.\n3-1. inverse-Wishart Distribution inverse-Wishart Distribution은 공분산 행렬의 semiconjugate prior을 주기 위해서 알아야 하는 함수이다. 낯선 확률분포처럼 보이기도 하지만, 자세히 살펴보면 이는 inverse-Gamma distribution의 다차원 확장 버전에 불과하긴 하다.\n3-2. Positive Definite Covariance Matrix는 Positive Definite Matrix이어야 한다. Positive Definite의 정의는 아래와 같다.\n$$\\boldsymbol{x'}\\Sigma\\boldsymbol{x} \u0026gt; 0 \\ \\text{ for all vectors} \\ \\boldsymbol{x}$$\nunivariate case에서 분산이 언제나 0 이상이어야 하는 것처럼, 이와 같은 맥락의 조건을 다차원에서 만족하려면 PD(Positive Definite)이어야 한다. 만약 공분산 행렬이 Positive Definite하다면, 이는 모든 분산이 0보다 크며 공분산이 -1과 1 사이에 있도록 한다.\n또한, PD는 대칭행렬(symmetric)에서 정의되는 개념이기 때문에, $\\sigma_{i,j} = \\sigma_{j,i}$라는 조건도 자연스럽게 성립한다.\n3-2-1. Positive Definite이 되기 위한 조건은? 다시 한번 말하자면, Positive Definite은 대칭행렬의 특수한 형태이며, 모든 eigenvalue들이 0보다 크다는 말과 같다.\n여기서 eigenvalue가 0보다 크다는 것은 정확히 무슨 의미일까?\n정방행렬을 Spectral Decomposition을 했을 때, $A = VDV^{-1}$\n$$A_{p\\text{ x }p} = \\begin{bmatrix}\r| \u0026amp; \u0026amp; |\\\\\ra_1 \u0026amp; \\cdots \u0026amp; a_p\\\\\r| \u0026amp; \u0026amp; |\\\\\r\\end{bmatrix} = \\begin{bmatrix}\r| \u0026amp; \u0026amp; |\\\\\rv_1 \u0026amp; \\cdots \u0026amp; v_p\\\\\r| \u0026amp; \u0026amp; | \\\\\r\\end{bmatrix} \\begin{bmatrix}\r\\lambda_1 \u0026amp; \u0026amp; \\\\\r\u0026amp; \\ddots \u0026amp; \\\\\r\u0026amp; \u0026amp; \\lambda_p\r\\end{bmatrix} \\begin{bmatrix}\r| \u0026amp; \u0026amp; |\\\\\rv_1 \u0026amp; \\cdots \u0026amp; v_p\\\\\r| \u0026amp; \u0026amp; |\\\\\r\\end{bmatrix}^{-1}$$\neigenvalue가 0보다 크다는 것은, 선형변환을 했을 때 그 기저의 방향이 반대로 바뀌지는 않는다는 것을 의미한다.\n3-3. random Covariance Matrix 만들기 이는 covariance matrix에 대해 uninformative prior를 주기 위함이다.\n$$\\frac{1}{n}\\sum_{i=1}^n\\boldsymbol{z_iz_i^T} = \\frac{1}{n}Z^TZ$$\n$$\\boldsymbol{z_iz_i^T} = \\begin{pmatrix}\rz_{i,1}^2 \u0026amp; z_{i,1}z_{i,2} \u0026amp; \\cdots \u0026amp; z_{i,1}z_{i,p} \\\\\rz_{i,2}z_{i,1} \u0026amp; z_{i,2}^2 \u0026amp; \\cdots \u0026amp; z_{i,2}z_{i,p} \\\\\r\\vdots \u0026amp; \u0026amp; \u0026amp; \\vdots \\\\\rz_{i,p}z_{i,1} \u0026amp; z_{i,p}z_{i,2} \u0026amp; \\cdots \u0026amp; z_{i,p}^2\r\\end{pmatrix}$$\n\\begin{align}\r\\frac{1}{n}\\big[Z^TZ\\big]_{j,j} \u0026amp;= \\frac{1}{n}\\sum_{i=1}^{n}z_{i,j}^2 = s_{j,j} = s_j^2\\\\\r\\frac{1}{n}\\big[Z^TZ\\big]_{j,k} \u0026amp;= \\frac{1}{n}\\sum_{i=1}^{n}z_{i,j}z_{i,k} = s_{j,k}\r\\end{align}\n여기서 n \u0026gt; p 이고, 모든 $\\boldsymbol{z_i}$들이 서로 선형독립이라면, $Z^TZ$는 항상 positive definite일 것이다.\n$$\\text{Proof) } \\boldsymbol{x}^{T}Z^{T}Z\\boldsymbol{x} = (Z\\boldsymbol{x})^{T}(Z\\boldsymbol{x}) = ||Z\\boldsymbol{x}||^2 \\ge 0$$\nSTEP1. Set $\\nu_0$(prior sample size), $\\Phi_0$(prior covariance matrix)\nSTEP2. Sample $\\boldsymbol{z_i} \\ \\stackrel{iid}{\\sim} \\ MVN(\\boldsymbol{0}, \\Phi_0)$\nSTEP3. Calculate $Z_TZ = \\sum_{i=1}^{\\nu_0}\\boldsymbol{z_iz_i^T}$\nSTEP4. repeat the procedure S times generating $Z_i^TZ_i$\n${Z_1^TZ_1, Z_2^TZ_2, ..., Z_S^TZ_S} \\text{ ~ } Wis(\\nu_0, \\Phi_0)$\nWishart분포와 inv-Wishart분포 특징 $$\\Sigma^{-1} \\sim Wis(\\nu_0, S_0^{-1}) \\rightarrow E[\\Sigma^{-1}]=\\nu_0S_0^{-1} \\\\\r\\Sigma \\sim Wis^{-1}(\\nu_0, S_0^{-1}) \\rightarrow E[\\Sigma]=\\frac{1}{\\nu_0-p-1}S_0$$\ncovariance matrix semiconjugate prior 모수 설정 방법 1-1. If belief that $\\Sigma = \\Sigma_0$ is strong, $\\nu_0$ \\uparrow 1-2. If belief that $\\Sigma = \\Sigma_0$ is weak, $\\nu_0 = p+2$\n2. Set $S_0=(\\nu_0-p-1)\\Sigma_0$\n3-4. Full conditional distribution of Covariance Matrix Prior: $\\Sigma \\sim \\text{inv-}Wis(\\nu_0, S_0^{-1})$\n$$p(\\Sigma) = \\bigg[2^{\\nu_0p/2}\\pi^{p/2}|S_0|^{\\nu_0/2}\\prod_{j=1}^{p}\\Gamma(\\frac{\\nu_0+1-j}{2})\\bigg]^{-1} \\times |\\Sigma|^{-(\\nu_0+p+1)/2} \\times exp\\Big(-\\frac{1}{2}tr(S_0\\Sigma^{-1})\\Big) $$\nLikelihood: $\\boldsymbol{Y}|\\boldsymbol{\\mu} \\stackrel{iid}\\sim MVN(\\boldsymbol{\\mu}, \\Sigma)$\n\\begin{align}\rp(\\boldsymbol{y_1, ..., y_n}|\\boldsymbol{\\mu}, \\Sigma) \u0026amp;= (2\\pi)^{-np/2}|\\Sigma|^{-n/2} exp\\bigg(-\\frac{1}{2}\\sum_{i=1}^{n} \\boldsymbol{(y_i-\\mu)}^T\\Sigma^{-1}\\boldsymbol{(y_i-\\mu)} \\bigg) \\\\\r\u0026amp;\\propto |\\Sigma|^{-n/2}exp\\bigg(-\\frac{1}{2}tr(S_\\mu\\Sigma^{-1})\\bigg)\r\\end{align}\nwhere $S_\\mu = \\sum_{i=1}^{n}\\boldsymbol{(y_i-\\mu)}\\boldsymbol{(y_i-\\mu)}^T$\nPosterior: $\\Sigma|\\boldsymbol{y} \\sim \\text{inv-}Wis(\\nu_0+n, [S_0+S_\\mu]^{-1})$\n\\begin{align}\rp(\\Sigma|\\boldsymbol{y_1, ..., y_n, \\mu}) \u0026amp;\\propto p(\\Sigma)p(\\boldsymbol{y_1, ..., y_n}|\\boldsymbol{\\mu},\\Sigma) \\\\\r\u0026amp;\\propto|\\Sigma|^{-(\\nu_0+p+1)/2} \\times exp\\Big(-\\frac{1}{2}tr(S_0\\Sigma^{-1})\\Big) \\times |\\Sigma|^{-n/2}exp\\bigg(-\\frac{1}{2}tr(S_\\mu\\Sigma^{-1})\\bigg) \\\\\r\u0026amp;\\propto |\\Sigma|^{-(\\nu_0+p+n+1)/2}exp\\bigg(-\\frac{1}{2}tr([S_0+S_\\mu]\\Sigma^{-1})\\bigg)\r\\end{align}\n\\begin{align}\rE[\\Sigma|\\boldsymbol{y_1, ..., y_n, \\mu}] \u0026amp;= \\frac{1}{\\nu_0+n-p-1}(S_0+S_\\mu) \\\\\r\u0026amp;= \\frac{\\nu_0-p-1}{\\nu_0+n-p-1}\\cdot\\frac{1}{\\nu_0-p-1}S_0 + \\frac{n}{\\nu_0+n-p-1}\\cdot\\frac{1}{n}S_\\mu \\\\\r\u0026amp;= \\frac{\\nu_0-p-1}{\\nu_0+n-p-1}\\cdot\\Sigma_0 + \\frac{n}{\\nu_0+n-p-1}\\cdot\\frac{1}{n}S_\\mu\r\\end{align}\nSummary  Semiconjugate prior for $\\mu$\nPrior: $\\boldsymbol{\\mu} \\text{ ~ } MVN(\\boldsymbol{\\mu_0}, \\Lambda_0)$\nLikelihood: $Y_1, ..., Y_n|\\boldsymbol{\\mu},\\Sigma \\text{ ~ iid } MVN(\\boldsymbol{\\mu}, \\Sigma)$\nPosterior: $\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma \\text{ ~ } MVN(\\boldsymbol{\\mu_n}, \\Lambda_n)$  \\begin{align}\r\\Lambda_n^{-1} \u0026amp;= \\Lambda_0^{-1}+n\\Sigma^{-1} \\\\\r\\boldsymbol{\\mu_n} \u0026amp;= (\\Lambda_0^{-1}+n\\Sigma^{-1})^{-1}(\\Lambda_0^{-1}\\boldsymbol{\\mu_0}+n\\Sigma^{-1}\\boldsymbol{\\bar{y}})\r\\end{align}\nSemiconjugate prior for $\\Sigma$\nPrior: $\\Sigma \\sim \\text{inv-}Wis(\\nu_0, S_0^{-1}) \\text{ where } S_0 = (\\nu_0-p-1)\\Sigma_0$\nLikelihood: $\\boldsymbol{Y}|\\boldsymbol{\\mu} \\stackrel{iid}\\sim MVN(\\boldsymbol{\\mu}, \\Sigma)$\nPosterior: $\\Sigma|\\boldsymbol{y_1, ..., y_n} \\sim \\text{inv-}Wis(\\nu_0+n, [S_0+S_\\mu]^{-1})$  $$E[\\Sigma|\\boldsymbol{y_1, ..., y_n, \\mu}] = \\frac{\\nu_0-p-1}{\\nu_0+n-p-1}\\cdot\\Sigma_0 + \\frac{n}{\\nu_0+n-p-1}\\cdot\\frac{1}{n}S_\\mu$$\n  그래프 그리기  4-2. Draw yourself Figure 7.2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  # Load Data test \u0026lt;- matrix(c(59, 43, 34, 32, 42, 38, 55, 67, 64, 45, 49, 72, 34, 70, 34, 50, 41, 52, 60, 34, 28, 35, 77, 39, 46, 26, 38, 43, 68, 86, 77, 60, 50, 59, 38, 48, 55, 58, 54, 60, 75, 47, 48, 33), ncol=2, byrow=FALSE) colnames(test) \u0026lt;- c(\u0026#39;pretest\u0026#39;,\u0026#39;posttest\u0026#39;) # Preparing n \u0026lt;- nrow(test) ybar \u0026lt;- colMeans(test) Sigma \u0026lt;- cov(test) THETA \u0026lt;- NULL SIGMA \u0026lt;- NULL inv \u0026lt;- solve sample.size = 5000 sample.new = NULL # prior mu0 \u0026lt;- c(50,50); nu0 \u0026lt;- 4 #(nu0 = p+2 = 4)  S0 \u0026lt;- L0 \u0026lt;- matrix(c(625,312.5,312.5,625), nrow=2, ncol=2) set.seed(2021) for(i in 1:sample.size){ # update theta Ln = inv(inv(L0) + n*inv(Sigma)) mun = Ln %*% (inv(L0)%*%mu0 + n*inv(Sigma)%*%ybar) theta = mvrnorm(1, mun, Ln) # update sigma Sn = S0 + (t(test)-theta)%*%t(t(test)-theta) Sigma = inv(rWishart(1, nu0+n, inv(Sn))[,,1]) # Save results THETA \u0026lt;- rbind(THETA, theta) SIGMA \u0026lt;- rbind(SIGMA, c(Sigma)) # sample new sample.new = rbind(sample.new, mvrnorm(n=1, mu=theta, Sigma=Sigma)) } rownames(THETA) \u0026lt;- 1:sample.size rownames(SIGMA) \u0026lt;- 1:sample.size   1 2 3 4 5 6 7 8 9 10  # graph(코드 따라하기) par(mfrow=c(1,2),mgp=c(1.75,.75,0),mar=c(3,3,1,1)) plot.hdr2d(THETA,xlab=expression(theta[1]),ylab=expression(theta[2]) ) abline(0,1) plot.hdr2d(sample.new,xlab=expression(italic(y[1])),ylab=expression(italic(y[2])), xlim=c(0,100),ylim=c(0,100) ) points(test[,1],test[,2],pch=16,cex=.7) abline(0,1)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # graph(ggplot 활용) p1 \u0026lt;- data.frame(THETA) %\u0026gt;% ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color=\u0026#39;orange\u0026#39;) + geom_abline(slope=1, intercept=0) + xlab(expression(theta[1])) + ylab(expression(theta[2])) + ggtitle(\u0026#39;Posterior draws of Mu\u0026#39;) p2 \u0026lt;- data.frame(sample.new) %\u0026gt;% ggplot(aes(x=pretest, y=posttest)) + geom_point(size=1, color=\u0026#39;orange\u0026#39;) + geom_abline(slope=1, intercept=0) + xlab(expression(y[1])) + ylab(expression(y[2])) + ggtitle(\u0026#39;Posterior Predictive\u0026#39;) grid.arrange(p1, p2, nrow=1)     4. Gibbs Sampling of the mean and covariance Gibbs sampling은 full conditional distribution을 통해 차례대로 모수를 업데이트하면서 joint posterior distribution을 구하는 것이 목적이다.\nSTEP1. Full conditional distribution을 확보한다.\n $\\boldsymbol{\\mu}|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}, \\Sigma \\sim MVN(\\boldsymbol{\\mu_n}, \\Lambda_n)$ $\\Sigma|\\boldsymbol{y_1, ..., y_n} \\sim \\text{inv-}Wis(\\nu_0+n, [S_0+S_\\mu]^{-1})$  STEP2. 차례대로 업데이트하면서 joint posterior distribution $\\boldsymbol{\\mu},\\Sigma|\\boldsymbol{y_1}, ..., \\boldsymbol{y_n}$을 구한다.\n4-1. NA imputation MAR(Missing at Random)인 경우에, missing data를 일종의 모수로 보고 gibbs sampling을 통해 na imputation을 해줄 수 있다.\nConclusion MVN도 잘 알아두자. inv-Wishart 분포도! 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":54,"section":"posts","tags":null,"title":"MVN","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb07/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Pymysql 패키지 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  import sys import requests import base64 import json import logging import pymysql #New library client_id = \u0026#39;\u0026#39; #직접 입력 client_secret = \u0026#39;\u0026#39; # 직접 입력 host = \u0026#39;\u0026#39; #host port = 3306 username = \u0026#39;\u0026#39; #user database = \u0026#39;\u0026#39; #db password = \u0026#39;\u0026#39; #passwd def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) cursor.execute(\u0026#39;SHOW TABLES\u0026#39;) print(cursor.fetchall()) print(\u0026#39;success\u0026#39;) sys.exit(0) if __name__ == \u0026#39;__main__\u0026#39;: main()   2. INSERT, UPDATE, REPLACE, INSERT IGNORE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  CREATE TABLE artists (id VARCHAR(255), name VARCHAR(255), followers INT, popularity INT, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET=\u0026#39;utf8\u0026#39;; CREATE TABLE artist_genres (artist_id VARCHAR(255), genre VARCHAR(255)) ENGINE=InnoDB DEFAULT CHARSET=\u0026#39;utf8\u0026#39;; SHOW CREATE TABLE artists; -- INSERT INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;1234\u0026#39;, \u0026#39;pop\u0026#39;); DELETE FROM artist_genres; --제거 DROP TABLE artist_genres; --제거 CREATE TABLE artist_genres (artist_id VARCHAR(255), genre VARCHAR(255), UNIQUE KEY(artist_id, genre)) ENGINE=InnoDB DEFAULT CHARSET=\u0026#39;utf8\u0026#39;; --unique key 생성 INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;1234\u0026#39;, \u0026#39;pop\u0026#39;); INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;1234\u0026#39;, \u0026#39;pop\u0026#39;); --두 번 하면 ERROR  -- UPDATE UPDATE artist_genres SET genre=\u0026#39;pop\u0026#39; WHERE artist_id =\u0026#39;1234\u0026#39;; ALTER TABLE artist_genres ADD COLUMN country VARCHAR(255); ALTER TABLE artist_genres ADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP; --업데이트되는 시각대 자동추가 INSERT INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;pop\u0026#39;,\u0026#39;UK\u0026#39;); -- 오류 발생함  -- REPLACE REPLACE INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;pop\u0026#39;,\u0026#39;UK\u0026#39;); /* 문제점(1) 지우고 업데이트하기 때문에 2번의 과정을 거쳐서 퍼포먼스적으로 문제 생길 수가 있다. 문제점(2) primary key가 auto_increment인 경우 새로운 숫자로 바뀌게 된다. */ -- INSERT IGNORE INSERT IGNORE INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;rock\u0026#39;,\u0026#39;UK\u0026#39;); /* 문제점(1) 이미 값이 있으면 추가하지 않게 된다.*/ -- INSERT ... ON DUPLICATE KEY UPDATE INSERT INTO artist_genres (artist_id, genre, country) VALUES (\u0026#39;1234\u0026#39;,\u0026#39;rock\u0026#39;,\u0026#39;UK\u0026#39;) ON DUPLICATE KEY UPDATE artist_id=\u0026#39;1234\u0026#39;, genre=\u0026#39;rock\u0026#39;, country=\u0026#39;FR\u0026#39;; --UK를 FR로 바꾼다.  -- ETC ALTER TABLE artist_genres DROP COLUMN country; --불필요한 칼럼 지우기   *MySQL에서 진행하였다.\n특이사항  data type을 INTEGER로 하니까 안 되고, INT로 하니까 됐다.  3. _, .format() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) cursor.execute(\u0026#39;SHOW TABLES\u0026#39;) print(cursor.fetchall()) query = \u0026#34;INSERT INTO artist_genres (artist_id, genre) VALUES (\u0026#39;{0}\u0026#39;, \u0026#39;{1}\u0026#39;)\u0026#34;.format(\u0026#39;2345\u0026#39;,\u0026#39;hip-hop\u0026#39;) cursor.execute(query) conn.commit() sys.exit(0) if __name__ == \u0026#39;__main__\u0026#39;: main()   4. Dictionary와 JSON Package 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) ## Spotify Search api params = { \u0026#39;q\u0026#39;: \u0026#39;BTS\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: \u0026#39;5\u0026#39; } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) print(raw[\u0026#39;artists\u0026#39;].keys) def get_headers(client_id, client_secret): endpoint = \u0026#39;https://accounts.spotify.com/api/token\u0026#39; encoded = base64.b64encode(\u0026#34;{}:{}\u0026#34;.format(client_id, client_secret).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;ascii\u0026#39;) headers = {\u0026#39;Authorization\u0026#39;: \u0026#39;Basic {}\u0026#39;.format(encoded)} payload = {\u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39;} r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)[\u0026#39;access_token\u0026#39;] headers = {\u0026#39;Authorization\u0026#39;: \u0026#34;Bearer {}\u0026#34;.format(access_token)} return headers if __name__ == \u0026#39;__main__\u0026#39;: main()   5. Duplicate Record 핸들링 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) ## Spotify Search api params = { \u0026#39;q\u0026#39;: \u0026#39;BTS\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: \u0026#39;1\u0026#39; } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) artist_raw = raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0] if artist_raw[\u0026#39;name\u0026#39;] == params[\u0026#39;q\u0026#39;]: artist = { \u0026#39;id\u0026#39;: artist_raw[\u0026#39;id\u0026#39;], \u0026#39;name\u0026#39;: artist_raw[\u0026#39;name\u0026#39;], \u0026#39;followers\u0026#39;: artist_raw[\u0026#39;followers\u0026#39;][\u0026#39;total\u0026#39;], \u0026#39;popularity\u0026#39;: artist_raw[\u0026#39;popularity\u0026#39;], \u0026#39;url\u0026#39;: artist_raw[\u0026#39;external_urls\u0026#39;][\u0026#39;spotify\u0026#39;], \u0026#39;image_url\u0026#39;: artist_raw[\u0026#39;images\u0026#39;][0][\u0026#39;url\u0026#39;] } query = \u0026#34;\u0026#34;\u0026#34; INSERT INTO artists (id, name, followers, popularity, url, image_url) VALUES (\u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;, {}, {}, \u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;) ON DUPLICATE KEY UPDATE id=\u0026#39;{}\u0026#39;, name=\u0026#39;{}\u0026#39;, followers={}, popularity={}, url=\u0026#39;{}\u0026#39;, image_url=\u0026#39;{}\u0026#39; \u0026#34;\u0026#34;\u0026#34;.format( artist[\u0026#39;id\u0026#39;], artist[\u0026#39;name\u0026#39;], artist[\u0026#39;followers\u0026#39;], artist[\u0026#39;popularity\u0026#39;], artist[\u0026#39;url\u0026#39;], artist[\u0026#39;image_url\u0026#39;], artist[\u0026#39;id\u0026#39;], artist[\u0026#39;name\u0026#39;], artist[\u0026#39;followers\u0026#39;], artist[\u0026#39;popularity\u0026#39;], artist[\u0026#39;url\u0026#39;], artist[\u0026#39;image_url\u0026#39;] ) cursor.execute(query) conn.commit()   6. Duplicate Record 핸들링을 위한 파이썬 함수 5와 다른 점을 눈여겨 보기 (5를 보다 간단하게 한 코드) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  def main(): ## .... 여기까지는 위와 동일 r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) artist = {} artist_raw = raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0] if artist_raw[\u0026#39;name\u0026#39;] == params[\u0026#39;q\u0026#39;]: artist.update({ \u0026#39;id\u0026#39;: artist_raw[\u0026#39;id\u0026#39;], \u0026#39;name\u0026#39;: artist_raw[\u0026#39;name\u0026#39;], \u0026#39;followers\u0026#39;: artist_raw[\u0026#39;followers\u0026#39;][\u0026#39;total\u0026#39;], \u0026#39;popularity\u0026#39;: artist_raw[\u0026#39;popularity\u0026#39;], \u0026#39;url\u0026#39;: artist_raw[\u0026#39;external_urls\u0026#39;][\u0026#39;spotify\u0026#39;], \u0026#39;image_url\u0026#39;: artist_raw[\u0026#39;images\u0026#39;][0][\u0026#39;url\u0026#39;] }) insert_row(cursor, data=artist, table=\u0026#39;artists\u0026#39;) conn.commit() def insert_row(cursor, data, table): placeholders = \u0026#39;, \u0026#39;.join([\u0026#39;%s\u0026#39;] * len(data)) columns = \u0026#39;, \u0026#39;.join(data.keys()) key_placeholders = \u0026#39;, \u0026#39;.join([\u0026#39;{0}=%s\u0026#39;.format(k) for k in data.keys()]) sql = \u0026#39;INSERT INTO %s( %s) VALUES ( %s) ON DUPLICATE KEY UPDATE %s\u0026#39; % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)   7. Artist list 추출하기  패스트캠퍼스 강좌를 통해 제공된 artist_list.csv 파일을 활용하였다.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) artists = [] with open(\u0026#39;../artist_list.csv\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) as f: raw = csv.reader(f) for row in raw: artists.append(row[0]) for a in artists: params = { \u0026#39;q\u0026#39;: a, \u0026#39;type\u0026#39;: \u0026#39;artist\u0026#39;, \u0026#39;limit\u0026#39;: \u0026#39;1\u0026#39; } r = requests.get(\u0026#39;https://api.spotify.com/v1/search\u0026#39;, params=params, headers=headers) raw = json.loads(r.text) artist = {} try: if raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;name\u0026#39;] == params[\u0026#39;q\u0026#39;]: artist.update( { \u0026#39;id\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;id\u0026#39;], \u0026#39;name\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;name\u0026#39;], \u0026#39;followers\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;followers\u0026#39;][\u0026#39;total\u0026#39;], \u0026#39;popularity\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;popularity\u0026#39;], \u0026#39;url\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;external_urls\u0026#39;][\u0026#39;spotify\u0026#39;], \u0026#39;image_url\u0026#39;: raw[\u0026#39;artists\u0026#39;][\u0026#39;items\u0026#39;][0][\u0026#39;images\u0026#39;][0][\u0026#39;url\u0026#39;] } ) insert_row(cursor, artist, \u0026#39;artists\u0026#39;) except: logging.error(\u0026#39;NO ITEMS FROM SEARCH API\u0026#39;) continue conn.commit() # sys.exit(0)   ERROR:root:NO ITEMS FROM SEARCH API와 같은 에러가 여러 개가 나오게 된다.\n말그대로 SERACH API를 통해서 ITEMS를 찾지 못하게 된 경우에 해당한다.\n8. Batch 형식으로 데이터 요청  한번에 묶어서 API에 전달하는 방식이다. 모든 API가 제공하는 것은 아니긴 하다! Spotify는 \u0026lsquo;Get Several Artists\u0026rsquo;하는 법을 제공하고 있다.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  def main(): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=\u0026#39;utf8\u0026#39;) cursor = conn.cursor() except: logging.error(\u0026#39;could not connect to RDS\u0026#39;) sys.exit(1) headers = get_headers(client_id, client_secret) cursor.execute(\u0026#34;SELECT id FROM artists\u0026#34;) artists = [] for (id, ) in cursor.fetchall(): artists.append(id) artist_batch = [artists[i: i+50] for i in range(0, len(artists), 50)] for i in artist_batch: ids = \u0026#39;,\u0026#39;.join(i) URL = \u0026#39;https://api.spotify.com/v1/artists/?ids={}\u0026#39;.format(ids) r= requests.get(URL, headers=headers) raw = json.loads(r.text) print(raw) print(len(raw[\u0026#39;artists\u0026#39;])) sys.exit(0)   1 2  -- 제대로 잘 들어갔는지 확인해보기 select * from artist_genres limit 10;   9. MySQL 테이블들로 데이터 저장 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  def main(): ## ... 여기까지는 위와 똑같음 artist_batch = [artists[i: i+50] for i in range(0, len(artists), 50)] artist_genres = [] for i in artist_batch: ids = \u0026#39;,\u0026#39;.join(i) URL = \u0026#39;https://api.spotify.com/v1/artists/?ids={}\u0026#39;.format(ids) r= requests.get(URL, headers=headers) raw = json.loads(r.text) for artist in raw[\u0026#39;artists\u0026#39;]: for genre in artist[\u0026#39;genres\u0026#39;]: artist_genres.append( { \u0026#39;artist_id\u0026#39;: artist[\u0026#39;id\u0026#39;], \u0026#39;genre\u0026#39;: genre } ) for data in artist_genres: insert_row(cursor, data, \u0026#39;artist_genres\u0026#39;) conn.commit() sys.exit(0) ## ... 아래도 다 똑같음   \n","description":"데이터의 이해와 데이터베이스","id":55,"section":"posts","tags":null,"title":"Python \u0026 MySQL","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_3/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Artist Data 1  SELECT genre, COUNT(*) FROM artist_genres GROUP BY 1 ORDER BY 2 DESC LIMIT 20;   2. Artist Genre Analysis with SQL 1 2 3  SELECT popularity, name FROM artists ORDER BY 1 DESC LIMIT 20; SELECT genre, COUNT(*) FROM artists t1 JOIN artist_genres t2 ON t2.artist_id = t1.id WHERE t1.popularity \u0026gt; 80 GROUP BY 1 ORDER BY 2 DESC LIMIT 20;    join을 통해 ERD의 장점을 활용하여 기초분석을 할 수 있다.  ","description":"데이터의 이해와 데이터베이스","id":56,"section":"posts","tags":null,"title":"SQL 활용 (MySQL)","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_4/"},{"content":"Chapter 09. The Multivariate Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.\n1. Linear Regression Model 2. Bayesian estimation for a regression model 3. Model Selection 1  library(tidyverse)     Code Example  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  ## Data load  data = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/yX.o2uptake\u0026#39;) y = data[,1] X = data[,-1] inv = solve ## set prior g = length(y) nu0 = 1 s20 = summary(lm(y~-1+X))$sigma^2 n = length(y) p = ncol(X) ## MCMC setup S = 1000 set.seed(2021) BETA = matrix(NA, nrow=S, ncol=p) sigma2 = matrix(NA, nrow=S, ncol=1) BETA[1,] = inv(t(X) %*% X) %*% t(X) %*% y sigma2[1,] = s20 ## gibbs sampling nun = nu0 + n betan = (g/(g+1)) * inv(t(X) %*% X) %*% t(X) %*% y for(s in 2:S){ s2n = nu0*s20 + t(y-X%*%BETA[s-1,]) %*% (y-X%*%BETA[s-1,]) sigma2[s,] = 1/rgamma(1, shape=nun/2, rate=s2n/2) Sigman = (g/(g+1)) * sigma2[s,] * inv(t(X) %*% X) BETA[s,] = MASS::mvrnorm(n=1, betan, Sigman) } ## graph colnames(BETA) = colnames(X) gather(as.data.frame(BETA)) %\u0026gt;% ggplot(aes(y=value, fill=key)) + geom_histogram() + coord_flip() + facet_wrap(~key, scales=\u0026#39;free_x\u0026#39;) + ggtitle(\u0026#39;Posterior samples of Beta\u0026#39;) + theme(legend.position = \u0026#39;None\u0026#39;)   ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r  ","description":"","id":57,"section":"posts","tags":null,"title":"Bayesian Linear Regression","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb09/"},{"content":"Part 4. 데이터의 이해와 데이터베이스 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n이 포스팅은 NoSQL 중 DynamoDB를 위주로 서술되어 있습니다.\n1. NoSQL vs. RDB  Not Only SQL 차이점(1) 다이나믹 스키마  구조를 정의하지 않고도 Documents, Key Values 등을 생성 각각의 Document가 서로 다른 구조로 구성 가능 데이터베이스들마다 다른 syntax 필드 추가 가능   차이점(2) Scalabilty  SQL DB: vertically scalable - CPU, RAM, SSD로 용량 문제 해결결 NoSQL DB: horizontally scalable - Sharding, Partitioning로 용량 문제 해결    2. Partition  데이터 나누기(vertical \u0026amp; horizontal)  데이터 매니지먼트, 퍼포먼스 등 다양한 이유     Vertical Partition  테이블을 더 작은 테이블로 나누기(Normalization와는 다름) ex. 지속적으로 업데이트되는 칼럼과 아닌 칼럼들 나누기   Horizontal Partition  Schema / Structure 자체를 복사하여 데이터 자체를 Sharded Key로 분리 NosQL DB에서는 필수적이다.    3. DynamoDB  aws.amazon.com \u0026gt; DynamoDB Partition Key는 SQL에서 Primary Key와 유사하다.  4. AWS SDK - Boto3 Package (DynamoDB 연결) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  import sys import os import boto3 import logging def main(): try: dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;, region_name=\u0026#39;ap-northeast-2\u0026#39;, endpoint_url=\u0026#39;http://dynamodb.ap-northeast-2.amazonaws.com\u0026#39;) except: logging.error(\u0026#34;could not connect to dynamodb\u0026#34;) sys.exit(1) print(\u0026#39;Success\u0026#39;) if __name__==\u0026#39;__main__\u0026#39;: main()   5. 테이블 생성 및 스펙  Provisioned(할당됨) vs. On-demand(온디맨드)  6. Global Index, Local Index 7. INSERT(Single, Batch items) boto3 Documentation 읽어보기\nCreating a New Item 1 2 3 4 5 6 7 8 9  table.put_item( Item={ \u0026#39;username\u0026#39;: \u0026#39;janedoe\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Jane\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doe\u0026#39;, \u0026#39;age\u0026#39;: 25, \u0026#39;account_type\u0026#39;: \u0026#39;standard_user\u0026#39;, } )   Batch Writing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  with table.batch_writer() as batch: batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;standard_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;johndoe\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;John\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doe\u0026#39;, \u0026#39;age\u0026#39;: 25, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;1 Jefferson Street\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Los Angeles\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;CA\u0026#39;, \u0026#39;zipcode\u0026#39;: 90001 } } ) batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;super_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;janedoering\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Jane\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doering\u0026#39;, \u0026#39;age\u0026#39;: 40, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;2 Washington Avenue\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Seattle\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;WA\u0026#39;, \u0026#39;zipcode\u0026#39;: 98109 } } ) batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;standard_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;bobsmith\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Bob\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Smith\u0026#39;, \u0026#39;age\u0026#39;: 18, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;3 Madison Lane\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Louisville\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;KY\u0026#39;, \u0026#39;zipcode\u0026#39;: 40213 } } ) batch.put_item( Item={ \u0026#39;account_type\u0026#39;: \u0026#39;super_user\u0026#39;, \u0026#39;username\u0026#39;: \u0026#39;alicedoe\u0026#39;, \u0026#39;first_name\u0026#39;: \u0026#39;Alice\u0026#39;, \u0026#39;last_name\u0026#39;: \u0026#39;Doe\u0026#39;, \u0026#39;age\u0026#39;: 27, \u0026#39;address\u0026#39;: { \u0026#39;road\u0026#39;: \u0026#39;1 Jefferson Street\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Los Angeles\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;CA\u0026#39;, \u0026#39;zipcode\u0026#39;: 90001 } } )   8. 데이터 요청 및 제한점 boto3 Documentation 읽어보기\nGetting an Item 1 2 3 4 5 6  response = table.get_item( Key = { \u0026#39;artist_id\u0026#39;: \u0026#39;00FQb4jTyendYWaN8pK0wa\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;0Oqc0kKFsQ6MhFOLBNZIGX\u0026#39; } )   ClientError: An error occurred (ValidationException) when calling the GetItem operation: The provided key element does not match the schema\r위와 같은 에러가 뜬다면, key값을 제대로 다 넣었는지 확인해본다.\nQuerying and Scanning  Querying: Primary Key 값을 알고 있을 때 활용 Scanning: Primary Key 값을 모르지만, 다른 attribute를 알 때 활용\n- Scan은 모든 행을 다 훑는 비효율적인 기능이므로 꼭 필요할 때만 쓰는 것이 권장된다.  1 2 3 4 5 6 7 8 9 10 11 12  # Querying response = table.query( KeyConditionExpression=Key(\u0026#39;artist_id\u0026#39;).eq(\u0026#39;00FQb4jTyendYWaN8pK0wa\u0026#39;), FilterExpression=Attr(\u0026#39;popularity\u0026#39;).gt(75) #query도 filterexpresson 쓸 수 있다! ) print(len(response[\u0026#39;Items\u0026#39;])) # Scanning response = table.scan( FilterExpression=Attr(\u0026#39;popularity\u0026#39;).gt(75) ) print(len(response[\u0026#39;Items\u0026#39;]))   \n","description":"데이터의 이해와 데이터베이스","id":58,"section":"posts","tags":null,"title":"NoSQL (DynamoDB)","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part4_5/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. 데이터 레이크 vs. 데이터 웨어하우스    구분 데이터 레이크 데이터 웨어하우스     Data Structure Raw Processed   Purpose of Data Not yet Determined In Use   Users Data Scientists Business Professionals   Accessibility High / Quick to update Complicated / Costly     Schema의 차이가 가장 크다. 데이터레이크는 차세대 시스템으로서 더욱 주목 받게 될 것이다. ETL(Extract-Transform-Load)  2. 데이터 레이크 아키텍처 데이터 파이프라인의 관심사  어떻게 관리 스케쥴링 에러 핸들링 데이터 백필  3. AWS S3  AWS S3 버킷 생성 cf. AWS Glue: table schema 관리 (데이터 레이크는 바뀐다.)  4. JSON, Parquet 1 2 3 4 5  # JSON 형식으로 하는 법 with open(\u0026#39;top_tracks.json\u0026#39;, \u0026#39;w\u0026#39;) as f: for i in top_tracks: json.dump(i, f) f.write(os.linesep)   1 2 3 4 5 6 7 8 9 10 11  # Parquet 형식으로 하는 법 # 퍼포먼스적으로 우수해진다. top_tracks = pd.DataFrame(raw) top_tracks.to_parquet(\u0026#39;top-tracks.parquet\u0026#39;, engine=\u0026#39;pyarrow\u0026#39;, compressions=\u0026#39;snappy\u0026#39;) dt = datetime.utcnow().strftime(\u0026#34;%Y-%m-%d\u0026#34;) s3 = boto3.resource(\u0026#39;s3\u0026#39;) object = s3.Object(\u0026#39;spotify-artists\u0026#39;, \u0026#39;dt={}/top-tracks.parquet\u0026#39;.format(dt)) data = open(\u0026#39;top-tracks.parquet\u0026#39;, \u0026#39;rb\u0026#39;) object.put(Body=data)   5. S3 Data Lake ImportError: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\rSpotify audio features\n","description":"데이터 엔지니어링 구축","id":59,"section":"posts","tags":null,"title":"데이터 레이크","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_1/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Presto 2. Serverless 3. AWS Athena 4. 테이블 생성 5. 데이터 쿼리 ","description":"데이터 엔지니어링 구축","id":60,"section":"posts","tags":null,"title":"S3 Athena","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_2/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Apache Spark 2. EC-2 제플린 인스턴스 생성 ","description":"데이터 엔지니어링 구축","id":61,"section":"posts","tags":null,"title":"Apache Spark","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_3/"},{"content":"Part 5. 데이터 엔지니어링 구축 본 포스팅은 패스트캠퍼스(FastCampus)의 데이터 엔지니어링 올인원 패키지 Online을 참고하였습니다.\n1. Spark RDD 2. Spark Dataframes 3. Select Subset Columns 4. Filter Rows 5. Create UDF 6. Join 7. SQL 8. 데이터분석 with Spark 9. 시각화 with Spark ","description":"데이터 엔지니어링 구축","id":62,"section":"posts","tags":null,"title":"Pyspark","uri":"https://jiwooblog.netlify.app/posts/dataengineering/fc_part5_4/"},{"content":"Bias-Variance Tradeoff $$\\text{let } y = f(x) + \\varepsilon, \\text{ where } \\varepsilon \\text{ ~ } N(0, \\sigma^2)$$\n\\begin{align}\rMSE(\\hat{y}) \u0026amp;= E[(y-\\hat{y})^2] \\\\\r\u0026amp;= E[(f(x) + \\varepsilon - \\hat{f}(x))^2] \\\\\r\u0026amp;= E[(f(x)-\\hat{f}(x))^2] + E[\\varepsilon^2] + 2E[\\varepsilon(f(x) - \\hat{f}(x))] \\\\\r\u0026amp;= E[(f(x) - E(\\hat{f}(x)) + E(\\hat{f}(x)) - \\hat{f}(x) )^2] + \\sigma^2 \\\\\r\u0026amp;= E[(f(x) - E(\\hat{f}(x)))^2] + E[(\\hat{f}(x)-E(\\hat{f}(x)))^2] + 2(f(x) - E(\\hat{f}(x)))E[\\hat{f}(x) - E(\\hat{f}(x))] + \\sigma^2 \\\\\r\u0026amp;= {bias[\\hat{f}(x)]}^2 + Var(\\hat{f}(x)) + \\sigma^2 \\\\\r\u0026amp;= bias^2(\\hat{y}) + Var(\\hat{y}) + \\sigma^2\r\\end{align}\n$bias^2(\\hat{y})$: 참값과 추정치 평균의 차이 (간단한 모형일수록 높음)\n$Var(\\hat{y})$: 추정치와 추정치 평균의 차이 (복잡한 모형일수록 높음)\n$\\sigma^2$: irreducible error\n편향(bias)는 간단한 모형일수록 높으며, 분산(variance)는 복잡한 모형일수록 높다. 이때 $\\sigma^2$는 줄일 수 없는 오차이다. 그렇다면 적절한 모형 선택과 실험설계를 통해서 과적합을 방지해야 하는데, 이러한 부분은 머신러닝을 하면서 특히 더 주의해야 하는 부분이다.\nbias와 variance의 차이가 헷갈린다면 아래의 그림을 보면 훨씬 잘 이해가 될 것이다.\n참고 [1] https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":63,"section":"posts","tags":null,"title":"편향-분산 Tradeoff","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/bias_variance/"},{"content":"Performance Measure 성능을 평가하는 지표에는 여러 가지가 있다. 그중에서 대표적인 몇 개를 알아보고자 한다.\n1. Accuracy $$Accuracy = \\frac{\\text{correctly predicted}}{\\text{all dataset}}$$\nAccuracy는 balanced data가 아니라면 좋은 지표로서의 역할을 하기 힘들다. 왜냐하면 A,B,C,D라는 그룹이 있을 때, B~D가 각각 10개의 케이스 밖에 없고 A가 혼자서 500개의 케이스가 있다고 가정한다면 Accuracy 지표는 A 그룹에 의해 좌지우지 될 것이기 때문이다.\n2. Precision  Given a class prediction from the classifier, how likely is to be correct?\n내가 \u0026lsquo;TRUE\u0026rsquo;라고 말한 것 중에서 몇 %가 진짜 \u0026lsquo;TRUE\u0026rsquo;인가?\n $$Precision = \\frac{TP}{TP + FP} $$\nTP: True Positive, FP: False Positive\n3. Recall  Given a class, will the classifier detect it?\n진짜 \u0026lsquo;TRUE\u0026rsquo; 중에서 내가 \u0026lsquo;TRUE\u0026rsquo;라고 말한 것은 몇 %인가?\n $$Recall = \\frac{TP}{TP + FN} $$\nTP: True Positive, FN: False Negative\n4. F1 Score Precision과 Recall의 조화평균\n$$F1 \\text{ score} = 2 \\times \\frac{\\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}} $$\n 출처: https://www.youtube.com/watch?v=HBi-P5j0Kec F1 Score는 상대적으로 imbalanced data에서도 효과가 좋다. 그 이유는 위 그림을 통해서 이해할 수 있다.\n조화평균은 더 극단치에 대해서 페널티를 주기 때문이다.\n참고 [1] 허민석 유튜브\n","description":"","id":64,"section":"posts","tags":null,"title":"F1 Score","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/performance_measure/"},{"content":"Fisher Information Fisher information은 score function의 분산이다. 이 점을 명심해서 의미를 생각해보자. 아래는 Fisher information을 다양한 꼴로 표현해본 것이다.\n\\begin{align}\rI(\\theta;x) \u0026amp;= Var(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)) \\\\\r\u0026amp;= E\\bigg[\\Big(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\Big)^2\\bigg] \\\\\r\u0026amp;= -E\\bigg[\\frac{\\partial^2}{\\partial\\theta^2}log(f(x;\\theta)\\bigg]\r\\end{align}\n계산 증명 위에 대한 자세한 계산 증명은 아래와 같다.\n우선, score function의 평균이 0임을 구하는 것부터 시작한다.\n$$\\int \\frac{\\partial}{\\partial\\theta}logf(x;\\theta)f(x;\\theta)=0 $$\n양변을 $\\theta$로 미분해준다.\n$$\\int\\frac{\\partial^2}{\\partial\\theta^2}logf(x;\\theta)f(x;\\theta)dx + \\int\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)f'(x;\\theta)dx = 0 $$\n오른쪽 부분을 우변으로 이항하여 정리해주면 아래와 같다.\n\\begin{align}\rE\\bigg[\\frac{\\partial^2}{\\partial\\theta^2}logf(x;\\theta)\\bigg] \u0026amp;= -\\int\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)f'(x;\\theta)dx \\\\ \u0026amp;= -\\int\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\frac{f'(x;\\theta)}{f(x;\\theta)}f(x;\\theta)dx \\\\\r\u0026amp;= -\\int\\bigg(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\bigg)^2f(x;\\theta)dx \\\\\r\u0026amp;= -E\\bigg[\\Big(\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\Big)^2\\bigg]\r\\end{align}\n","description":"","id":65,"section":"posts","tags":null,"title":"Fisher Information","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/fisher_information/"},{"content":"Jeffrey\u0026rsquo;s Prior 1. uninformative prior의 제한점 uninformative prior를 임의로 주게 될 경우, 여러 문제점이 있을 수 있는데 그 중 하나는 변수변환에 취약해질 수 있다는 점이다.\n예를 들어, $p(\\theta) \\propto 1$라고 uninformative prior를 주자. 그리고 $ \\phi = exp(\\theta)$라고 가정해보자.\n\\begin{align}\rp(\\phi) \u0026amp;\\propto p(\\theta) \\bigg|\\frac{d\\theta}{d\\phi}\\bigg| \\\\\r\u0026amp;\\propto \\frac{1}{\\phi} \\neq 1\r\\end{align}\n변수변환 후에는 prior가 uninformative하지 않게 되어버림을 확인할 수 있다.\n2. Jeffrey\u0026rsquo;s prior 그렇다면 어떻게 해야 변수변환에 강건한 prior를 줄 수 있을까?\n$$\\pi(\\phi) \\propto \\sqrt{I(\\theta)} $$\n위와 같이 주면 된다. 여기서 $I(\\theta) $는 Fisher Information을 뜻하며, 아래와 같다.\n$$I(\\theta) = -E\\Big[ \\frac{\\partial^2}{\\partial{\\theta}^2}ln L(x|\\theta) \\Big]$$\n3. 증명 이를 한 번 증명해보자. 단, $\\phi = f(\\theta), \\ f:\\text{one-to-one}$이다.\n\\begin{align}\rp(\\theta) \\propto \\sqrt{I(\\theta)} \u0026amp;\\xrightarrow{?} \\ p(\\phi) \\propto \\sqrt{I(\\phi)} \\\\\r\\\\\rp(\\phi) \u0026amp;= p(\\theta) \\bigg| \\frac{\\partial\\theta}{\\partial\\phi} \\bigg| \\\\\r\\\\\rI(\\phi) \u0026amp;= -E\\bigg[\\frac{\\partial^2}{\\partial\\phi^2}lnL(y|\\phi) \\bigg] \\\\\r\u0026amp;= E\\bigg[\\big(\\frac{\\partial}{\\partial\\phi}lnL(y|\\phi) \\big)^2 \\bigg] \\\\\r\u0026amp;= E\\bigg[\\big(\\frac{\\partial}{\\partial\\theta}lnL(y|\\theta) \\big)^2 \\big(\\frac{\\partial\\theta}{\\partial\\phi} \\big)^2 \\bigg] \\\\ \u0026amp;= I(\\theta) \\bigg|\\frac{\\partial\\theta}{\\partial\\phi} \\bigg|^2\\\\\r\\\\\rp(\\phi) \u0026amp;= p(\\theta)\\bigg|\\frac{\\partial\\theta}{\\partial\\phi} \\bigg| \\\\\r\u0026amp;\\propto \\sqrt{I(\\theta)}\\bigg|\\frac{\\partial\\theta}{\\partial\\phi} \\bigg| \\\\\r\u0026amp;=\\sqrt{I(\\phi)} \\\\\r\\\\\r\\rightarrow p(\\phi) \u0026amp;\\propto \\sqrt{I(\\phi)}\r\\end{align}\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":66,"section":"posts","tags":null,"title":"Jeffrey's Prior","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/jeffrey_prior/"},{"content":"Score Function $X \\text{ ~ } f(x;\\theta)$일 때, score function $s(\\theta;x)$은 다음과 같이 정의한다.\n$$\\frac{\\partial}{\\partial\\theta}logf(x;\\theta) $$\n평균 계산 증명 \\begin{align}\rE\\big[s(\\theta;x) \\big] \u0026amp;= \\int\\bigg[\\frac{\\partial}{\\partial\\theta}logf(x;\\theta)\\bigg]f(x;\\theta)dx \\\\\r\u0026amp;= \\int\\frac{f'(x;\\theta)}{f(x;\\theta)}f(x;\\theta)dx \\\\\r\u0026amp;= \\int f'(x;\\theta)dx \\\\\r\u0026amp;= \\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta)dx \\\\\r\u0026amp;= \\frac{\\partial}{\\partial\\theta}1 = 0\r\\end{align}\n의미 score function은 log likelihood의 기울기를 나타낸다는 데에서 의미가 있다.\nscore function의 평균이 0이라는 것의 의미를 그래프를 likelihood 그래프를 상상하여 생각해보자.\n","description":"","id":67,"section":"posts","tags":null,"title":"Score Function","uri":"https://jiwooblog.netlify.app/posts/statistics/statistics/score_function/"},{"content":"Empirical Bayes 베이지안은 기본적으로 prior, 즉 사전확률을 설정한다.\n그런데 이때 data density(histogram)에 비슷한 모양을 갖도록 prior를 설정하고자 하는 경우도 있는데, 이를 Empirical Bayes라고 한다.\n사전확률인데 왜 데이터를 보고 설정하는지에 대한 의문이 생길 수도 있긴 한다\u0026hellip;\n혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. ","description":"","id":68,"section":"posts","tags":null,"title":"Empirical Bayes","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/empricialbayes/"},{"content":"Gibbs Sampler 기본 원리 (추후 업데이트)\n예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  ## Data load  data = dget(\u0026#39;https://www2.stat.duke.edu/~pdh10/FCBS/Inline/yX.o2uptake\u0026#39;) y = data[,1] X = data[,-1] inv = solve ## set prior g = length(y) nu0 = 1 s20 = summary(lm(y~-1+X))$sigma^2 n = length(y) p = ncol(X) ## setup S = 1000 set.seed(2021) BETA = matrix(NA, nrow=S, ncol=p) sigma2 = matrix(NA, nrow=S, ncol=1) BETA[1,] = inv(t(X) %*% X) %*% t(X) %*% y sigma2[1,] = s20 ## gibbs sampling nun = nu0 + n betan = (g/(g+1)) * inv(t(X) %*% X) %*% t(X) %*% y for(s in 2:S){ s2n = nu0*s20 + t(y-X%*%BETA[s-1,]) %*% (y-X%*%BETA[s-1,]) sigma2[s,] = 1/rgamma(1, shape=nun/2, rate=s2n/2) Sigman = (g/(g+1)) * sigma2[s,] * inv(t(X) %*% X) BETA[s,] = MASS::mvrnorm(n=1, betan, Sigman) } ## graph colnames(BETA) = colnames(X) gather(as.data.frame(BETA)) %\u0026gt;% ggplot(aes(y=value, fill=key)) + geom_histogram() + coord_flip() + facet_wrap(~key, scales=\u0026#39;free_x\u0026#39;) + ggtitle(\u0026#39;Posterior samples of Beta\u0026#39;) + theme(legend.position = \u0026#39;None\u0026#39;)   ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r종류 1. Systematic Sweep Gibbs Sampler 모든 모수에 대해 샘플링을 반복하여 업데이트하는 방법\n2. Random Sweep Gibbs Sampler 무작위로 모수를 뽑아서 업데이트하는 방법\n3. Grouped Gibbs Sampler 여러 개의 모수를 한번에 뽑아서 업데이트하는 방법\n이때 모수 간의 연관성이 생긴다.\n4. Collapsed Gibbs Sampler 적분을 통해서 특정 모수와 독립적인 분포를 계산한 후, 샘플링하여 업데이트하는 방법\n참고사이트 [1] https://niceguy1575.tistory.com/entry/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%86%B5%EA%B3%84%ED%95%99-4-Gibbs-Sampler%EA%B9%81%EC%8A%A4-%EC%83%98%ED%94%8C%EB%9F%AC-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%EC%84%B1%EC%A7%88\n","description":"","id":69,"section":"posts","tags":null,"title":"Gibbs Sampler","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/gibbs_sampler/"},{"content":"fread 패키지로 대용량 데이터 빠르게 불러오기 약 24000행의 샘플 csv가 있다고 가정하자. 그렇다면 fread와 read.csv의 성능은 다음과 같다.\n1 2 3 4 5 6 7 8 9 10 11  library(data.table) fread(\u0026#39;sample.csv\u0026#39;) #약 24000x3 system.time(fread(\u0026#39;sample.csv\u0026#39;)) # 사용자 시스템 elapsed  # 0.02 0.00 0.01  system.time(read.csv(\u0026#39;sample.csv\u0026#39;)) # 사용자 시스템 elapsed  # 0.74 0.03 0.77   read.csv는 0.77초가 걸리는 데에 반해 fread는 0.01초 만에 읽어왔다.\n이외에도 3백만 행의 csv으로 실험해본 결과, 각각 2초와 33초로 그 성능 차이가 더욱 도드라짐을 알 수 있었다.\n","description":"","id":70,"section":"posts","tags":null,"title":"fread","uri":"https://jiwooblog.netlify.app/posts/r/fread/"},{"content":"\r\ry \u0026lt;- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114)\rn \u0026lt;- length(y)\rs2 \u0026lt;- var(y)\rmy \u0026lt;- mean(y) \r# helper functions to sample from and evaluate\r# scaled inverse chi-squared distribution\rrsinvchisq \u0026lt;- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)\rdsinvchisq \u0026lt;- function(x, nu, s2){\rexp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x)\r}\rns \u0026lt;- 1000\rsigma2 \u0026lt;- rsinvchisq(ns, n-1, s2)\rmu \u0026lt;- my + sqrt(sigma2/n)*rnorm(length(sigma2))\rsigma \u0026lt;- sqrt(sigma2)\rynew \u0026lt;- rnorm(ns, mu, sigma)\rt1l \u0026lt;- c(90, 150)\rt2l \u0026lt;- c(10, 60)\rnl \u0026lt;- c(50, 185)\rt1 \u0026lt;- seq(t1l[1], t1l[2], length.out = ns)\rt2 \u0026lt;- seq(t2l[1], t2l[2], length.out = ns)\rxynew \u0026lt;- seq(nl[1], nl[2], length.out = ns)\r# multiplication by 1./sqrt(s2/n) is due to the transformation of\r# variable z=(x-mean(y))/sqrt(s2/n), see BDA3 p. 21\rpm \u0026lt;- dt((t1-my) / sqrt(s2/n), n-1) / sqrt(s2/n)\rpmk \u0026lt;- density(mu, adjust = 2, n = ns, from = t1l[1], to = t1l[2])$y\r# the multiplication by 2*t2 is due to the transformation of\r# variable z=t2^2, see BDA3 p. 21\rps \u0026lt;- dsinvchisq(t2^2, n-1, s2) * 2*t2\rpsk \u0026lt;- density(sigma, n = ns, from = t2l[1], to = t2l[2])$y\r# multiplication by 1./sqrt(s2/n) is due to the transformation of variable\r# see BDA3 p. 21\rp_new \u0026lt;- dt((xynew-my) / sqrt(s2*(1+1/n)), n-1) / sqrt(s2*(1+1/n))\r# Combine grid points into another data frame\r# with all pairwise combinations\rdfj \u0026lt;- data.frame(t1 = rep(t1, each = length(t2)),\rt2 = rep(t2, length(t1)))\rdfj$z \u0026lt;- dsinvchisq(dfj$t2^2, n-1, s2) * 2*dfj$t2 * dnorm(dfj$t1, my, dfj$t2/sqrt(n))\r# breaks for plotting the contours\rcl \u0026lt;- seq(1e-5, max(dfj$z), length.out = 6)\rDemo 3.1\rdfm \u0026lt;- data.frame(t1, Exact = pm, Empirical = pmk) %\u0026gt;% gather(grp, p, -t1)\rmargmu \u0026lt;- ggplot(dfm) +\rgeom_line(aes(t1, p, color = grp)) +\rcoord_cartesian(xlim = t1l) +\rlabs(title = \u0026#39;Marginal of mu\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) +\rscale_y_continuous(breaks = NULL) +\rtheme(legend.background = element_blank(),\rlegend.position = c(0.75, 0.8),\rlegend.title = element_blank())\rdfs \u0026lt;- data.frame(t2, Exact = ps, Empirical = psk) %\u0026gt;% gather(grp, p, -t2)\rmargsig \u0026lt;- ggplot(dfs) +\rgeom_line(aes(t2, p, color = grp)) +\rcoord_cartesian(xlim = t2l) +\rcoord_flip() +\rlabs(title = \u0026#39;Marginal of sigma\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) +\rscale_y_continuous(breaks = NULL) +\rtheme(legend.background = element_blank(),\rlegend.position = c(0.75, 0.8),\rlegend.title = element_blank())\rjoint1labs \u0026lt;- c(\u0026#39;Samples\u0026#39;,\u0026#39;Exact contour\u0026#39;)\rjoint1 \u0026lt;- ggplot() +\rgeom_point(data = data.frame(mu,sigma), aes(mu, sigma, col = \u0026#39;1\u0026#39;), size = 0.1) +\rgeom_contour(data = dfj, aes(t1, t2, z = z, col = \u0026#39;2\u0026#39;), breaks = cl) +\rcoord_cartesian(xlim = t1l,ylim = t2l) +\rlabs(title = \u0026#39;Joint posterior\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;\u0026#39;) +\rscale_y_continuous(labels = NULL) +\rscale_x_continuous(labels = NULL) +\rscale_color_manual(values=c(\u0026#39;blue\u0026#39;, \u0026#39;black\u0026#39;), labels = joint1labs) +\rguides(color = guide_legend(nrow = 1, override.aes = list(\rshape = c(16, NA), linetype = c(0, 1), size = c(2, 1)))) +\rtheme(legend.background = element_blank(),\rlegend.position = c(0.5, 0.9),\rlegend.title = element_blank())\rgrid.arrange(joint1, margsig, margmu, nrow = 2)\r\r","description":"","id":71,"section":"posts","tags":null,"title":"BDA Example","uri":"https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/"},{"content":"\r\r\r\r\r\r.scroll-100 {\rmax-height: 100px;\roverflow-y: auto;\rbackground-color: 'white';\r}\r\rpre {\rmax-height: 300px;\roverflow-y: auto;\rbackground-color: 'white';\r}\rpre[class] {\rmax-height: 100px;\rbackground-color: 'green';\r}\r\r\r# plotly\r```r\rlibrary(ggplot2)\rlibrary(plotly)\rlibrary(gapminder)\r1 2 3 4 5 6 7 8  p \u0026lt;- gapminder %\u0026gt;% filter(year==1977) %\u0026gt;% ggplot( aes(gdpPercap, lifeExp, size = pop, color=continent)) + geom_point() + scale_x_log10() + theme_bw() ggplotly(p)   \r{\"x\":{\"data\":[{\"x\":[3.69111835304969,3.47837128685849,3.0124834262036,3.50716177034781,2.87121498358872,2.74515544499733,3.25125676748187,3.04507811557752,3.05460729073385,3.06915101836112,2.90078062155373,3.51310821042633,3.40101028408395,3.48879895792208,3.44490216214739,2.98162238872728,2.7039391611549,2.74570576546197,4.3373708614867,2.94682314869669,2.99704718642708,2.94185210799775,2.88350583493768,3.10298675433845,2.87237164152654,2.8063987207202,4.34145849934605,3.18871158763867,2.82166002181421,2.83657428165302,3.17536457562432,3.56948896052176,3.37486193965761,2.70098023955392,3.58843821514052,2.90789326381397,3.29709308978461,3.6354640490023,2.82612704524949,3.23994022438144,3.19361683040004,3.12978175413031,3.16166517152032,3.90464260358686,3.34301221488094,3.57765383934322,2.9833972607425,3.185478974435,3.49427662628827,2.92620510614229,3.20103869689067,2.83606300605042],\"y\":[58.014,39.483,49.19,59.319,46.137,45.91,49.355,46.775,47.383,50.939,47.804,55.625,52.374,46.519,53.319,42.024,44.535,44.51,52.79,41.842,51.756,40.762,37.465,56.155,52.208,43.764,57.442,46.881,43.767,41.714,50.852,64.93,55.73,42.495,56.437,41.291,44.514,67.064,45,58.55,48.879,36.788,41.974,55.527,47.8,52.537,49.919,52.887,59.837,50.35,51.386,57.674],\"text\":[\"gdpPercap: 4910.4168\nlifeExp: 58.01400\npop: 17152804\ncontinent: Africa\",\"gdpPercap: 3008.6474\nlifeExp: 39.48300\npop: 6162675\ncontinent: Africa\",\"gdpPercap: 1029.1613\nlifeExp: 49.19000\npop: 3168267\ncontinent: Africa\",\"gdpPercap: 3214.8578\nlifeExp: 59.31900\npop: 781472\ncontinent: Africa\",\"gdpPercap: 743.3870\nlifeExp: 46.13700\npop: 5889574\ncontinent: Africa\",\"gdpPercap: 556.1033\nlifeExp: 45.91000\npop: 3834415\ncontinent: Africa\",\"gdpPercap: 1783.4329\nlifeExp: 49.35500\npop: 7959865\ncontinent: Africa\",\"gdpPercap: 1109.3743\nlifeExp: 46.77500\npop: 2167533\ncontinent: Africa\",\"gdpPercap: 1133.9850\nlifeExp: 47.38300\npop: 4388260\ncontinent: Africa\",\"gdpPercap: 1172.6030\nlifeExp: 50.93900\npop: 304739\ncontinent: Africa\",\"gdpPercap: 795.7573\nlifeExp: 47.80400\npop: 26480870\ncontinent: Africa\",\"gdpPercap: 3259.1790\nlifeExp: 55.62500\npop: 1536769\ncontinent: Africa\",\"gdpPercap: 2517.7365\nlifeExp: 52.37400\npop: 7459574\ncontinent: Africa\",\"gdpPercap: 3081.7610\nlifeExp: 46.51900\npop: 228694\ncontinent: Africa\",\"gdpPercap: 2785.4936\nlifeExp: 53.31900\npop: 38783863\ncontinent: Africa\",\"gdpPercap: 958.5668\nlifeExp: 42.02400\npop: 192675\ncontinent: Africa\",\"gdpPercap: 505.7538\nlifeExp: 44.53500\npop: 2512642\ncontinent: Africa\",\"gdpPercap: 556.8084\nlifeExp: 44.51000\npop: 34617799\ncontinent: Africa\",\"gdpPercap: 21745.5733\nlifeExp: 52.79000\npop: 706367\ncontinent: Africa\",\"gdpPercap: 884.7553\nlifeExp: 41.84200\npop: 608274\ncontinent: Africa\",\"gdpPercap: 993.2240\nlifeExp: 51.75600\npop: 10538093\ncontinent: Africa\",\"gdpPercap: 874.6859\nlifeExp: 40.76200\npop: 4227026\ncontinent: Africa\",\"gdpPercap: 764.7260\nlifeExp: 37.46500\npop: 745228\ncontinent: Africa\",\"gdpPercap: 1267.6132\nlifeExp: 56.15500\npop: 14500404\ncontinent: Africa\",\"gdpPercap: 745.3695\nlifeExp: 52.20800\npop: 1251524\ncontinent: Africa\",\"gdpPercap: 640.3224\nlifeExp: 43.76400\npop: 1703617\ncontinent: Africa\",\"gdpPercap: 21951.2118\nlifeExp: 57.44200\npop: 2721783\ncontinent: Africa\",\"gdpPercap: 1544.2286\nlifeExp: 46.88100\npop: 8007166\ncontinent: Africa\",\"gdpPercap: 663.2237\nlifeExp: 43.76700\npop: 5637246\ncontinent: Africa\",\"gdpPercap: 686.3953\nlifeExp: 41.71400\npop: 6491649\ncontinent: Africa\",\"gdpPercap: 1497.4922\nlifeExp: 50.85200\npop: 1456688\ncontinent: Africa\",\"gdpPercap: 3710.9830\nlifeExp: 64.93000\npop: 913025\ncontinent: Africa\",\"gdpPercap: 2370.6200\nlifeExp: 55.73000\npop: 18396941\ncontinent: Africa\",\"gdpPercap: 502.3197\nlifeExp: 42.49500\npop: 11127868\ncontinent: Africa\",\"gdpPercap: 3876.4860\nlifeExp: 56.43700\npop: 977026\ncontinent: Africa\",\"gdpPercap: 808.8971\nlifeExp: 41.29100\npop: 5682086\ncontinent: Africa\",\"gdpPercap: 1981.9518\nlifeExp: 44.51400\npop: 62209173\ncontinent: Africa\",\"gdpPercap: 4319.8041\nlifeExp: 67.06400\npop: 492095\ncontinent: Africa\",\"gdpPercap: 670.0806\nlifeExp: 45.00000\npop: 4657072\ncontinent: Africa\",\"gdpPercap: 1737.5617\nlifeExp: 58.55000\npop: 86796\ncontinent: Africa\",\"gdpPercap: 1561.7691\nlifeExp: 48.87900\npop: 5260855\ncontinent: Africa\",\"gdpPercap: 1348.2852\nlifeExp: 36.78800\npop: 3140897\ncontinent: Africa\",\"gdpPercap: 1450.9925\nlifeExp: 41.97400\npop: 4353666\ncontinent: Africa\",\"gdpPercap: 8028.6514\nlifeExp: 55.52700\npop: 27129932\ncontinent: Africa\",\"gdpPercap: 2202.9884\nlifeExp: 47.80000\npop: 17104986\ncontinent: Africa\",\"gdpPercap: 3781.4106\nlifeExp: 52.53700\npop: 551425\ncontinent: Africa\",\"gdpPercap: 962.4923\nlifeExp: 49.91900\npop: 17129565\ncontinent: Africa\",\"gdpPercap: 1532.7770\nlifeExp: 52.88700\npop: 2308582\ncontinent: Africa\",\"gdpPercap: 3120.8768\nlifeExp: 59.83700\npop: 6005061\ncontinent: Africa\",\"gdpPercap: 843.7331\nlifeExp: 50.35000\npop: 11457758\ncontinent: Africa\",\"gdpPercap: 1588.6883\nlifeExp: 51.38600\npop: 5216550\ncontinent: Africa\",\"gdpPercap: 685.5877\nlifeExp: 57.67400\npop: 6642107\ncontinent: Africa\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":[6.32127781530702,5.29612899121632,4.85958274371316,4.29233940761101,5.26165272498491,4.97061767194295,5.5059176412168,4.66704214356912,5.05559917855771,4.06676322554192,6.94049569932146,4.52040583610645,5.45016604603988,4.01129639562885,7.60694585246374,3.97973112975184,4.73782100329548,7.39505365224446,4.26382530133105,4.22383634289638,5.76860640347983,5.03145493864573,4.27878249624843,6.11542223529683,4.44354450304809,4.56187177172394,4.77827606357594,5.51109590728372,5.22907021838905,5.3366454219231,4.49965617393791,4.33879188894232,6.41229679974379,5.82395887424691,4.36004866819238,5.23491359575959,8.62896025232351,4.17122854948148,5.09486783222961,3.77952755905512,5.17905878226218,4.854775454452,5.05045749460444,6.97912565512447,6.31771439639559,4.19891947998289,6.31954665895726,4.69663035294985,5.27632870617264,5.85427629024999,5.17305387000117,5.35482857766587],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"Africa\",\"legendgroup\":\"Africa\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.00341859740172,3.54999558616307,3.82348196645535,4.34421307668719,3.67731159008544,3.58158649939174,3.77282591203612,3.80485437036137,3.42845697609622,3.82475196835622,3.71087205749874,3.68841917661215,3.27283885748582,3.50558514939778,3.82283441748743,3.88507437266432,3.7392851792172,3.72850897552038,3.51166593362054,3.7980489038637,3.98991789679654,3.89760258369119,4.38152357914239,3.81320321246222,4.11872592974958],\"y\":[68.481,50.023,61.489,74.21,67.052,63.837,70.75,72.649,61.788,61.31,56.696,56.029,49.923,57.402,70.11,65.032,57.47,68.681,66.353,58.447,73.44,68.3,73.38,69.481,67.456],\"text\":[\"gdpPercap: 10079.0267\nlifeExp: 68.48100\npop: 26983828\ncontinent: Americas\",\"gdpPercap: 3548.0978\nlifeExp: 50.02300\npop: 5079716\ncontinent: Americas\",\"gdpPercap: 6660.1187\nlifeExp: 61.48900\npop: 114313951\ncontinent: Americas\",\"gdpPercap: 22090.8831\nlifeExp: 74.21000\npop: 23796400\ncontinent: Americas\",\"gdpPercap: 4756.7638\nlifeExp: 67.05200\npop: 10599793\ncontinent: Americas\",\"gdpPercap: 3815.8079\nlifeExp: 63.83700\npop: 25094412\ncontinent: Americas\",\"gdpPercap: 5926.8770\nlifeExp: 70.75000\npop: 2108457\ncontinent: Americas\",\"gdpPercap: 6380.4950\nlifeExp: 72.64900\npop: 9537988\ncontinent: Americas\",\"gdpPercap: 2681.9889\nlifeExp: 61.78800\npop: 5302800\ncontinent: Americas\",\"gdpPercap: 6679.6233\nlifeExp: 61.31000\npop: 7278866\ncontinent: Americas\",\"gdpPercap: 5138.9224\nlifeExp: 56.69600\npop: 4282586\ncontinent: Americas\",\"gdpPercap: 4879.9927\nlifeExp: 56.02900\npop: 5703430\ncontinent: Americas\",\"gdpPercap: 1874.2989\nlifeExp: 49.92300\npop: 4908554\ncontinent: Americas\",\"gdpPercap: 3203.2081\nlifeExp: 57.40200\npop: 3055235\ncontinent: Americas\",\"gdpPercap: 6650.1956\nlifeExp: 70.11000\npop: 2156814\ncontinent: Americas\",\"gdpPercap: 7674.9291\nlifeExp: 65.03200\npop: 63759976\ncontinent: Americas\",\"gdpPercap: 5486.3711\nlifeExp: 57.47000\npop: 2554598\ncontinent: Americas\",\"gdpPercap: 5351.9121\nlifeExp: 68.68100\npop: 1839782\ncontinent: Americas\",\"gdpPercap: 3248.3733\nlifeExp: 66.35300\npop: 2984494\ncontinent: Americas\",\"gdpPercap: 6281.2909\nlifeExp: 58.44700\npop: 15990099\ncontinent: Americas\",\"gdpPercap: 9770.5249\nlifeExp: 73.44000\npop: 3080828\ncontinent: Americas\",\"gdpPercap: 7899.5542\nlifeExp: 68.30000\npop: 1039009\ncontinent: Americas\",\"gdpPercap: 24072.6321\nlifeExp: 73.38000\npop: 220239000\ncontinent: Americas\",\"gdpPercap: 6504.3397\nlifeExp: 69.48100\npop: 2873520\ncontinent: Americas\",\"gdpPercap: 13143.9510\nlifeExp: 67.45600\npop: 13503563\ncontinent: Americas\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(163,165,0,1)\",\"opacity\":1,\"size\":[6.97047083068522,5.15434238640086,10.3553727984455,6.77543956769576,5.77446910007444,6.85635435789675,4.65435232683653,5.6710443310913,5.18472018302092,5.42956521861593,5.0398270921093,5.23768683822733,5.13057191783969,4.83958883002819,4.66475315834704,8.68911697083458,4.74607251669638,4.59414954550014,4.82688148286866,6.23316601826461,4.84414878929001,4.37991825947829,12.9086371873162,4.80663028657321,6.03320025163606],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\"}},\"hoveron\":\"points\",\"name\":\"Americas\",\"legendgroup\":\"Americas\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2.89548517717506,4.28645875933068,2.81946314412135,2.72013629197205,2.86995736499049,4.048680298964,2.91027070197067,3.14072860861607,4.07513053543838,4.16696961434647,4.12406772903255,4.22037948985684,3.45520305365965,3.61345080718246,3.66812684894598,4.7728017852095,3.93750268823479,3.58296303108212,3.21682849824462,2.56937390961505,2.84142982784579,4.07365765189317,3.07037821749517,3.37533512418369,4.53361654092087,4.04960907918865,3.12993971717703,3.50453672644772,3.74791804661293,3.29252733979409,2.85341657088882,3.56618184839601,3.26239535810376],\"y\":[38.438,65.593,46.923,31.22,63.96736,73.6,54.208,52.702,57.702,60.413,73.06,75.38,61.134,67.159,64.766,69.343,66.099,65.256,55.491,56.059,46.748,57.367,54.043,60.06,58.69,70.795,65.949,61.195,70.59,62.494,55.764,60.765,44.175],\"text\":[\"gdpPercap: 786.1134\nlifeExp: 38.43800\npop: 14880372\ncontinent: Asia\",\"gdpPercap: 19340.1020\nlifeExp: 65.59300\npop: 297410\ncontinent: Asia\",\"gdpPercap: 659.8772\nlifeExp: 46.92300\npop: 80428306\ncontinent: Asia\",\"gdpPercap: 524.9722\nlifeExp: 31.22000\npop: 6978607\ncontinent: Asia\",\"gdpPercap: 741.2375\nlifeExp: 63.96736\npop: 943455000\ncontinent: Asia\",\"gdpPercap: 11186.1413\nlifeExp: 73.60000\npop: 4583700\ncontinent: Asia\",\"gdpPercap: 813.3373\nlifeExp: 54.20800\npop: 634000000\ncontinent: Asia\",\"gdpPercap: 1382.7021\nlifeExp: 52.70200\npop: 136725000\ncontinent: Asia\",\"gdpPercap: 11888.5951\nlifeExp: 57.70200\npop: 35480679\ncontinent: Asia\",\"gdpPercap: 14688.2351\nlifeExp: 60.41300\npop: 11882916\ncontinent: Asia\",\"gdpPercap: 13306.6192\nlifeExp: 73.06000\npop: 3495918\ncontinent: Asia\",\"gdpPercap: 16610.3770\nlifeExp: 75.38000\npop: 113872473\ncontinent: Asia\",\"gdpPercap: 2852.3516\nlifeExp: 61.13400\npop: 1937652\ncontinent: Asia\",\"gdpPercap: 4106.3012\nlifeExp: 67.15900\npop: 16325320\ncontinent: Asia\",\"gdpPercap: 4657.2210\nlifeExp: 64.76600\npop: 36436000\ncontinent: Asia\",\"gdpPercap: 59265.4771\nlifeExp: 69.34300\npop: 1140357\ncontinent: Asia\",\"gdpPercap: 8659.6968\nlifeExp: 66.09900\npop: 3115787\ncontinent: Asia\",\"gdpPercap: 3827.9216\nlifeExp: 65.25600\npop: 12845381\ncontinent: Asia\",\"gdpPercap: 1647.5117\nlifeExp: 55.49100\npop: 1528000\ncontinent: Asia\",\"gdpPercap: 371.0000\nlifeExp: 56.05900\npop: 31528087\ncontinent: Asia\",\"gdpPercap: 694.1124\nlifeExp: 46.74800\npop: 13933198\ncontinent: Asia\",\"gdpPercap: 11848.3439\nlifeExp: 57.36700\npop: 1004533\ncontinent: Asia\",\"gdpPercap: 1175.9212\nlifeExp: 54.04300\npop: 78152686\ncontinent: Asia\",\"gdpPercap: 2373.2043\nlifeExp: 60.06000\npop: 46850962\ncontinent: Asia\",\"gdpPercap: 34167.7626\nlifeExp: 58.69000\npop: 8128505\ncontinent: Asia\",\"gdpPercap: 11210.0895\nlifeExp: 70.79500\npop: 2325300\ncontinent: Asia\",\"gdpPercap: 1348.7757\nlifeExp: 65.94900\npop: 14116836\ncontinent: Asia\",\"gdpPercap: 3195.4846\nlifeExp: 61.19500\npop: 7932503\ncontinent: Asia\",\"gdpPercap: 5596.5198\nlifeExp: 70.59000\npop: 16785196\ncontinent: Asia\",\"gdpPercap: 1961.2246\nlifeExp: 62.49400\npop: 44148285\ncontinent: Asia\",\"gdpPercap: 713.5371\nlifeExp: 55.76400\npop: 50533506\ncontinent: Asia\",\"gdpPercap: 3682.8315\nlifeExp: 60.76500\npop: 1261091\ncontinent: Asia\",\"gdpPercap: 1829.7652\nlifeExp: 44.17500\npop: 8403990\ncontinent: Asia\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,191,125,1)\",\"opacity\":1,\"size\":[6.14601109401926,4.06189233680308,9.29441811927379,5.39475462377927,22.6771653543307,5.08426676230058,19.2706113323364,10.9715821633898,7.43994831281003,5.89270757284543,4.9155533082844,10.342652961876,4.61658106384004,6.25889092142746,7.4890188516279,4.41106157328659,4.85034613145665,5.97722629927717,4.51816213431315,7.2295116202132,6.06899974449858,4.36894912815136,9.21575428359059,7.98702151037774,5.52430918996644,4.70007429187427,6.08413180764231,5.50291508915639,6.29375367739212,7.86362869673143,8.14954658274176,4.44626602186696,5.5539430462659],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\"}},\"hoveron\":\"points\",\"name\":\"Asia\",\"legendgroup\":\"Asia\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[3.54814411807184,4.29555439638822,4.28144187754483,3.54758782074795,3.88151249733011,4.05328536317461,4.17026642863828,4.31011744274222,4.19327554039949,4.26227627214274,4.31202751232983,4.15215143685527,4.06725083957618,4.29347221889702,4.04731308089082,4.15399722211687,3.98208706693409,4.32652140433408,4.36756741361793,3.97809563416623,4.00742708886894,3.97110865215781,4.11329709452241,4.03832857580155,4.18403893629866,4.12178698250771,4.27544324046184,4.43107881405245,3.63033859911073,4.24126620200209],\"y\":[68.93,72.17,72.8,69.86,70.81,70.64,70.71,74.69,72.52,73.83,72.5,73.68,69.95,76.11,72.03,73.48,73.066,75.24,75.37,70.67,70.41,69.46,70.3,70.45,70.97,74.39,75.44,75.39,59.507,72.76],\"text\":[\"gdpPercap: 3533.0039\nlifeExp: 68.93000\npop: 2509048\ncontinent: Europe\",\"gdpPercap: 19749.4223\nlifeExp: 72.17000\npop: 7568430\ncontinent: Europe\",\"gdpPercap: 19117.9745\nlifeExp: 72.80000\npop: 9821800\ncontinent: Europe\",\"gdpPercap: 3528.4813\nlifeExp: 69.86000\npop: 4086000\ncontinent: Europe\",\"gdpPercap: 7612.2404\nlifeExp: 70.81000\npop: 8797022\ncontinent: Europe\",\"gdpPercap: 11305.3852\nlifeExp: 70.64000\npop: 4318673\ncontinent: Europe\",\"gdpPercap: 14800.1606\nlifeExp: 70.71000\npop: 10161915\ncontinent: Europe\",\"gdpPercap: 20422.9015\nlifeExp: 74.69000\npop: 5088419\ncontinent: Europe\",\"gdpPercap: 15605.4228\nlifeExp: 72.52000\npop: 4738902\ncontinent: Europe\",\"gdpPercap: 18292.6351\nlifeExp: 73.83000\npop: 53165019\ncontinent: Europe\",\"gdpPercap: 20512.9212\nlifeExp: 72.50000\npop: 78160773\ncontinent: Europe\",\"gdpPercap: 14195.5243\nlifeExp: 73.68000\npop: 9308479\ncontinent: Europe\",\"gdpPercap: 11674.8374\nlifeExp: 69.95000\npop: 10637171\ncontinent: Europe\",\"gdpPercap: 19654.9625\nlifeExp: 76.11000\npop: 221823\ncontinent: Europe\",\"gdpPercap: 11150.9811\nlifeExp: 72.03000\npop: 3271900\ncontinent: Europe\",\"gdpPercap: 14255.9847\nlifeExp: 73.48000\npop: 56059245\ncontinent: Europe\",\"gdpPercap: 9595.9299\nlifeExp: 73.06600\npop: 560073\ncontinent: Europe\",\"gdpPercap: 21209.0592\nlifeExp: 75.24000\npop: 13852989\ncontinent: Europe\",\"gdpPercap: 23311.3494\nlifeExp: 75.37000\npop: 4043205\ncontinent: Europe\",\"gdpPercap: 9508.1415\nlifeExp: 70.67000\npop: 34621254\ncontinent: Europe\",\"gdpPercap: 10172.4857\nlifeExp: 70.41000\npop: 9662600\ncontinent: Europe\",\"gdpPercap: 9356.3972\nlifeExp: 69.46000\npop: 21658597\ncontinent: Europe\",\"gdpPercap: 12980.6696\nlifeExp: 70.30000\npop: 8686367\ncontinent: Europe\",\"gdpPercap: 10922.6640\nlifeExp: 70.45000\npop: 4827803\ncontinent: Europe\",\"gdpPercap: 15277.0302\nlifeExp: 70.97000\npop: 1746919\ncontinent: Europe\",\"gdpPercap: 13236.9212\nlifeExp: 74.39000\npop: 36439000\ncontinent: Europe\",\"gdpPercap: 18855.7252\nlifeExp: 75.44000\npop: 8251648\ncontinent: Europe\",\"gdpPercap: 26982.2905\nlifeExp: 75.39000\npop: 6316424\ncontinent: Europe\",\"gdpPercap: 4269.1223\nlifeExp: 59.50700\npop: 42404033\ncontinent: Europe\",\"gdpPercap: 17428.7485\nlifeExp: 72.76000\npop: 56179000\ncontinent: Europe\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,176,246,1)\",\"opacity\":1,\"size\":[4.73711086274366,5.46245399673978,5.69923465730081,5.00994841135964,5.59538443706805,5.04523525790558,5.73248147708556,5.15554006271282,5.10659105931128,8.26207702648979,9.21603585103979,5.64793676344929,5.77801236958701,4.00561540333621,4.87759423629259,8.38266622872152,4.20280449359801,6.06235892227666,5.00334741211677,7.39523452347783,5.68347312501541,6.63718460047671,5.58381321630558,5.11921103857739,4.57227899402677,7.48917192574021,5.53761739047554,5.31519777193933,7.78197424302696,8.38758788519898],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\"}},\"hoveron\":\"points\",\"name\":\"Europe\",\"legendgroup\":\"Europe\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.26326190559163,4.21041798943211],\"y\":[73.49,72.22],\"text\":[\"gdpPercap: 18334.1975\nlifeExp: 73.49000\npop: 14074100\ncontinent: Oceania\",\"gdpPercap: 16233.7177\nlifeExp: 72.22000\npop: 3164900\ncontinent: Oceania\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(231,107,243,1)\",\"opacity\":1,\"size\":[6.08061917751821,4.85899251590851],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\"}},\"hoveron\":\"points\",\"name\":\"Oceania\",\"legendgroup\":\"Oceania\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":26.2283105022831,\"r\":7.30593607305936,\"b\":40.1826484018265,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[2.45920251583532,4.88297317898922],\"tickmode\":\"array\",\"ticktext\":[\"300\",\"1000\",\"3000\",\"10000\",\"30000\"],\"tickvals\":[2.47712125471966,3,3.47712125471966,4,4.47712125471966],\"categoryorder\":\"array\",\"categoryarray\":[\"300\",\"1000\",\"3000\",\"10000\",\"30000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"gdpPercap\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[28.9755,78.3545],\"tickmode\":\"array\",\"ticktext\":[\"30\",\"40\",\"50\",\"60\",\"70\"],\"tickvals\":[30,40,50,60,70],\"categoryorder\":\"array\",\"categoryarray\":[\"30\",\"40\",\"50\",\"60\",\"70\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"lifeExp\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.93503937007874},\"annotations\":[{\"text\":\"continent\npop\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"11b040177098\":{\"x\":{},\"y\":{},\"size\":{},\"colour\":{},\"type\":\"scatter\"}},\"cur_data\":\"11b040177098\",\"visdat\":{\"11b040177098\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r1 2  tmp \u0026lt;- c(1,2,3,4) print(tmp)   ## [1] 1 2 3 4\r","description":"test","id":72,"section":"posts","tags":null,"title":"test","uri":"https://jiwooblog.netlify.app/posts/r/test/"},{"content":"Natural Scene Categories Revealed in Distributed Patterns of Activity in the Human Brain(Walther, 2009)\nReference Walther, D. B., Caddigan, E., Fei-Fei, L., \u0026amp; Beck, D. M. (2009). Natural scene categories revealed in distributed patterns of activity in the human brain. Journal of neuroscience, 29(34), 10573-10581.\nSummary 자연 장면을 보고 카테고리를 분류할 때, 뇌에서 어떠한 패턴이 일어나는지 분석하는 것이 본 논문의 핵심이다. fMRI와 MVPA (multivoxel pattern analysis)를 활용하여 뇌의 어떤 영역에서 그리고 어떠한 패턴으로 자연 장면의 카테고리를 구분하는지 확인하였다. 다섯 명의 참가자가 있었으며 총 여섯 개의 카테고리(카테고리별 120개 사진)를 활용하였다. 연구 결과, V1, PPA, RSC, LOC 영역 모두에서 자연장면의 카테고리를 구분할 때 활용되는 정보를 갖고 있음을 확인하였다. 그리고 직접 버튼을 눌러서 카테고리를 분류하는 인간행동연구와 비교해본 결과, V1을 제외한 PPA, RSC, LOC가 해석한 정보를 인간이 활용하여 카테고리 분류를 한다고 판단하였다. 특히, fMRI 신호를 기반으로 카테고리를 예측분류한 것의 오답 패턴이 참가자들의 행동 오답패턴과 상관관계가 높은 것으로 앞서 언급한 세 뇌 영역에서 나타났다. 또한 눈여겨보아야 할 결과로는, 그림을 상하 반전하여 제시하였을 때 PPA 신호를 통한 예측과 행동연구 모두에서 정확도가 떨어졌다는 점이다. 종합적인 결론으로, PPA, RSC, LOC를 포함한 뇌영역들이 일종의 네트워크를 형성하여 정보를 모아 자연장면의 카테고리를 분류하는 작업을 시행한다고 볼 수 있다.\n해당 논문은 자연장면을 해석하는 데에 있어서 MVPA를 활용하여 여러 뇌 영역 간의 연관성 있는 패턴을 밝혔다는 데에 차별적인 의의가 있다고 할 수 있다. 또한, fMRI 신호를 분석하여 예측한 것과 실제 행동분석을 비교하면서 단순히 정보가 뇌 영역에 있다는 것과 그것을 활용하는 것의 차이를 명확하게 구분하였다는 점에서 한층 더 철저한 연구설계가 이루어졌음을 알 수 있다. 나아가, 뇌가 이미지 그 자체를 해석하는(decode) 것과 해당 이미지의 카테고리를 해석하는(decode)하는 것의 차이를 역시 확실히 하고 있다.\n그럼에도 불구하고 완벽하게 이해가 되지 않는 점들이 있긴 하다. 본 연구의 한계점은, 많은 연구에서 직면할 수밖에 없는 한계이기는 하지만, 판단기준에 대한 통계적 근거가 다소 설득력이 부족하다는 점이다. 행동연구와 fMRI 분석을 통한 예측의 실패를 비교할 때, frequent confusion과 아닌 것을 구분하는 기준이 다소 모호하다. 또한, 각 영역별로 행동분석과 상관관계를 분석하였을 때, 이상치처럼 보이는 것들에 의해 상관계수가 왜곡되었을 가능성도 충분히 존재한다. 그리고 피어슨 상관계수는 오로지 선형적 연관성만을 이야기할 수 있기 때문에 뇌 영역끼리 복잡한 관계로 관련이 있다면 이를 분석하는 데에 있어서는 상당히 제한적인 통계방법을 사용했다고 볼 수 있다.\n궁금증   decoding accuracy에 대해 t검정을 할 때, 기준치로서 1/6이 적절한가?! 만약 연구자가 20개의 카테고리를 활용하였다면 1/20이 적절하다고 할 수 있을까? 그리고 단순히 p-value만 볼 것이 아니라 effect size를 고려하지 않아도 괜찮은 걸까? 왜냐하면 이미지 개수가 많아지면 많아질수록 p-value은 점점 작게 나올 가능성이 높아지기 때문이다.\n  Figure 2에서 오답 비율이 높은 것(노란색)이 대각선을 기준으로 대칭이 되지 않아도 문제가 없는가? industry를 보고 building이라고 생각했으면, building을 보고 industry라고 생각했을 비율도 같이 높아야 하는 것 아닐까? 이부분에서 놓친 논리는 없을까?\n  연구자는 모든 뇌 영역을 decoder라고 보았다. 하지만, 뇌 영역의 입장에서 보면, 특정 영역은 다른 영역의 encoder로서 역할을 할 수도 있지 않을까? 만약 그렇다면 어떠한 방식으로 연구를 설계할 수 있을까?\n ","description":"","id":73,"section":"blog","tags":["Psychology"],"title":"Natural Scene Categories Revealed in Distributed Patterns of Activity in the Human Brain","uri":"https://jiwooblog.netlify.app/blog/210312_natural_scene_categorization/"},{"content":"빅콘테스트 챔피언리그 데이터분석 분야\n배운 점\n 자연어 전처리   정규표현식 기존에 제시된 칼럼의 수가 굉장히 적었다. 그중에서 그나마 정보를 담고 있는 것은 \u0026lsquo;상품명\u0026rsquo; 칼럼이었다. 이 데이터를 예를 들어, \u0026lsquo;NIKE 스트라이프 셔츠\u0026rsquo;와 같은 형식으로 브랜드가 일반적으로 앞에 나오고 뒤에 디테일한 상품 분류가 나왔다. 하지만 식료품과 같은 경우에는 브랜드가 없는 것들이 많았고, 띄어쓰기가 제대로 되어있지 않은 경우도 많았다. (아마도 현실 데이터는 이것보다도 더 정돈되지 않은 경우가 많을 터이다\u0026hellip;) 또한 브랜드의 표기 자체가 통일되지 않은 경우가 있었다.(ex. 카사미아=까사미아) NS Shop+ 공식홈페이지에서 나눠놓은 대분류, 중분류, 소분류 체계를 참고해서 나눴다. 너무 많은 상품명이 있었기 때문에, 대분류 3개씩 묶어서 역할분담을 해서 나머지 분류를 채웠는데, 그러다보니 사소하게 통일되지 않은 분류기준이 있어서 추후 약간의 어려움을 겪었다. 기준을 명확하게 정해두거나, 만약 그러지 못할 경우 최소한의 사람이 해당 업무를 했으면 어땠을까라는 생각이 든다. ","description":"","id":74,"section":"blog","tags":["project"],"title":"빅콘테스트 NS Shop+ 홈쇼핑 실적 예측","uri":"https://jiwooblog.netlify.app/blog/bigcontest2020/"},{"content":"서울시 수소차 충전소 입지 추천\n연세대학교 데이터사이언스입문 수업에서 진행한 프로젝트\n목표 최근 심각해진 미세먼지 등의 환경 문제에 대한 대책으로 수소차의 생산 및 보급 이슈가 주목받고 있다.\n이와 관련하여 서울시는 현대차가 수소전기차 보급 활성화를 위한 MOU를 체결하는 등의 노력을 기울이고 있다.\n다만, 수소차 충전소 인프라 형성이 아직 미흡한 탓에 보급에 다소 어려움을 겪고 있는 실정이다.\n환경부와 국토교통부 등의 관련 부처에서는 2040년까지 총 1200기의 충전소를 설치할 예정이다.\n이에 대해 지역, 사회별 변수들을 고려하여 서울 시내의 적합한 수소차 충전소 입지를 선정하는 것을 목표로 한다.\n결과물 블로그\n대쉬보드\n역할 마침 지난 2019년 겨울방학 때 머신러닝을 공부해보기 시작해서, 주로 모델링과 관련된 역할을 많이 맡았다.\n다양한 방법론을 시도하였는데, 의미 해석을 도출하기 위해 Random Forest를 채택하게 되었다. 하지만 데이터캠프를 통해서 shiny dashboard를 포함한 다양한 것들을 공부할 수 있었다.\n배운 점  데이터 수집 및 전처리   기존에 데이터가 주어지는 것이 아니기 때문에, 필요한 데이터를 직접 일일이 찾아나서는 노력을 했다. 그 과정에서 거리 변수를 직접 계산하여 파생변수를 만들었다. 지역 단위를 최대한 동 단위로 나눠서 계산을 했다.  tidyverse   이전에는 tidyverse 문법을 잘 모르고 기본 base 문법만 썼다. tidyverse의 편리함을 잘 모르고 python만 겨울방학동안 썼었는데, R의 %\u0026gt;% pipe line과 ggplot의 편리함을 알 수 있었다.  다양한 R 활용법   blogdown: R로 블로그 만들기 shiny: R로 대쉬보드 만들기 xaringan: R로 프렌젠테이션 자료 만들기  개선할 점  수소차 보유 대수 자체가 애초에 충분하지 못했기 때문에, 포아송회귀분석을 한번 해봤으면 어떨까 하는 아쉬움이 있다. xaringan 패키지를 너무 늦게 알게 되어서, 숙련도가 높지 못했다. (2021.03.14에 알게 된 정보로는, ) ","description":"","id":75,"section":"blog","tags":["project"],"title":"수소차 충전소 입지 추천","uri":"https://jiwooblog.netlify.app/blog/rhino/"},{"content":"마크다운 가이드라인\nLorem est tota propiore conpellat pectoribus de\npectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nThis article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\n\rHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae.\nNote that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rCode block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item  First Sub-item Second Sub-item    Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","description":"","id":76,"section":"blog","tags":["markdown"],"title":"Markdown Syntax Guide","uri":"https://jiwooblog.netlify.app/blog/markdown-syntax/"},{"content":"Youtube 링크 거는 법\nHugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n\rYoutube 링크는 이렇게 하기!   ","description":"","id":77,"section":"blog","tags":["Youtube"],"title":"How to Link","uri":"https://jiwooblog.netlify.app/blog/rich-content/"},{"content":"이건 정확히 무엇인지 모르겠다..\nVagus elidunt \nThe Van de Graaf Canon\n","description":"","id":78,"section":"blog","tags":null,"title":"Placeholder Text","uri":"https://jiwooblog.netlify.app/blog/placeholder-text/"},{"content":"이모지 관련\nLorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice\nsubde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc\ncaluere tempus\nEmoji can be enabled in a Hugo project in a number of ways.\n\rThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 🙈 🙉 🙉 🙊 🙊\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3  .emoji { font-family: Apple Color Emoji,Segoe UI Emoji,NotoColorEmoji,Segoe UI Symbol,Android Emoji,EmojiSymbols; }  ","description":"","id":79,"section":"blog","tags":["emoji"],"title":"Emoji Support","uri":"https://jiwooblog.netlify.app/blog/emoji-support/"}]